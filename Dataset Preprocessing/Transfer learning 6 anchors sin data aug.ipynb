{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = \"/tf/home/sergio/Tesis\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(root_path+\"/TinyYOLOv3-Pedestrian-Detection\")\n",
    "\n",
    "from YOLOblocks import TinyYOLOv3,BasicBlock,PredictionLayer#,YOLOLossBasicBlock\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "#from tensorflow.python.tools import freeze_graph\n",
    "#from skimage.io import imread,imshow\n",
    "#from skimage.transform import resize \n",
    "import time\n",
    "#from tensorflow.compat.v1.image import decode_image\n",
    "import imgaug.augmenters as iaa\n",
    "from imgaug.augmentables.bbs import BoundingBox, BoundingBoxesOnImage\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.mixed_precision import experimental as mixed_precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Mixed precision compatibility check (mixed_float16): OK\n",
      "Your GPU will likely run quickly with dtype policy mixed_float16 as it has compute capability of at least 7.0. Your GPU: GeForce RTX 2070 SUPER, compute capability 7.5\n"
     ]
    }
   ],
   "source": [
    "policy = mixed_precision.Policy('mixed_float16')\n",
    "mixed_precision.set_policy(policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_feature_description = {\n",
    "    'bboxes': tf.io.FixedLenFeature([], tf.string),\n",
    "    'image_raw': tf.io.FixedLenFeature([], tf.string),\n",
    "    'num_real_boxes':tf.io.FixedLenFeature([], tf.int64),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_iou_matrix_tf(box_arr1, box_arr2):\n",
    "    \n",
    "    box_arr1 = box_arr1 -tf.tile(box_arr1[:,:2],[1,2])\n",
    "    #print(box_arr1)\n",
    "    x11, y11, x12, y12 = tf.split(box_arr1, 4, axis=1)\n",
    "    x21, y21, x22, y22 = tf.split(box_arr2, 4, axis=1)\n",
    "    xA = tf.maximum(x11, tf.transpose(x21))\n",
    "    yA = tf.maximum(y11, tf.transpose(y21))\n",
    "    xB = tf.minimum(x12, tf.transpose(x22))\n",
    "    yB = tf.minimum(y12, tf.transpose(y22))\n",
    "    interArea = tf.maximum((xB - xA + 1e-9), 0) * tf.maximum((yB - yA + 1e-9), 0)\n",
    "    boxAArea = (x12 - x11 + 1e-9) * (y12 - y11 + 1e-9)\n",
    "    boxBArea = (x22 - x21 + 1e-9) * (y22 - y21 + 1e-9)\n",
    "    iou = interArea / (boxAArea + tf.transpose(boxBArea) - interArea)\n",
    "    return iou,tf.argmax(iou,axis=1)#[:,tf.newaxis]\n",
    "\n",
    "\n",
    "def fill_yolo_output(boxes,grid_size,num_anchors,which_anchor_box,which_anchor_box_index):\n",
    "    #print(boxes.shape)\n",
    "    #noobj_mask = tf.ones((1,grid_size*grid_size*num_anchors))\n",
    "    #print(noobj_mask.shape)\n",
    "    \n",
    "    x_min,y_min,x_max,y_max =tf.split(boxes,4,axis=1)\n",
    "\n",
    "    #Transforma las coordenadas de (xmin,ymin,xmax,ymax) --> (xcenter,ycenter,width,height)\n",
    "    width = x_max-x_min\n",
    "    height = y_max-y_min\n",
    "    x_global =x_min + tf.math.divide(x_max - x_min,2)\n",
    "    y_global =y_min + tf.math.divide(y_max - y_min,2)\n",
    "    \n",
    "    \n",
    "    x_min_anchor,y_min_anchor,x_max_anchor,y_max_anchor =tf.split(which_anchor_box,4,axis=1)\n",
    "    \n",
    "    width_anchor = x_max_anchor-x_min_anchor\n",
    "    height_anchor = y_max_anchor-y_min_anchor\n",
    "    x_global_anchor =x_min_anchor + tf.math.divide(x_max_anchor - x_min_anchor,2)\n",
    "    y_global_anchor =y_min_anchor + tf.math.divide(y_max_anchor - y_min_anchor,2)   \n",
    "\n",
    "    \n",
    "    #print(\"el x original\",x_global)\n",
    "    #print(\"el y original\",y_global)\n",
    "    #print(\"el w original\",width)\n",
    "    #print(\"el h original\",height)\n",
    "    \n",
    "    #porción de la imagen que hay en cada celda\n",
    "    pixel_per_grid = tf.math.divide(1.,grid_size)\n",
    "    #print(pixel_per_grid)\n",
    "    \n",
    "    #Obtenemos la coordenada de la celda donde están los boundingboxes\n",
    "    offset_grid_x = x_global//pixel_per_grid\n",
    "    offset_grid_y = y_global//pixel_per_grid\n",
    "    \n",
    "    #Obtenemos el el centro locacon referencia  al celda encontrada previamente\n",
    "    x_local =tf.math.floormod(x_global,pixel_per_grid)\n",
    "    y_local =tf.math.floormod(y_global,pixel_per_grid)\n",
    "    #print(x_local,y_local)\n",
    "    \n",
    "    #Valores tx e ty del groudtruth\n",
    "    tx = tf.math.log(x_local + 1e-07/(1-x_local))\n",
    "    ty = tf.math.log(y_local+1e-07/(1-y_local))\n",
    "    tw = tf.math.log(tf.math.divide(width+1e-07,width_anchor))\n",
    "    th = tf.math.log(tf.math.divide(height+1e-07,height_anchor))\n",
    "    tobj_mask = tf.ones_like(tx)\n",
    "    tobj = tf.concat([tobj_mask,tobj_mask],axis=0)\n",
    "    \n",
    "    #tnoobj = tf.zeros_like(tx)    \n",
    "    #tobj = tf.ones((grid_size*grid_size*num_anchors,1))\n",
    "    #tnoobj = tf.zeros((grid_size*grid_size*num_anchors,1))\n",
    "    #print(\"Lo que la red debe predecir\",tx.numpy(),ty.numpy(),tw.numpy(),th.numpy())\n",
    "    #x_global = (offset_grid_x * pixel_per_grid) + tf.math.sigmoid(tx)\n",
    "    #y_global = (offset_grid_y * pixel_per_grid) + tf.math.sigmoid(ty)\n",
    "    #w = width_anchor*tf.math.exp(tw)\n",
    "    #h = height_anchor*tf.math.exp(th)\n",
    "    #print(\"obtnemos el x_real\",x_global)\n",
    "    #print(\"obtenemos el y_real\",y_global)\n",
    "    #print(\"obtenemos el w real\",w)\n",
    "    #print(\"obtenemos el h real\",h)\n",
    "    \n",
    "    #anchor_boxes_per_output = num_anchors//2\n",
    "\n",
    "    #Residuo indica cual de los 3 anchor boxes de la coordenada es la que llevara el 1\n",
    "    #Coord representa la coordenada del grid\n",
    "    \n",
    "    residuo = tf.math.floormod(which_anchor_box_index,num_anchors)[:,tf.newaxis]\n",
    "    coord = tf.cast(num_anchors*(offset_grid_y*grid_size + offset_grid_x),dtype=tf.int64)\n",
    "    \n",
    "    coord_objectness = tf.cast(2*(offset_grid_y*grid_size + offset_grid_x),dtype=tf.int64)\n",
    "    coord_objectness2 = coord_objectness+1\n",
    "    coord_objectess_global = tf.concat([coord_objectness,coord_objectness2],axis=0)\n",
    "    \n",
    "    output_position = residuo+coord\n",
    "    print(\"tipo de aoutput_positivon\",output_position)\n",
    "    \n",
    "    print(output_position)\n",
    "    \n",
    "    dense_shape = grid_size*grid_size*num_anchors\n",
    "    print(dense_shape)\n",
    "    tx_vector = tf.sparse.reorder(tf.sparse.SparseTensor(indices=output_position, values=tx[:,0], dense_shape=[dense_shape]))\n",
    "    ty_vector = tf.sparse.reorder(tf.sparse.SparseTensor(indices=output_position, values=ty[:,0], dense_shape=[dense_shape]))\n",
    "    tw_vector = tf.sparse.reorder(tf.sparse.SparseTensor(indices=output_position, values=tw[:,0], dense_shape=[dense_shape]))\n",
    "    th_vector = tf.sparse.reorder(tf.sparse.SparseTensor(indices=output_position, values=th[:,0], dense_shape=[dense_shape]))\n",
    "    obj_mask = tf.sparse.reorder(tf.sparse.SparseTensor(indices=output_position, values=tobj_mask[:,0], dense_shape=[dense_shape]))\n",
    "    objectness_vector = tf.sparse.reorder(tf.sparse.SparseTensor(indices=coord_objectess_global, values=tobj[:,0], dense_shape=[dense_shape]))\n",
    "    #noobj_mask = tf.sparse.reorder(tf.sparse.SparseTensor(indices=output_position, values=tnoobj[:,0], dense_shape=[dense_shape]))\n",
    "    #obj_mask =tx_vector=ty_vector=tw_vector=th_vector = tf.zeros((1,grid_size*grid_size*num_anchors))\n",
    "    \n",
    "    tx_vector_dense = tf.sparse.to_dense(tx_vector, default_value=0, validate_indices=False, name=\"Dense_tx\")\n",
    "    ty_vector_dense = tf.sparse.to_dense(ty_vector, default_value=0, validate_indices=False, name=\"Dense_ty\")\n",
    "    tw_vector_dense = tf.sparse.to_dense(tw_vector, default_value=0, validate_indices=False, name=\"Dense_tw\")\n",
    "    th_vector_dense = tf.sparse.to_dense(th_vector, default_value=0, validate_indices=False, name=\"Dense_th\")\n",
    "    obj_mask_dense =  tf.sparse.to_dense(obj_mask, default_value=0, validate_indices=False, name=\"Dense_obj\")\n",
    "    #noobj_mask_dense = 1-obj_mask_dense\n",
    "    objectness_vector_dense =  tf.sparse.to_dense(objectness_vector, default_value=0, validate_indices=False)\n",
    "    \n",
    "    #noobj_mask_dense= tf.sparse.to_dense(noobj_mask, default_value=1, validate_indices=False, name=\"Dense_noobj\")\n",
    "    ##print(tx_vector.to_dense)\n",
    "    #print(tf.sparse.to_dense(tx_vector, default_value=0, validate_indices=True, name=None)\n",
    "    #tx_vector=tx_vector[[3,2],]\n",
    "    #tx_vector[output_position[:,0]] = tx\n",
    "    #print(\"coordenada de la salida:\",output_position)\n",
    "    \n",
    "    #return ((tx_vector_dense,ty_vector_dense,obj_mask_dense),(tw_vector_dense,th_vector_dense,obj_mask_dense),(objectness),(objectness))\n",
    "    \n",
    "    return tx_vector_dense,ty_vector_dense,tw_vector_dense,th_vector_dense,obj_mask_dense,objectness_vector_dense\n",
    "\n",
    "def build_targets(image,image_bboxes,num_real_boxes,anchor_boxes):\n",
    "    \n",
    "    images_bboxes_original = image_bboxes\n",
    "    #Obtenemos los boduing boxes que son reales\n",
    "    image_bboxes = image_bboxes[:num_real_boxes,:]\n",
    "    #print(\"Bouding boxes de la imagen\",image_bboxes)\n",
    "    #Obteneos  la matriz de IoU , y el índice del anchor box que dió mejor resultado\n",
    "    \n",
    "    #Nprmalizamos con respecto al tamaño de la imagen y obtenemos la Iou con los anchor boxes\n",
    "    image_bboxes = tf.math.divide(image_bboxes,416)\n",
    "    iou_matrix,which_anchor_box_index = get_iou_matrix_tf(image_bboxes,anchor_boxes)\n",
    "    \n",
    "    print(which_anchor_box_index)\n",
    "\n",
    "    anchor_boxes_per_output = len(anchor_boxes)//2\n",
    "    #Indices de los bouding boxes que irian en cada salida, index_best_ yolo nos dice que bouding boxes de la imagen van a la salida YOLO1,\n",
    "    #porque su mejor IoU fue con los len(anchor_boxes)//2 anchor boxes mas grandes\n",
    "    index_best_yolo1 = tf.where(which_anchor_box_index>=anchor_boxes_per_output)[:,0]\n",
    "    index_best_yolo2 = tf.where(which_anchor_box_index<anchor_boxes_per_output)[:,0]\n",
    "    index_best_anchor_yolo1 = tf.gather(which_anchor_box_index,index_best_yolo1,axis=0)\n",
    "    index_best_anchor_yolo2 = tf.gather(which_anchor_box_index,index_best_yolo2,axis=0)\n",
    "    \n",
    "    print(index_best_yolo1)\n",
    "    print(index_best_anchor_yolo1)\n",
    "\n",
    "    print(index_best_yolo2)\n",
    "    print(index_best_anchor_yolo2)\n",
    "\n",
    "    \n",
    "    best_bboxes_yolo1 = tf.gather(image_bboxes,index_best_yolo1,axis =0)\n",
    "    best_anchors_yolo1 = tf.gather(anchor_boxes,index_best_anchor_yolo1, axis =0) #LOs dos anchor boxes grandes corrsponden a YOLO1\n",
    "    best_bboxes_yolo2 = tf.gather(image_bboxes,index_best_yolo2,axis =0)\n",
    "    best_anchors_yolo2 = tf.gather(anchor_boxes,index_best_anchor_yolo2, axis =0) #Los dos anchor boxes pequeños corresponden a YOLO2\n",
    "    \n",
    "    \n",
    "    if best_anchors_yolo1.shape[0] !=0:\n",
    "        tx_vector_yolo1,ty_vector_yolo1,tw_vector_yolo1,th_vector_yolo1,obj_mask_yolo1,obj_vector_yolo1= fill_yolo_output(best_bboxes_yolo1,13,anchor_boxes_per_output,best_anchors_yolo1,index_best_anchor_yolo1)\n",
    "    else:\n",
    "        tx_vector_yolo1=ty_vector_yolo1=tw_vector_yolo1=th_vector_yolo1=obj_mask_yolo1= obj_vector_yolo1=tf.zeros((1,grid_size*grid_size*num_anchors))\n",
    "        #noobj_mask_yolo1 = tf.ones((1,13*13*num_anchors))\n",
    "    \n",
    "    if best_anchors_yolo2.shape[0] != 0:\n",
    "        tx_vector_yolo2,ty_vector_yolo2,tw_vector_yolo2,th_vector_yolo2,obj_mask_yolo2,obj_vector_yolo2 = fill_yolo_output(best_bboxes_yolo2,26,anchor_boxes_per_output,best_anchors_yolo2,index_best_anchor_yolo2)\n",
    "    else:\n",
    "        tx_vector_yolo2=ty_vector_yolo2=tw_vector_yolo2=th_vector_yolo2=obj_mask_yolo2 = obj_vector_yolo2=tf.zeros((1,grid_size*grid_size*num_anchors))\n",
    "        #noobj_mask_yolo2 = tf.ones((1,26*26*num_anchors))\n",
    "        \n",
    "    tx_vector = tf.concat([tx_vector_yolo1,tx_vector_yolo2],axis=0)[:,tf.newaxis]\n",
    "    ty_vector = tf.concat([ty_vector_yolo1,ty_vector_yolo2],axis=0)[:,tf.newaxis]\n",
    "    tw_vector = tf.concat([tw_vector_yolo1,tw_vector_yolo2],axis=0)[:,tf.newaxis]\n",
    "    th_vector = tf.concat([th_vector_yolo1,th_vector_yolo2],axis=0)[:,tf.newaxis]\n",
    "    obj_mask = tf.concat([obj_mask_yolo1,obj_mask_yolo2],axis=0)[:,tf.newaxis]\n",
    "    #noobj_mask = tf.concat([noobj_mask_yolo1,noobj_mask_yolo2],axis=0)[:,tf.newaxis]\n",
    "    obj_vector = tf.concat([obj_vector_yolo1,obj_vector_yolo2],axis=0)[:,tf.newaxis]\n",
    "    \n",
    "    #output = tf.concat([tx_vector,ty_vector,tw_vector,th_vector,obj_mask,noobj_mask,obj_vector],axis=1)\n",
    "    #images_bboxes_original\n",
    "    #return image,output\n",
    "    #Vamos a regresar obj mask que es 1 cuando hay objeto en grid y el anchor box especifico\n",
    "    return tf.cast(image,tf.float32)/255,(tf.concat([tx_vector,ty_vector,obj_mask],axis=1),tf.concat([tw_vector,th_vector,obj_mask],axis=1),(obj_mask),(obj_mask))\n",
    "\n",
    "def imgaug_data_augmentation(image,bboxes,num_real_boxes):\n",
    "    im_shape = image.shape\n",
    "    bbs = BoundingBoxesOnImage.from_xyxy_array(bboxes*416, shape=(416,416))\n",
    "    \n",
    "    policy = np.random.randint(5)\n",
    "    \n",
    "    #policy = 2\n",
    "    if policy == 0:\n",
    "        \n",
    "        p = np.random.random()\n",
    "        if p<=0.6:\n",
    "            aug = iaa.TranslateX(px=(-60, 60),cval=128)\n",
    "            image, bbs = aug(image=image, bounding_boxes=bbs)\n",
    "            #bbs.remove_out_of_image().clip_out_of_image()\n",
    "    \n",
    "        p = np.random.random()\n",
    "        if p<=0.8:\n",
    "            aug = iaa.HistogramEqualization()\n",
    "            image, bbs = aug(image=image, bounding_boxes=bbs)\n",
    "            #bbs.remove_out_of_image().clip_out_of_image()\n",
    "    \n",
    "    elif policy==1:\n",
    "        \n",
    "        p=np.random.random()\n",
    "        if p<=0.2:\n",
    "            aug = iaa.TranslateY(px=(int(-0.18*416), int(0.18*416)),cval=128)\n",
    "            for i in bbs.to_xyxy_array(np.int32)[:num_real_boxes,:]:\n",
    "                bbox = image[i[1]:i[3],i[0]:i[2]]\n",
    "                bbox_augmented = aug(image=bbox)\n",
    "                image[i[1]:i[3],i[0]:i[2]] = bbox_augmented\n",
    "        \n",
    "        p=np.random.random()\n",
    "        if p<=0.8:\n",
    "            square_size = np.random.randint(48)\n",
    "            aug = iaa.Cutout(nb_iterations=1, size=square_size/416, squared=True)\n",
    "            image, bbs = aug(image=image, bounding_boxes=bbs)\n",
    "            #bbs.remove_out_of_image().clip_out_of_image()\n",
    "            \n",
    "    elif policy==2:\n",
    "        p=np.random.random()\n",
    "        if p<=1:\n",
    "            aug = iaa.ShearY(shear=(int(-0.06*416), int(0.06*416)), order=1, cval=128)\n",
    "            image, bbs = aug(image=image, bounding_boxes=bbs)\n",
    "            #bbs.remove_out_of_image().clip_out_of_image()\n",
    "            \n",
    "        p=np.random.random()\n",
    "        if p<=0.6:\n",
    "            aug = iaa.TranslateY(px=(int(-0.18*416), int(0.18*416)),cval=128)\n",
    "            for i in bbs.to_xyxy_array(np.int32)[:num_real_boxes,:]:\n",
    "                bbox = image[i[1]:i[3],i[0]:i[2]]\n",
    "                bbox_augmented = aug(image=bbox)\n",
    "                image[i[1]:i[3],i[0]:i[2]] = bbox_augmented\n",
    "            \n",
    "    elif policy==3:\n",
    "        p=np.random.random()\n",
    "        if p<=0.6:    \n",
    "            aug = iaa.Rotate(rotate=(-30, 30), order=1, cval=128)\n",
    "            image, bbs = aug(image=image, bounding_boxes=bbs)\n",
    "            #bbs_aug.remove_out_of_image().clip_out_of_image()\n",
    "        \n",
    "        p=np.random.random()\n",
    "        if p<=1:\n",
    "            aug = iaa.MultiplySaturation((0.54, 1.54))\n",
    "            image, bbs = aug(image=image, bounding_boxes=bbs)\n",
    "            #bbs.remove_out_of_image().clip_out_of_image()\n",
    "            \n",
    "    bbs.remove_out_of_image()\n",
    "    \n",
    "    return image,np.clip(bbs.to_xyxy_array(np.float32),1,415),num_real_boxes\n",
    "    \n",
    "    \n",
    "def preprocessing(example_proto):\n",
    "    image_features = tf.io.parse_single_example(example_proto, image_feature_description)\n",
    "    image = tf.image.decode_jpeg(image_features['image_raw'],channels = 3)\n",
    "    image = tf.cast(tf.image.resize(image,size=(416,416)), tf.uint8)\n",
    "    bboxes =  tf.io.parse_tensor(image_features['bboxes'], out_type=tf.float32)\n",
    "    \n",
    "    num_real_boxes = image_features['num_real_boxes']\n",
    "    return image,bboxes,num_real_boxes\n",
    "\n",
    "def preprocessing_validation_set(example_proto):\n",
    "    image_features = tf.io.parse_single_example(example_proto, image_feature_description)\n",
    "    image = tf.image.decode_jpeg(image_features['image_raw'],channels = 3)\n",
    "    image = tf.cast(tf.image.resize(image,size=(416,416)), tf.uint8)\n",
    "    bboxes =  tf.io.parse_tensor(image_features['bboxes'], out_type=tf.float32)\n",
    "    bboxes = tf.clip_by_value(bboxes*416,1,415)\n",
    "    \n",
    "    num_real_boxes = image_features['num_real_boxes']\n",
    "    return image,bboxes,tf.cast(num_real_boxes,tf.int64)\n",
    "    \n",
    "@tf.function(input_signature=[tf.TensorSpec((416,416,3), tf.uint8),tf.TensorSpec((None,4), tf.float32),tf.TensorSpec((), tf.int64)]) \n",
    "def tf_numpy_albumentations_real(image,bboxes,num_real_boxes):\n",
    "    \n",
    "    boxes_shape = bboxes.shape\n",
    "    im_shape = image.shape\n",
    "\n",
    "    image,bboxes,num_real_boxes = tf.numpy_function(imgaug_data_augmentation,[image,bboxes,num_real_boxes],Tout =[tf.uint8,tf.float32,tf.int64])\n",
    " \n",
    "    image.set_shape(im_shape)\n",
    "    bboxes.set_shape(boxes_shape)\n",
    "    print(\"Imagen data type\",image.dtype)\n",
    "    print(\"Bboxes data type\",bboxes.dtype)\n",
    "    print(\"num_real_boxes\",num_real_boxes.dtype)\n",
    "\n",
    "    return image,bboxes,num_real_boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tranfer Learning without Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comentamos la operacion de map en la función de tf_numpy_albumentations y ademas usamos la función preprocessing_validation, esto nos permitirá leer los datos sin data augmentatio, además usaremos la la opcion de mode=\"transfer\" para hacer No entrenables todas las capas menos las de detección"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"ArgMax:0\", dtype=int64)\n",
      "Tensor(\"strided_slice_2:0\", shape=(None,), dtype=int64)\n",
      "Tensor(\"GatherV2:0\", dtype=int64)\n",
      "Tensor(\"strided_slice_3:0\", shape=(None,), dtype=int64)\n",
      "Tensor(\"GatherV2_1:0\", dtype=int64)\n",
      "tipo de aoutput_positivon Tensor(\"add_18:0\", dtype=int64)\n",
      "Tensor(\"add_18:0\", dtype=int64)\n",
      "507\n",
      "tipo de aoutput_positivon Tensor(\"add_30:0\", dtype=int64)\n",
      "Tensor(\"add_30:0\", dtype=int64)\n",
      "2028\n",
      "Tensor(\"ArgMax:0\", dtype=int64)\n",
      "Tensor(\"strided_slice_2:0\", shape=(None,), dtype=int64)\n",
      "Tensor(\"GatherV2:0\", dtype=int64)\n",
      "Tensor(\"strided_slice_3:0\", shape=(None,), dtype=int64)\n",
      "Tensor(\"GatherV2_1:0\", dtype=int64)\n",
      "tipo de aoutput_positivon Tensor(\"add_18:0\", dtype=int64)\n",
      "Tensor(\"add_18:0\", dtype=int64)\n",
      "507\n",
      "tipo de aoutput_positivon Tensor(\"add_30:0\", dtype=int64)\n",
      "Tensor(\"add_30:0\", dtype=int64)\n",
      "2028\n"
     ]
    }
   ],
   "source": [
    "#USANDO TF.IMAGE MODULE\n",
    "#anchors =tf.constant(np.array([[0,0,0.026,0.062],[0,0,0.067,0.183],[0,0,0.128,0.323],[0,0,0.343,0.650]]),dtype=tf.float32)\n",
    "anchors =tf.constant(np.array([[0,0,0.02078,0.049],[0,0,0.0426,0.128],[0,0,0.08523,0.19356],[0,0,0.1506,0.4163],[0,0,0.27835,0.58651],[0,0,0.5632,0.78614]]),dtype=tf.float32)\n",
    "\n",
    "os.chdir(root_path+\"/pedestrian_dataset_train_tfr\")\n",
    "filenames = os. listdir()\n",
    "raw_image_dataset = tf.data.TFRecordDataset(filenames)\n",
    "\n",
    "os.chdir(root_path+\"/pedestrian_dataset_val_tfr_fixed\")\n",
    "filenames = os. listdir()\n",
    "raw_image_dataset_val =tf.data.TFRecordDataset(filenames)\n",
    "\n",
    "os.chdir(root_path+\"/pedestrian_dataset_train_tfr\")\n",
    "\n",
    "train_dataset = raw_image_dataset.map(preprocessing_validation_set,num_parallel_calls=8)\n",
    "#train_dataset = train_dataset.map(tf_numpy_albumentations_real,num_parallel_calls=8)\n",
    "train_dataset = train_dataset.map(lambda x,y,z:build_targets(x,y,z,anchors),num_parallel_calls=8)\n",
    "train_dataset = train_dataset.batch(16)\n",
    "\n",
    "val_dataset = raw_image_dataset_val.map(preprocessing_validation_set,num_parallel_calls=8)\n",
    "val_dataset = val_dataset.map(lambda x,y,z:build_targets(x,y,z,anchors),num_parallel_calls=8)\n",
    "val_dataset = val_dataset.batch(16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.losses import Loss,BinaryCrossentropy,MeanSquaredError,MeanSquaredLogarithmicError\n",
    "\n",
    "def loss_xy(y_true,y_pred):\n",
    "    \n",
    "    mse = MeanSquaredError(reduction=tf.keras.losses.Reduction.NONE)\n",
    "    \n",
    "    tx_true,ty_true,obj_mask = tf.split(y_true, [1,1,1], axis=-1)\n",
    "    tx_pred,ty_pred = tf.split(y_pred, [1,1], axis=-1)\n",
    "    \n",
    "    loss_x = tf.reduce_mean(tf.reduce_sum(obj_mask*(mse(tx_true,tx_pred)[:,:,tf.newaxis]),axis=1))\n",
    "    loss_y = tf.reduce_mean(tf.reduce_sum(obj_mask*(mse(ty_true,ty_pred)[:,:,tf.newaxis]),axis=1))\n",
    "\n",
    "    return loss_x+loss_y\n",
    "\n",
    "def loss_wh(y_true,y_pred):\n",
    "    \n",
    "    mse = MeanSquaredError(reduction=tf.keras.losses.Reduction.NONE)\n",
    "    \n",
    "    tw_true,th_true,obj_mask = tf.split(y_true, [1,1,1], axis=-1)\n",
    "    tw_pred,th_pred = tf.split(y_pred, [1,1], axis=-1)\n",
    "    \n",
    "    loss_w = tf.reduce_mean(tf.reduce_sum(obj_mask*(mse(tw_true,tw_pred)[:,:,tf.newaxis]),axis=1))\n",
    "    loss_h = tf.reduce_mean(tf.reduce_sum(obj_mask*(mse(th_true,th_pred)[:,:,tf.newaxis]),axis=1))\n",
    "\n",
    "\n",
    "    return loss_w+loss_h\n",
    "\n",
    "\n",
    "def loss_objectness(y_true,y_pred):\n",
    "    bce = BinaryCrossentropy(reduction=tf.keras.losses.Reduction.NONE)\n",
    "    \n",
    "    #tw_true,th_true,obj_mask = tf.split(y_true, [1,1,1], axis=-1)\n",
    "    #tw_pred,th_pred = tf.split(y_pred, [1,1], axis=-1)   \n",
    "    \n",
    "    loss_obj =tf.reduce_mean(tf.reduce_sum( y_true*bce(y_true,y_pred)[:,tf.newaxis],axis=1))\n",
    "    \n",
    "    return loss_obj\n",
    "\n",
    "def loss_no_objectness(y_true,y_pred):\n",
    "    bce = BinaryCrossentropy(reduction=tf.keras.losses.Reduction.NONE)\n",
    "    \n",
    "    \n",
    "    loss_noobj =tf.reduce_mean(tf.reduce_sum((1-y_true)*bce(y_true,y_pred)[:,tf.newaxis],axis=1))\n",
    "    \n",
    "    return loss_noobj\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with Model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer leaky_re_lu is casting an input tensor from dtype float16 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float16 by default, call `tf.keras.backend.set_floatx('float16')`. To change just this layer, pass dtype='float16' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "Modo entrenamiento\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f9773503390>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#anchors =tf.constant(np.array([[0.026,0.062],[0.067,0.183],[0.128,0.323],[0.343,0.650]]),dtype=tf.float32)\n",
    "anchors =tf.constant(np.array([[0,0,0.02078,0.049],[0,0,0.0426,0.128],[0,0,0.08523,0.19356],[0,0,0.1506,0.4163],[0,0,0.27835,0.58651],[0,0,0.5632,0.78614]]),dtype=tf.float32)\n",
    "\n",
    "model = TinyYOLOv3(1,anchor_boxes=anchors,train=True,mode = \"transfer\")\n",
    "model.build(batch_input_shape=(None,416,416,3))\n",
    "model.load_weights(root_path+'/weights_saved/pesos_transfer_learning_5521_20_epoch_nadam_0dot00001_mse_3anchors_con_data_aug')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BasicBlock1 False\n",
      "BasicBlock2 False\n",
      "BasicBlock3 False\n",
      "BasicBlock4 False\n",
      "BasicBlock5 False\n",
      "BasicBlock6 False\n",
      "BasicBlock7 False\n",
      "BasicBlock8 False\n",
      "BasicBlock9 False\n",
      "FinalBlock1 True\n",
      "BasicBlock11 False\n",
      "BasicBlock12 False\n",
      "FinalBlock2 True\n",
      "Concatenate True\n",
      "Upsampling True\n",
      "Prediction1 True\n",
      "Prediction2 True\n",
      "Concatenate_BBOX True\n"
     ]
    }
   ],
   "source": [
    "for l in model.layers:\n",
    "    print(l.name, l.trainable)#,l.weights[0].shape)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tiny_yol_ov3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "BasicBlock1 (BasicBlock)     multiple                  496       \n",
      "_________________________________________________________________\n",
      "BasicBlock2 (BasicBlock)     multiple                  4736      \n",
      "_________________________________________________________________\n",
      "BasicBlock3 (BasicBlock)     multiple                  18688     \n",
      "_________________________________________________________________\n",
      "BasicBlock4 (BasicBlock)     multiple                  74240     \n",
      "_________________________________________________________________\n",
      "BasicBlock5 (BasicBlock)     multiple                  295936    \n",
      "_________________________________________________________________\n",
      "BasicBlock6 (BasicBlock)     multiple                  1181696   \n",
      "_________________________________________________________________\n",
      "BasicBlock7 (BasicBlock)     multiple                  4722688   \n",
      "_________________________________________________________________\n",
      "BasicBlock8 (BasicBlock)     multiple                  263168    \n",
      "_________________________________________________________________\n",
      "BasicBlock9 (BasicBlock)     multiple                  1181696   \n",
      "_________________________________________________________________\n",
      "FinalBlock1 (BasicBlock)     multiple                  7695      \n",
      "_________________________________________________________________\n",
      "BasicBlock11 (BasicBlock)    multiple                  33280     \n",
      "_________________________________________________________________\n",
      "BasicBlock12 (BasicBlock)    multiple                  885760    \n",
      "_________________________________________________________________\n",
      "FinalBlock2 (BasicBlock)     multiple                  3855      \n",
      "_________________________________________________________________\n",
      "Concatenate (Concatenate)    multiple                  0         \n",
      "_________________________________________________________________\n",
      "Upsampling (UpSampling2D)    multiple                  0         \n",
      "_________________________________________________________________\n",
      "Prediction1 (PredictionLayer multiple                  0         \n",
      "_________________________________________________________________\n",
      "Prediction2 (PredictionLayer multiple                  0         \n",
      "_________________________________________________________________\n",
      "Concatenate_BBOX (Concatenat multiple                  0         \n",
      "=================================================================\n",
      "Total params: 8,673,934\n",
      "Trainable params: 11,550\n",
      "Non-trainable params: 8,662,384\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(0)\n",
    "\n",
    "from tensorflow.keras.metrics import TrueNegatives,TruePositives,FalseNegatives,FalsePositives,Precision,Recall\n",
    "\n",
    "opt = tf.keras.optimizers.Nadam(learning_rate=1e-4)\n",
    "\n",
    "losses = {\"output_1\": loss_xy,\n",
    "          \"output_2\": loss_wh,\n",
    "          \"output_3\":loss_objectness,\n",
    "          \"output_4\":loss_no_objectness  \n",
    "}\n",
    "\n",
    "metrics = {\"output_3\":[Precision(0.5),Recall(0.5),TrueNegatives(0.5),TruePositives(0.5),FalseNegatives(0.5),FalsePositives(0.5)]}\n",
    "model.compile(optimizer=opt, loss=losses,metrics=metrics,loss_weights=[5,5,2,1])\n",
    "os.chdir(root_path+\"/pedestrian_dataset_train_tfr\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenamiento tipo transfer learning con lr = 0.0001 y por 20 epocas, la función de costo será con mse normal y sin Data Augmentation y ahora con 4 anchors totales, no con 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "Modo entrenamiento\n",
      "Modo entrenamiento\n",
      "   4607/Unknown - 196s 43ms/step - loss: 44.0756 - output_1_loss: 5.5150 - output_2_loss: 0.6571 - output_3_loss: 0.0377 - output_4_loss: 13.1389 - output_3_precision: 0.6143 - output_3_recall: 0.1324 - output_3_true_negatives: 186531488.0000 - output_3_true_positives: 35822.0000 - output_3_false_negatives: 234702.0000 - output_3_false_positives: 22496.0000Modo entrenamiento\n",
      "4607/4607 [==============================] - 203s 44ms/step - loss: 44.0756 - output_1_loss: 5.5150 - output_2_loss: 0.6571 - output_3_loss: 0.0377 - output_4_loss: 13.1389 - output_3_precision: 0.6143 - output_3_recall: 0.1324 - output_3_true_negatives: 186531488.0000 - output_3_true_positives: 35822.0000 - output_3_false_negatives: 234702.0000 - output_3_false_positives: 22496.0000 - val_loss: 45.0617 - val_output_1_loss: 5.6229 - val_output_2_loss: 0.6033 - val_output_3_loss: 0.0416 - val_output_4_loss: 13.8499 - val_output_3_precision: 0.6927 - val_output_3_recall: 0.1035 - val_output_3_true_negatives: 7592633.0000 - val_output_3_true_positives: 1224.0000 - val_output_3_false_negatives: 10600.0000 - val_output_3_false_positives: 543.0000\n",
      "Epoch 2/10\n",
      "4607/4607 [==============================] - 203s 44ms/step - loss: 43.1492 - output_1_loss: 5.4899 - output_2_loss: 0.5127 - output_3_loss: 0.0375 - output_4_loss: 13.0620 - output_3_precision: 0.6099 - output_3_recall: 0.1381 - output_3_true_negatives: 186530112.0000 - output_3_true_positives: 37354.0000 - output_3_false_negatives: 233170.0000 - output_3_false_positives: 23894.0000 - val_loss: 44.6597 - val_output_1_loss: 5.6090 - val_output_2_loss: 0.5455 - val_output_3_loss: 0.0414 - val_output_4_loss: 13.8046 - val_output_3_precision: 0.6898 - val_output_3_recall: 0.1059 - val_output_3_true_negatives: 7592613.0000 - val_output_3_true_positives: 1252.0000 - val_output_3_false_negatives: 10572.0000 - val_output_3_false_positives: 563.0000\n",
      "Epoch 3/10\n",
      "4607/4607 [==============================] - 201s 44ms/step - loss: 42.8737 - output_1_loss: 5.4772 - output_2_loss: 0.4757 - output_3_loss: 0.0374 - output_4_loss: 13.0349 - output_3_precision: 0.6103 - output_3_recall: 0.1395 - output_3_true_negatives: 186530112.0000 - output_3_true_positives: 37736.0000 - output_3_false_negatives: 232788.0000 - output_3_false_positives: 24091.0000 - val_loss: 44.4926 - val_output_1_loss: 5.5993 - val_output_2_loss: 0.5258 - val_output_3_loss: 0.0414 - val_output_4_loss: 13.7841 - val_output_3_precision: 0.6892 - val_output_3_recall: 0.1071 - val_output_3_true_negatives: 7592605.0000 - val_output_3_true_positives: 1266.0000 - val_output_3_false_negatives: 10558.0000 - val_output_3_false_positives: 571.0000\n",
      "Epoch 4/10\n",
      "4607/4607 [==============================] - 201s 44ms/step - loss: 42.7402 - output_1_loss: 5.4681 - output_2_loss: 0.4611 - output_3_loss: 0.0373 - output_4_loss: 13.0196 - output_3_precision: 0.6105 - output_3_recall: 0.1401 - output_3_true_negatives: 186529888.0000 - output_3_true_positives: 37902.0000 - output_3_false_negatives: 232622.0000 - output_3_false_positives: 24185.0000 - val_loss: 44.4082 - val_output_1_loss: 5.5918 - val_output_2_loss: 0.5180 - val_output_3_loss: 0.0413 - val_output_4_loss: 13.7776 - val_output_3_precision: 0.6851 - val_output_3_recall: 0.1071 - val_output_3_true_negatives: 7592594.0000 - val_output_3_true_positives: 1266.0000 - val_output_3_false_negatives: 10558.0000 - val_output_3_false_positives: 582.0000\n",
      "Epoch 5/10\n",
      "4607/4607 [==============================] - 201s 44ms/step - loss: 42.6600 - output_1_loss: 5.4606 - output_2_loss: 0.4546 - output_3_loss: 0.0373 - output_4_loss: 13.0086 - output_3_precision: 0.6101 - output_3_recall: 0.1404 - output_3_true_negatives: 186529792.0000 - output_3_true_positives: 37988.0000 - output_3_false_negatives: 232536.0000 - output_3_false_positives: 24277.0000 - val_loss: 44.3526 - val_output_1_loss: 5.5855 - val_output_2_loss: 0.5145 - val_output_3_loss: 0.0413 - val_output_4_loss: 13.7685 - val_output_3_precision: 0.6840 - val_output_3_recall: 0.1069 - val_output_3_true_negatives: 7592592.0000 - val_output_3_true_positives: 1264.0000 - val_output_3_false_negatives: 10560.0000 - val_output_3_false_positives: 584.0000\n",
      "Epoch 6/10\n",
      "4607/4607 [==============================] - 193s 42ms/step - loss: 42.6044 - output_1_loss: 5.4543 - output_2_loss: 0.4514 - output_3_loss: 0.0373 - output_4_loss: 13.0015 - output_3_precision: 0.6106 - output_3_recall: 0.1408 - output_3_true_negatives: 186529728.0000 - output_3_true_positives: 38078.0000 - output_3_false_negatives: 232446.0000 - output_3_false_positives: 24285.0000 - val_loss: 44.3045 - val_output_1_loss: 5.5800 - val_output_2_loss: 0.5129 - val_output_3_loss: 0.0413 - val_output_4_loss: 13.7592 - val_output_3_precision: 0.6840 - val_output_3_recall: 0.1067 - val_output_3_true_negatives: 7592593.0000 - val_output_3_true_positives: 1262.0000 - val_output_3_false_negatives: 10562.0000 - val_output_3_false_positives: 583.0000\n",
      "Epoch 7/10\n",
      "4607/4607 [==============================] - 194s 42ms/step - loss: 42.5624 - output_1_loss: 5.4485 - output_2_loss: 0.4496 - output_3_loss: 0.0373 - output_4_loss: 12.9972 - output_3_precision: 0.6110 - output_3_recall: 0.1409 - output_3_true_negatives: 186529760.0000 - output_3_true_positives: 38121.0000 - output_3_false_negatives: 232403.0000 - output_3_false_positives: 24272.0000 - val_loss: 44.2705 - val_output_1_loss: 5.5748 - val_output_2_loss: 0.5121 - val_output_3_loss: 0.0413 - val_output_4_loss: 13.7540 - val_output_3_precision: 0.6846 - val_output_3_recall: 0.1066 - val_output_3_true_negatives: 7592595.0000 - val_output_3_true_positives: 1261.0000 - val_output_3_false_negatives: 10563.0000 - val_output_3_false_positives: 581.0000\n",
      "Epoch 8/10\n",
      "4607/4607 [==============================] - 194s 42ms/step - loss: 42.5273 - output_1_loss: 5.4434 - output_2_loss: 0.4486 - output_3_loss: 0.0372 - output_4_loss: 12.9932 - output_3_precision: 0.6113 - output_3_recall: 0.1411 - output_3_true_negatives: 186529856.0000 - output_3_true_positives: 38167.0000 - output_3_false_negatives: 232357.0000 - output_3_false_positives: 24273.0000 - val_loss: 44.2352 - val_output_1_loss: 5.5696 - val_output_2_loss: 0.5115 - val_output_3_loss: 0.0413 - val_output_4_loss: 13.7471 - val_output_3_precision: 0.6836 - val_output_3_recall: 0.1069 - val_output_3_true_negatives: 7592591.0000 - val_output_3_true_positives: 1264.0000 - val_output_3_false_negatives: 10560.0000 - val_output_3_false_positives: 585.0000\n",
      "Epoch 9/10\n",
      "4607/4607 [==============================] - 193s 42ms/step - loss: 42.4965 - output_1_loss: 5.4384 - output_2_loss: 0.4479 - output_3_loss: 0.0372 - output_4_loss: 12.9901 - output_3_precision: 0.6113 - output_3_recall: 0.1412 - output_3_true_negatives: 186529696.0000 - output_3_true_positives: 38199.0000 - output_3_false_negatives: 232325.0000 - output_3_false_positives: 24292.0000 - val_loss: 44.2100 - val_output_1_loss: 5.5648 - val_output_2_loss: 0.5112 - val_output_3_loss: 0.0413 - val_output_4_loss: 13.7487 - val_output_3_precision: 0.6826 - val_output_3_recall: 0.1064 - val_output_3_true_negatives: 7592591.0000 - val_output_3_true_positives: 1258.0000 - val_output_3_false_negatives: 10566.0000 - val_output_3_false_positives: 585.0000\n",
      "Epoch 10/10\n",
      "4607/4607 [==============================] - 194s 42ms/step - loss: 42.4686 - output_1_loss: 5.4339 - output_2_loss: 0.4475 - output_3_loss: 0.0372 - output_4_loss: 12.9873 - output_3_precision: 0.6113 - output_3_recall: 0.1413 - output_3_true_negatives: 186529760.0000 - output_3_true_positives: 38214.0000 - output_3_false_negatives: 232310.0000 - output_3_false_positives: 24300.0000 - val_loss: 44.1851 - val_output_1_loss: 5.5601 - val_output_2_loss: 0.5109 - val_output_3_loss: 0.0413 - val_output_4_loss: 13.7477 - val_output_3_precision: 0.6839 - val_output_3_recall: 0.1065 - val_output_3_true_negatives: 7592594.0000 - val_output_3_true_positives: 1259.0000 - val_output_3_false_negatives: 10565.0000 - val_output_3_false_positives: 582.0000\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_dataset, epochs=10,validation_data=val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"/home/sergio/Documents/json_experiments\")\n",
    "import json\n",
    "#json.dumps(str(a))\n",
    "with open('history_transfer_5521_20_epoch_nadam_0dot00001_mse_2anchors.json', 'w') as fp:\n",
    "    json.dump(str(history.history), fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(root_path+'/weights_saved/pesos_5521_20_30_epoch_nadam_0dot00001_mse_2anchors_tl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CONTINUACION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer leaky_re_lu is casting an input tensor from dtype float16 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float16 by default, call `tf.keras.backend.set_floatx('float16')`. To change just this layer, pass dtype='float16' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "Modo entrenamiento\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7fbcdf319400>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#anchors =tf.constant(np.array([[0.026,0.062],[0.067,0.183],[0.128,0.323],[0.343,0.650]]),dtype=tf.float32)\n",
    "anchors =tf.constant(np.array([[0,0,0.02078,0.049],[0,0,0.0426,0.128],[0,0,0.08523,0.19356],[0,0,0.1506,0.4163],[0,0,0.27835,0.58651],[0,0,0.5632,0.78614]]),dtype=tf.float32)\n",
    "\n",
    "model = TinyYOLOv3(1,anchor_boxes=anchors,train=True,mode = \"transfer\")\n",
    "model.build(batch_input_shape=(None,416,416,3))\n",
    "model.load_weights(root_path+'/weights_saved/pesos_5521_20_30_epoch_nadam_0dot00001_mse_2anchors_tl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(0)\n",
    "\n",
    "from tensorflow.keras.metrics import TrueNegatives,TruePositives,FalseNegatives,FalsePositives,Precision,Recall\n",
    "\n",
    "opt = tf.keras.optimizers.Nadam(learning_rate=1e-4)\n",
    "\n",
    "losses = {\"output_1\": loss_xy,\n",
    "          \"output_2\": loss_wh,\n",
    "          \"output_3\":loss_objectness,\n",
    "          \"output_4\":loss_no_objectness  \n",
    "}\n",
    "\n",
    "metrics = {\"output_3\":[Precision(0.5),Recall(0.5),TrueNegatives(0.5),TruePositives(0.5),FalseNegatives(0.5),FalsePositives(0.5)]}\n",
    "model.compile(optimizer=opt, loss=losses,metrics=metrics,loss_weights=[2,2,3,1])\n",
    "os.chdir(root_path+\"/pedestrian_dataset_train_tfr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "Modo entrenamiento\n",
      "Modo entrenamiento\n",
      "   4607/Unknown - 187s 41ms/step - loss: 24.8305 - output_1_loss: 5.4214 - output_2_loss: 0.4453 - output_3_loss: 0.0372 - output_4_loss: 12.9852 - output_3_precision: 0.6113 - output_3_recall: 0.1414 - output_3_true_negatives: 186529696.0000 - output_3_true_positives: 38240.0000 - output_3_false_negatives: 232284.0000 - output_3_false_positives: 24314.0000Modo entrenamiento\n",
      "4607/4607 [==============================] - 194s 42ms/step - loss: 24.8305 - output_1_loss: 5.4214 - output_2_loss: 0.4453 - output_3_loss: 0.0372 - output_4_loss: 12.9852 - output_3_precision: 0.6113 - output_3_recall: 0.1414 - output_3_true_negatives: 186529696.0000 - output_3_true_positives: 38240.0000 - output_3_false_negatives: 232284.0000 - output_3_false_positives: 24314.0000 - val_loss: 26.0017 - val_output_1_loss: 5.5574 - val_output_2_loss: 0.5106 - val_output_3_loss: 0.0412 - val_output_4_loss: 13.7422 - val_output_3_precision: 0.6848 - val_output_3_recall: 0.1066 - val_output_3_true_negatives: 7592596.0000 - val_output_3_true_positives: 1260.0000 - val_output_3_false_negatives: 10564.0000 - val_output_3_false_positives: 580.0000\n",
      "Epoch 2/10\n",
      "4607/4607 [==============================] - 198s 43ms/step - loss: 24.8427 - output_1_loss: 5.4265 - output_2_loss: 0.4470 - output_3_loss: 0.0372 - output_4_loss: 12.9840 - output_3_precision: 0.6112 - output_3_recall: 0.1414 - output_3_true_negatives: 186529568.0000 - output_3_true_positives: 38260.0000 - output_3_false_negatives: 232264.0000 - output_3_false_positives: 24338.0000 - val_loss: 25.9908 - val_output_1_loss: 5.5528 - val_output_2_loss: 0.5107 - val_output_3_loss: 0.0412 - val_output_4_loss: 13.7406 - val_output_3_precision: 0.6854 - val_output_3_recall: 0.1065 - val_output_3_true_negatives: 7592598.0000 - val_output_3_true_positives: 1259.0000 - val_output_3_false_negatives: 10565.0000 - val_output_3_false_positives: 578.0000\n",
      "Epoch 3/10\n",
      "4607/4607 [==============================] - 196s 43ms/step - loss: 24.8327 - output_1_loss: 5.4223 - output_2_loss: 0.4468 - output_3_loss: 0.0372 - output_4_loss: 12.9828 - output_3_precision: 0.6113 - output_3_recall: 0.1415 - output_3_true_negatives: 186529536.0000 - output_3_true_positives: 38279.0000 - output_3_false_negatives: 232245.0000 - output_3_false_positives: 24340.0000 - val_loss: 25.9837 - val_output_1_loss: 5.5487 - val_output_2_loss: 0.5106 - val_output_3_loss: 0.0412 - val_output_4_loss: 13.7422 - val_output_3_precision: 0.6865 - val_output_3_recall: 0.1065 - val_output_3_true_negatives: 7592601.0000 - val_output_3_true_positives: 1259.0000 - val_output_3_false_negatives: 10565.0000 - val_output_3_false_positives: 575.0000\n",
      "Epoch 4/10\n",
      "4607/4607 [==============================] - 198s 43ms/step - loss: 24.8227 - output_1_loss: 5.4182 - output_2_loss: 0.4467 - output_3_loss: 0.0372 - output_4_loss: 12.9812 - output_3_precision: 0.6114 - output_3_recall: 0.1415 - output_3_true_negatives: 186529536.0000 - output_3_true_positives: 38284.0000 - output_3_false_negatives: 232240.0000 - output_3_false_positives: 24333.0000 - val_loss: 25.9703 - val_output_1_loss: 5.5445 - val_output_2_loss: 0.5106 - val_output_3_loss: 0.0412 - val_output_4_loss: 13.7376 - val_output_3_precision: 0.6850 - val_output_3_recall: 0.1066 - val_output_3_true_negatives: 7592596.0000 - val_output_3_true_positives: 1261.0000 - val_output_3_false_negatives: 10563.0000 - val_output_3_false_positives: 580.0000\n",
      "Epoch 5/10\n",
      "4607/4607 [==============================] - 192s 42ms/step - loss: 24.8131 - output_1_loss: 5.4143 - output_2_loss: 0.4466 - output_3_loss: 0.0372 - output_4_loss: 12.9799 - output_3_precision: 0.6115 - output_3_recall: 0.1415 - output_3_true_negatives: 186529568.0000 - output_3_true_positives: 38273.0000 - output_3_false_negatives: 232251.0000 - output_3_false_positives: 24316.0000 - val_loss: 25.9626 - val_output_1_loss: 5.5407 - val_output_2_loss: 0.5105 - val_output_3_loss: 0.0412 - val_output_4_loss: 13.7375 - val_output_3_precision: 0.6868 - val_output_3_recall: 0.1066 - val_output_3_true_negatives: 7592601.0000 - val_output_3_true_positives: 1261.0000 - val_output_3_false_negatives: 10563.0000 - val_output_3_false_positives: 575.0000\n",
      "Epoch 6/10\n",
      "4607/4607 [==============================] - 201s 44ms/step - loss: 24.8053 - output_1_loss: 5.4105 - output_2_loss: 0.4465 - output_3_loss: 0.0372 - output_4_loss: 12.9798 - output_3_precision: 0.6118 - output_3_recall: 0.1416 - output_3_true_negatives: 186529664.0000 - output_3_true_positives: 38297.0000 - output_3_false_negatives: 232227.0000 - output_3_false_positives: 24302.0000 - val_loss: 25.9539 - val_output_1_loss: 5.5371 - val_output_2_loss: 0.5104 - val_output_3_loss: 0.0412 - val_output_4_loss: 13.7359 - val_output_3_precision: 0.6863 - val_output_3_recall: 0.1064 - val_output_3_true_negatives: 7592601.0000 - val_output_3_true_positives: 1258.0000 - val_output_3_false_negatives: 10566.0000 - val_output_3_false_positives: 575.0000\n",
      "Epoch 7/10\n",
      "4607/4607 [==============================] - 201s 44ms/step - loss: 24.7965 - output_1_loss: 5.4068 - output_2_loss: 0.4464 - output_3_loss: 0.0372 - output_4_loss: 12.9786 - output_3_precision: 0.6118 - output_3_recall: 0.1415 - output_3_true_negatives: 186529760.0000 - output_3_true_positives: 38291.0000 - output_3_false_negatives: 232233.0000 - output_3_false_positives: 24294.0000 - val_loss: 25.9459 - val_output_1_loss: 5.5328 - val_output_2_loss: 0.5104 - val_output_3_loss: 0.0412 - val_output_4_loss: 13.7374 - val_output_3_precision: 0.6859 - val_output_3_recall: 0.1064 - val_output_3_true_negatives: 7592600.0000 - val_output_3_true_positives: 1258.0000 - val_output_3_false_negatives: 10566.0000 - val_output_3_false_positives: 576.0000\n",
      "Epoch 8/10\n",
      "4607/4607 [==============================] - 206s 45ms/step - loss: 24.7881 - output_1_loss: 5.4031 - output_2_loss: 0.4464 - output_3_loss: 0.0372 - output_4_loss: 12.9775 - output_3_precision: 0.6119 - output_3_recall: 0.1417 - output_3_true_negatives: 186529728.0000 - output_3_true_positives: 38327.0000 - output_3_false_negatives: 232197.0000 - output_3_false_positives: 24309.0000 - val_loss: 25.9374 - val_output_1_loss: 5.5290 - val_output_2_loss: 0.5104 - val_output_3_loss: 0.0412 - val_output_4_loss: 13.7355 - val_output_3_precision: 0.6884 - val_output_3_recall: 0.1065 - val_output_3_true_negatives: 7592606.0000 - val_output_3_true_positives: 1259.0000 - val_output_3_false_negatives: 10565.0000 - val_output_3_false_positives: 570.0000\n",
      "Epoch 9/10\n",
      "4607/4607 [==============================] - 206s 45ms/step - loss: 24.7801 - output_1_loss: 5.3996 - output_2_loss: 0.4463 - output_3_loss: 0.0372 - output_4_loss: 12.9767 - output_3_precision: 0.6119 - output_3_recall: 0.1417 - output_3_true_negatives: 186529728.0000 - output_3_true_positives: 38324.0000 - output_3_false_negatives: 232200.0000 - output_3_false_positives: 24303.0000 - val_loss: 25.9314 - val_output_1_loss: 5.5253 - val_output_2_loss: 0.5103 - val_output_3_loss: 0.0412 - val_output_4_loss: 13.7367 - val_output_3_precision: 0.6885 - val_output_3_recall: 0.1066 - val_output_3_true_negatives: 7592606.0000 - val_output_3_true_positives: 1260.0000 - val_output_3_false_negatives: 10564.0000 - val_output_3_false_positives: 570.0000\n",
      "Epoch 10/10\n",
      "4607/4607 [==============================] - 205s 45ms/step - loss: 24.7719 - output_1_loss: 5.3961 - output_2_loss: 0.4463 - output_3_loss: 0.0372 - output_4_loss: 12.9757 - output_3_precision: 0.6122 - output_3_recall: 0.1417 - output_3_true_negatives: 186529792.0000 - output_3_true_positives: 38331.0000 - output_3_false_negatives: 232193.0000 - output_3_false_positives: 24286.0000 - val_loss: 25.9219 - val_output_1_loss: 5.5217 - val_output_2_loss: 0.5103 - val_output_3_loss: 0.0412 - val_output_4_loss: 13.7344 - val_output_3_precision: 0.6879 - val_output_3_recall: 0.1066 - val_output_3_true_negatives: 7592604.0000 - val_output_3_true_positives: 1261.0000 - val_output_3_false_negatives: 10563.0000 - val_output_3_false_positives: 572.0000\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_dataset, epochs=10,validation_data=val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(root_path+'/weights_saved/pesos_5521_30_40_epoch_nadam_0dot00001_mse_3anchors_tl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(0)\n",
    "\n",
    "from tensorflow.keras.metrics import TrueNegatives,TruePositives,FalseNegatives,FalsePositives,Precision,Recall\n",
    "\n",
    "opt = tf.keras.optimizers.Nadam(learning_rate=1e-5)\n",
    "\n",
    "losses = {\"output_1\": loss_xy,\n",
    "          \"output_2\": loss_wh,\n",
    "          \"output_3\":loss_objectness,\n",
    "          \"output_4\":loss_no_objectness  \n",
    "}\n",
    "\n",
    "metrics = {\"output_3\":[Precision(0.5),Recall(0.5),TrueNegatives(0.5),TruePositives(0.5),FalseNegatives(0.5),FalsePositives(0.5)]}\n",
    "model.compile(optimizer=opt, loss=losses,metrics=metrics,loss_weights=[2,2,3,1])\n",
    "os.chdir(root_path+\"/pedestrian_dataset_train_tfr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "Modo entrenamiento\n",
      "Modo entrenamiento\n",
      "   4606/Unknown - 196s 43ms/step - loss: 24.7161 - output_1_loss: 5.3660 - output_2_loss: 0.4397 - output_3_loss: 0.0373 - output_4_loss: 12.9927 - output_3_precision_1: 0.6200 - output_3_recall_1: 0.1363 - output_3_true_negatives_1: 186526272.0000 - output_3_true_positives_1: 36878.0000 - output_3_false_negatives_1: 233640.0000 - output_3_false_positives_1: 22604.0000Modo entrenamiento\n",
      "4607/4607 [==============================] - 203s 44ms/step - loss: 24.7159 - output_1_loss: 5.3660 - output_2_loss: 0.4397 - output_3_loss: 0.0373 - output_4_loss: 12.9927 - output_3_precision_1: 0.6200 - output_3_recall_1: 0.1363 - output_3_true_negatives_1: 186531328.0000 - output_3_true_positives_1: 36878.0000 - output_3_false_negatives_1: 233646.0000 - output_3_false_positives_1: 22604.0000 - val_loss: 25.6000 - val_output_1_loss: 5.4981 - val_output_2_loss: 0.4940 - val_output_3_loss: 0.0402 - val_output_4_loss: 13.4948 - val_output_3_precision_1: 0.6356 - val_output_3_recall_1: 0.1467 - val_output_3_true_negatives_1: 7592182.0000 - val_output_3_true_positives_1: 1734.0000 - val_output_3_false_negatives_1: 10090.0000 - val_output_3_false_positives_1: 994.0000\n",
      "Epoch 2/10\n",
      "4607/4607 [==============================] - 201s 44ms/step - loss: 24.6910 - output_1_loss: 5.3617 - output_2_loss: 0.4387 - output_3_loss: 0.0371 - output_4_loss: 12.9787 - output_3_precision_1: 0.6137 - output_3_recall_1: 0.1414 - output_3_true_negatives_1: 186529856.0000 - output_3_true_positives_1: 38242.0000 - output_3_false_negatives_1: 232282.0000 - output_3_false_positives_1: 24076.0000 - val_loss: 25.5953 - val_output_1_loss: 5.4960 - val_output_2_loss: 0.4939 - val_output_3_loss: 0.0402 - val_output_4_loss: 13.4953 - val_output_3_precision_1: 0.6361 - val_output_3_recall_1: 0.1467 - val_output_3_true_negatives_1: 7592184.0000 - val_output_3_true_positives_1: 1734.0000 - val_output_3_false_negatives_1: 10090.0000 - val_output_3_false_positives_1: 992.0000\n",
      "Epoch 3/10\n",
      "4607/4607 [==============================] - 199s 43ms/step - loss: 24.6888 - output_1_loss: 5.3605 - output_2_loss: 0.4386 - output_3_loss: 0.0371 - output_4_loss: 12.9791 - output_3_precision_1: 0.6138 - output_3_recall_1: 0.1413 - output_3_true_negatives_1: 186529888.0000 - output_3_true_positives_1: 38217.0000 - output_3_false_negatives_1: 232307.0000 - output_3_false_positives_1: 24042.0000 - val_loss: 25.5904 - val_output_1_loss: 5.4949 - val_output_2_loss: 0.4939 - val_output_3_loss: 0.0402 - val_output_4_loss: 13.4927 - val_output_3_precision_1: 0.6361 - val_output_3_recall_1: 0.1462 - val_output_3_true_negatives_1: 7592187.0000 - val_output_3_true_positives_1: 1729.0000 - val_output_3_false_negatives_1: 10095.0000 - val_output_3_false_positives_1: 989.0000\n",
      "Epoch 4/10\n",
      "4607/4607 [==============================] - 202s 44ms/step - loss: 24.6864 - output_1_loss: 5.3597 - output_2_loss: 0.4385 - output_3_loss: 0.0371 - output_4_loss: 12.9787 - output_3_precision_1: 0.6139 - output_3_recall_1: 0.1412 - output_3_true_negatives_1: 186529888.0000 - output_3_true_positives_1: 38201.0000 - output_3_false_negatives_1: 232323.0000 - output_3_false_positives_1: 24027.0000 - val_loss: 25.5887 - val_output_1_loss: 5.4943 - val_output_2_loss: 0.4939 - val_output_3_loss: 0.0402 - val_output_4_loss: 13.4925 - val_output_3_precision_1: 0.6361 - val_output_3_recall_1: 0.1461 - val_output_3_true_negatives_1: 7592188.0000 - val_output_3_true_positives_1: 1727.0000 - val_output_3_false_negatives_1: 10097.0000 - val_output_3_false_positives_1: 988.0000\n",
      "Epoch 5/10\n",
      "4607/4607 [==============================] - 203s 44ms/step - loss: 24.6850 - output_1_loss: 5.3592 - output_2_loss: 0.4384 - output_3_loss: 0.0371 - output_4_loss: 12.9783 - output_3_precision_1: 0.6139 - output_3_recall_1: 0.1411 - output_3_true_negatives_1: 186529952.0000 - output_3_true_positives_1: 38183.0000 - output_3_false_negatives_1: 232341.0000 - output_3_false_positives_1: 24014.0000 - val_loss: 25.5887 - val_output_1_loss: 5.4934 - val_output_2_loss: 0.4939 - val_output_3_loss: 0.0402 - val_output_4_loss: 13.4934 - val_output_3_precision_1: 0.6362 - val_output_3_recall_1: 0.1461 - val_output_3_true_negatives_1: 7592188.0000 - val_output_3_true_positives_1: 1728.0000 - val_output_3_false_negatives_1: 10096.0000 - val_output_3_false_positives_1: 988.0000\n",
      "Epoch 6/10\n",
      "4607/4607 [==============================] - 205s 44ms/step - loss: 24.6841 - output_1_loss: 5.3586 - output_2_loss: 0.4384 - output_3_loss: 0.0371 - output_4_loss: 12.9786 - output_3_precision_1: 0.6139 - output_3_recall_1: 0.1411 - output_3_true_negatives_1: 186529920.0000 - output_3_true_positives_1: 38175.0000 - output_3_false_negatives_1: 232349.0000 - output_3_false_positives_1: 24007.0000 - val_loss: 25.5907 - val_output_1_loss: 5.4934 - val_output_2_loss: 0.4939 - val_output_3_loss: 0.0402 - val_output_4_loss: 13.4950 - val_output_3_precision_1: 0.6366 - val_output_3_recall_1: 0.1462 - val_output_3_true_negatives_1: 7592189.0000 - val_output_3_true_positives_1: 1729.0000 - val_output_3_false_negatives_1: 10095.0000 - val_output_3_false_positives_1: 987.0000\n",
      "Epoch 7/10\n",
      "4607/4607 [==============================] - 203s 44ms/step - loss: 24.6825 - output_1_loss: 5.3582 - output_2_loss: 0.4384 - output_3_loss: 0.0371 - output_4_loss: 12.9781 - output_3_precision_1: 0.6139 - output_3_recall_1: 0.1411 - output_3_true_negatives_1: 186529920.0000 - output_3_true_positives_1: 38182.0000 - output_3_false_negatives_1: 232342.0000 - output_3_false_positives_1: 24018.0000 - val_loss: 25.5851 - val_output_1_loss: 5.4925 - val_output_2_loss: 0.4938 - val_output_3_loss: 0.0402 - val_output_4_loss: 13.4926 - val_output_3_precision_1: 0.6361 - val_output_3_recall_1: 0.1461 - val_output_3_true_negatives_1: 7592188.0000 - val_output_3_true_positives_1: 1727.0000 - val_output_3_false_negatives_1: 10097.0000 - val_output_3_false_positives_1: 988.0000\n",
      "Epoch 8/10\n",
      "4607/4607 [==============================] - 202s 44ms/step - loss: 24.6813 - output_1_loss: 5.3578 - output_2_loss: 0.4384 - output_3_loss: 0.0371 - output_4_loss: 12.9778 - output_3_precision_1: 0.6139 - output_3_recall_1: 0.1411 - output_3_true_negatives_1: 186529920.0000 - output_3_true_positives_1: 38178.0000 - output_3_false_negatives_1: 232346.0000 - output_3_false_positives_1: 24008.0000 - val_loss: 25.5853 - val_output_1_loss: 5.4920 - val_output_2_loss: 0.4938 - val_output_3_loss: 0.0402 - val_output_4_loss: 13.4925 - val_output_3_precision_1: 0.6360 - val_output_3_recall_1: 0.1460 - val_output_3_true_negatives_1: 7592188.0000 - val_output_3_true_positives_1: 1726.0000 - val_output_3_false_negatives_1: 10098.0000 - val_output_3_false_positives_1: 988.0000\n",
      "Epoch 9/10\n",
      "4607/4607 [==============================] - 202s 44ms/step - loss: 24.6806 - output_1_loss: 5.3574 - output_2_loss: 0.4383 - output_3_loss: 0.0371 - output_4_loss: 12.9778 - output_3_precision_1: 0.6138 - output_3_recall_1: 0.1411 - output_3_true_negatives_1: 186529952.0000 - output_3_true_positives_1: 38181.0000 - output_3_false_negatives_1: 232343.0000 - output_3_false_positives_1: 24019.0000 - val_loss: 25.5852 - val_output_1_loss: 5.4917 - val_output_2_loss: 0.4938 - val_output_3_loss: 0.0402 - val_output_4_loss: 13.4938 - val_output_3_precision_1: 0.6362 - val_output_3_recall_1: 0.1460 - val_output_3_true_negatives_1: 7592189.0000 - val_output_3_true_positives_1: 1726.0000 - val_output_3_false_negatives_1: 10098.0000 - val_output_3_false_positives_1: 987.0000\n",
      "Epoch 10/10\n",
      "4607/4607 [==============================] - 203s 44ms/step - loss: 24.6803 - output_1_loss: 5.3570 - output_2_loss: 0.4383 - output_3_loss: 0.0371 - output_4_loss: 12.9781 - output_3_precision_1: 0.6140 - output_3_recall_1: 0.1411 - output_3_true_negatives_1: 186529952.0000 - output_3_true_positives_1: 38177.0000 - output_3_false_negatives_1: 232347.0000 - output_3_false_positives_1: 24005.0000 - val_loss: 25.5830 - val_output_1_loss: 5.4909 - val_output_2_loss: 0.4938 - val_output_3_loss: 0.0402 - val_output_4_loss: 13.4931 - val_output_3_precision_1: 0.6361 - val_output_3_recall_1: 0.1461 - val_output_3_true_negatives_1: 7592188.0000 - val_output_3_true_positives_1: 1727.0000 - val_output_3_false_negatives_1: 10097.0000 - val_output_3_false_positives_1: 988.0000\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_dataset, epochs=10,validation_data=val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(root_path+'/weights_saved/pesos_5521_40_50_epoch_nadam_0dot000001_mse_3anchors_tl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
