{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = \"/tf/home/sergio/Tesis\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(root_path+\"/TinyYOLOv3-Pedestrian-Detection\")\n",
    "\n",
    "from YOLOfunctional import TinyYOLOv3_functional,nms_layer\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "#from tensorflow.python.tools import freeze_graph\n",
    "#from skimage.io import imread,imshow\n",
    "#from skimage.transform import resize \n",
    "import time\n",
    "#from tensorflow.compat.v1.image import decode_image\n",
    "import imgaug.augmenters as iaa\n",
    "from imgaug.augmentables.bbs import BoundingBox, BoundingBoxesOnImage\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_feature_description = {\n",
    "    'bboxes': tf.io.FixedLenFeature([], tf.string),\n",
    "    'image_raw': tf.io.FixedLenFeature([], tf.string),\n",
    "    'num_real_boxes':tf.io.FixedLenFeature([], tf.int64),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_iou_matrix_tf(box_arr1, box_arr2):\n",
    "    \n",
    "    box_arr1 = box_arr1 -tf.tile(box_arr1[:,:2],[1,2])\n",
    "    #print(box_arr1)\n",
    "    x11, y11, x12, y12 = tf.split(box_arr1, 4, axis=1)\n",
    "    x21, y21, x22, y22 = tf.split(box_arr2, 4, axis=1)\n",
    "    xA = tf.maximum(x11, tf.transpose(x21))\n",
    "    yA = tf.maximum(y11, tf.transpose(y21))\n",
    "    xB = tf.minimum(x12, tf.transpose(x22))\n",
    "    yB = tf.minimum(y12, tf.transpose(y22))\n",
    "    interArea = tf.maximum((xB - xA + 1e-9), 0) * tf.maximum((yB - yA + 1e-9), 0)\n",
    "    boxAArea = (x12 - x11 + 1e-9) * (y12 - y11 + 1e-9)\n",
    "    boxBArea = (x22 - x21 + 1e-9) * (y22 - y21 + 1e-9)\n",
    "    iou = interArea / (boxAArea + tf.transpose(boxBArea) - interArea)\n",
    "    return iou,tf.argmax(iou,axis=1)#[:,tf.newaxis]\n",
    "\n",
    "\n",
    "def fill_yolo_output(boxes,grid_size,num_anchors,which_anchor_box,which_anchor_box_index):\n",
    "    #print(boxes.shape)\n",
    "    #noobj_mask = tf.ones((1,grid_size*grid_size*num_anchors))\n",
    "    #print(noobj_mask.shape)\n",
    "    \n",
    "    x_min,y_min,x_max,y_max =tf.split(boxes,4,axis=1)\n",
    "\n",
    "    #Transforma las coordenadas de (xmin,ymin,xmax,ymax) --> (xcenter,ycenter,width,height)\n",
    "    width = x_max-x_min\n",
    "    height = y_max-y_min\n",
    "    x_global =x_min + tf.math.divide(x_max - x_min,2)\n",
    "    y_global =y_min + tf.math.divide(y_max - y_min,2)\n",
    "    \n",
    "    \n",
    "    x_min_anchor,y_min_anchor,x_max_anchor,y_max_anchor =tf.split(which_anchor_box,4,axis=1)\n",
    "    \n",
    "    width_anchor = x_max_anchor-x_min_anchor\n",
    "    height_anchor = y_max_anchor-y_min_anchor\n",
    "    x_global_anchor =x_min_anchor + tf.math.divide(x_max_anchor - x_min_anchor,2)\n",
    "    y_global_anchor =y_min_anchor + tf.math.divide(y_max_anchor - y_min_anchor,2)   \n",
    "\n",
    "    \n",
    "    #print(\"el x original\",x_global)\n",
    "    #print(\"el y original\",y_global)\n",
    "    #print(\"el w original\",width)\n",
    "    #print(\"el h original\",height)\n",
    "    \n",
    "    #porción de la imagen que hay en cada celda\n",
    "    pixel_per_grid = tf.math.divide(1.,grid_size)\n",
    "    #print(pixel_per_grid)\n",
    "    \n",
    "    #Obtenemos la coordenada de la celda donde están los boundingboxes\n",
    "    offset_grid_x = x_global//pixel_per_grid\n",
    "    offset_grid_y = y_global//pixel_per_grid\n",
    "    \n",
    "    #Obtenemos el el centro locacon referencia  al celda encontrada previamente\n",
    "    x_local =tf.math.floormod(x_global,pixel_per_grid)\n",
    "    y_local =tf.math.floormod(y_global,pixel_per_grid)\n",
    "    #print(x_local,y_local)\n",
    "    \n",
    "    #Valores tx e ty del groudtruth\n",
    "    tx = tf.math.log(x_local + 1e-07/(1-x_local))\n",
    "    ty = tf.math.log(y_local+1e-07/(1-y_local))\n",
    "    tw = tf.math.log(tf.math.divide(width+1e-07,width_anchor))\n",
    "    th = tf.math.log(tf.math.divide(height+1e-07,height_anchor))\n",
    "    tobj_mask = tf.ones_like(tx)\n",
    "    tobj = tf.concat([tobj_mask,tobj_mask],axis=0)\n",
    "    \n",
    "    #tnoobj = tf.zeros_like(tx)    \n",
    "    #tobj = tf.ones((grid_size*grid_size*num_anchors,1))\n",
    "    #tnoobj = tf.zeros((grid_size*grid_size*num_anchors,1))\n",
    "    #print(\"Lo que la red debe predecir\",tx.numpy(),ty.numpy(),tw.numpy(),th.numpy())\n",
    "    #x_global = (offset_grid_x * pixel_per_grid) + tf.math.sigmoid(tx)\n",
    "    #y_global = (offset_grid_y * pixel_per_grid) + tf.math.sigmoid(ty)\n",
    "    #w = width_anchor*tf.math.exp(tw)\n",
    "    #h = height_anchor*tf.math.exp(th)\n",
    "    #print(\"obtnemos el x_real\",x_global)\n",
    "    #print(\"obtenemos el y_real\",y_global)\n",
    "    #print(\"obtenemos el w real\",w)\n",
    "    #print(\"obtenemos el h real\",h)\n",
    "    \n",
    "    #anchor_boxes_per_output = num_anchors//2\n",
    "\n",
    "    #Residuo indica cual de los 3 anchor boxes de la coordenada es la que llevara el 1\n",
    "    #Coord representa la coordenada del grid\n",
    "    \n",
    "    residuo = tf.math.floormod(which_anchor_box_index,num_anchors)[:,tf.newaxis]\n",
    "    coord = tf.cast(num_anchors*(offset_grid_y*grid_size + offset_grid_x),dtype=tf.int64)\n",
    "    \n",
    "    coord_objectness = tf.cast(2*(offset_grid_y*grid_size + offset_grid_x),dtype=tf.int64)\n",
    "    coord_objectness2 = coord_objectness+1\n",
    "    coord_objectess_global = tf.concat([coord_objectness,coord_objectness2],axis=0)\n",
    "    \n",
    "    output_position = residuo+coord\n",
    "    print(\"tipo de aoutput_positivon\",output_position)\n",
    "    \n",
    "    print(output_position)\n",
    "    \n",
    "    dense_shape = grid_size*grid_size*num_anchors\n",
    "    print(dense_shape)\n",
    "    tx_vector = tf.sparse.reorder(tf.sparse.SparseTensor(indices=output_position, values=tx[:,0], dense_shape=[dense_shape]))\n",
    "    ty_vector = tf.sparse.reorder(tf.sparse.SparseTensor(indices=output_position, values=ty[:,0], dense_shape=[dense_shape]))\n",
    "    tw_vector = tf.sparse.reorder(tf.sparse.SparseTensor(indices=output_position, values=tw[:,0], dense_shape=[dense_shape]))\n",
    "    th_vector = tf.sparse.reorder(tf.sparse.SparseTensor(indices=output_position, values=th[:,0], dense_shape=[dense_shape]))\n",
    "    obj_mask = tf.sparse.reorder(tf.sparse.SparseTensor(indices=output_position, values=tobj_mask[:,0], dense_shape=[dense_shape]))\n",
    "    objectness_vector = tf.sparse.reorder(tf.sparse.SparseTensor(indices=coord_objectess_global, values=tobj[:,0], dense_shape=[dense_shape]))\n",
    "    #noobj_mask = tf.sparse.reorder(tf.sparse.SparseTensor(indices=output_position, values=tnoobj[:,0], dense_shape=[dense_shape]))\n",
    "    #obj_mask =tx_vector=ty_vector=tw_vector=th_vector = tf.zeros((1,grid_size*grid_size*num_anchors))\n",
    "    \n",
    "    tx_vector_dense = tf.sparse.to_dense(tx_vector, default_value=0, validate_indices=False, name=\"Dense_tx\")\n",
    "    ty_vector_dense = tf.sparse.to_dense(ty_vector, default_value=0, validate_indices=False, name=\"Dense_ty\")\n",
    "    tw_vector_dense = tf.sparse.to_dense(tw_vector, default_value=0, validate_indices=False, name=\"Dense_tw\")\n",
    "    th_vector_dense = tf.sparse.to_dense(th_vector, default_value=0, validate_indices=False, name=\"Dense_th\")\n",
    "    obj_mask_dense =  tf.sparse.to_dense(obj_mask, default_value=0, validate_indices=False, name=\"Dense_obj\")\n",
    "    #noobj_mask_dense = 1-obj_mask_dense\n",
    "    objectness_vector_dense =  tf.sparse.to_dense(objectness_vector, default_value=0, validate_indices=False)\n",
    "    \n",
    "    #noobj_mask_dense= tf.sparse.to_dense(noobj_mask, default_value=1, validate_indices=False, name=\"Dense_noobj\")\n",
    "    ##print(tx_vector.to_dense)\n",
    "    #print(tf.sparse.to_dense(tx_vector, default_value=0, validate_indices=True, name=None)\n",
    "    #tx_vector=tx_vector[[3,2],]\n",
    "    #tx_vector[output_position[:,0]] = tx\n",
    "    #print(\"coordenada de la salida:\",output_position)\n",
    "    \n",
    "    #return ((tx_vector_dense,ty_vector_dense,obj_mask_dense),(tw_vector_dense,th_vector_dense,obj_mask_dense),(objectness),(objectness))\n",
    "    \n",
    "    return tx_vector_dense,ty_vector_dense,tw_vector_dense,th_vector_dense,obj_mask_dense,objectness_vector_dense\n",
    "\n",
    "def build_targets(image,image_bboxes,num_real_boxes,anchor_boxes):\n",
    "    \n",
    "    images_bboxes_original = image_bboxes\n",
    "    #Obtenemos los boduing boxes que son reales\n",
    "    image_bboxes = image_bboxes[:num_real_boxes,:]\n",
    "    #print(\"Bouding boxes de la imagen\",image_bboxes)\n",
    "    #Obteneos  la matriz de IoU , y el índice del anchor box que dió mejor resultado\n",
    "    \n",
    "    #Nprmalizamos con respecto al tamaño de la imagen y obtenemos la Iou con los anchor boxes\n",
    "    image_bboxes = tf.math.divide(image_bboxes,416)\n",
    "    iou_matrix,which_anchor_box_index = get_iou_matrix_tf(image_bboxes,anchor_boxes)\n",
    "    \n",
    "    print(which_anchor_box_index)\n",
    "\n",
    "    anchor_boxes_per_output = len(anchor_boxes)//2\n",
    "    #Indices de los bouding boxes que irian en cada salida, index_best_ yolo nos dice que bouding boxes de la imagen van a la salida YOLO1,\n",
    "    #porque su mejor IoU fue con los len(anchor_boxes)//2 anchor boxes mas grandes\n",
    "    index_best_yolo1 = tf.where(which_anchor_box_index>=anchor_boxes_per_output)[:,0]\n",
    "    index_best_yolo2 = tf.where(which_anchor_box_index<anchor_boxes_per_output)[:,0]\n",
    "    index_best_anchor_yolo1 = tf.gather(which_anchor_box_index,index_best_yolo1,axis=0)\n",
    "    index_best_anchor_yolo2 = tf.gather(which_anchor_box_index,index_best_yolo2,axis=0)\n",
    "    \n",
    "    print(index_best_yolo1)\n",
    "    print(index_best_anchor_yolo1)\n",
    "\n",
    "    print(index_best_yolo2)\n",
    "    print(index_best_anchor_yolo2)\n",
    "\n",
    "    \n",
    "    best_bboxes_yolo1 = tf.gather(image_bboxes,index_best_yolo1,axis =0)\n",
    "    best_anchors_yolo1 = tf.gather(anchor_boxes,index_best_anchor_yolo1, axis =0) #LOs dos anchor boxes grandes corrsponden a YOLO1\n",
    "    best_bboxes_yolo2 = tf.gather(image_bboxes,index_best_yolo2,axis =0)\n",
    "    best_anchors_yolo2 = tf.gather(anchor_boxes,index_best_anchor_yolo2, axis =0) #Los dos anchor boxes pequeños corresponden a YOLO2\n",
    "    \n",
    "    \n",
    "    if best_anchors_yolo1.shape[0] !=0:\n",
    "        tx_vector_yolo1,ty_vector_yolo1,tw_vector_yolo1,th_vector_yolo1,obj_mask_yolo1,obj_vector_yolo1= fill_yolo_output(best_bboxes_yolo1,13,anchor_boxes_per_output,best_anchors_yolo1,index_best_anchor_yolo1)\n",
    "    else:\n",
    "        tx_vector_yolo1=ty_vector_yolo1=tw_vector_yolo1=th_vector_yolo1=obj_mask_yolo1= obj_vector_yolo1=tf.zeros((1,grid_size*grid_size*num_anchors))\n",
    "        #noobj_mask_yolo1 = tf.ones((1,13*13*num_anchors))\n",
    "    \n",
    "    if best_anchors_yolo2.shape[0] != 0:\n",
    "        tx_vector_yolo2,ty_vector_yolo2,tw_vector_yolo2,th_vector_yolo2,obj_mask_yolo2,obj_vector_yolo2 = fill_yolo_output(best_bboxes_yolo2,26,anchor_boxes_per_output,best_anchors_yolo2,index_best_anchor_yolo2)\n",
    "    else:\n",
    "        tx_vector_yolo2=ty_vector_yolo2=tw_vector_yolo2=th_vector_yolo2=obj_mask_yolo2 = obj_vector_yolo2=tf.zeros((1,grid_size*grid_size*num_anchors))\n",
    "        #noobj_mask_yolo2 = tf.ones((1,26*26*num_anchors))\n",
    "        \n",
    "    tx_vector = tf.concat([tx_vector_yolo1,tx_vector_yolo2],axis=0)[:,tf.newaxis]\n",
    "    ty_vector = tf.concat([ty_vector_yolo1,ty_vector_yolo2],axis=0)[:,tf.newaxis]\n",
    "    tw_vector = tf.concat([tw_vector_yolo1,tw_vector_yolo2],axis=0)[:,tf.newaxis]\n",
    "    th_vector = tf.concat([th_vector_yolo1,th_vector_yolo2],axis=0)[:,tf.newaxis]\n",
    "    obj_mask = tf.concat([obj_mask_yolo1,obj_mask_yolo2],axis=0)[:,tf.newaxis]\n",
    "    #noobj_mask = tf.concat([noobj_mask_yolo1,noobj_mask_yolo2],axis=0)[:,tf.newaxis]\n",
    "    obj_vector = tf.concat([obj_vector_yolo1,obj_vector_yolo2],axis=0)[:,tf.newaxis]\n",
    "    \n",
    "    #output = tf.concat([tx_vector,ty_vector,tw_vector,th_vector,obj_mask,noobj_mask,obj_vector],axis=1)\n",
    "    #images_bboxes_original\n",
    "    #return image,output\n",
    "    #Vamos a regresar obj mask que es 1 cuando hay objeto en grid y el anchor box especifico\n",
    "    return tf.cast(image,tf.float32)/255,(tf.concat([tx_vector,ty_vector,obj_mask],axis=1),tf.concat([tw_vector,th_vector,obj_mask],axis=1),(obj_mask),(obj_mask))\n",
    "\n",
    "def imgaug_data_augmentation(image,bboxes,num_real_boxes):\n",
    "    im_shape = image.shape\n",
    "    bbs = BoundingBoxesOnImage.from_xyxy_array(bboxes*416, shape=(416,416))\n",
    "    \n",
    "    policy = np.random.randint(5)\n",
    "    \n",
    "    #policy = 2\n",
    "    if policy == 0:\n",
    "        \n",
    "        p = np.random.random()\n",
    "        if p<=0.6:\n",
    "            aug = iaa.TranslateX(px=(-60, 60),cval=128)\n",
    "            image, bbs = aug(image=image, bounding_boxes=bbs)\n",
    "            #bbs.remove_out_of_image().clip_out_of_image()\n",
    "    \n",
    "        p = np.random.random()\n",
    "        if p<=0.8:\n",
    "            aug = iaa.HistogramEqualization()\n",
    "            image, bbs = aug(image=image, bounding_boxes=bbs)\n",
    "            #bbs.remove_out_of_image().clip_out_of_image()\n",
    "    \n",
    "    elif policy==1:\n",
    "        \n",
    "        p=np.random.random()\n",
    "        if p<=0.2:\n",
    "            aug = iaa.TranslateY(px=(int(-0.18*416), int(0.18*416)),cval=128)\n",
    "            for i in bbs.to_xyxy_array(np.int32)[:num_real_boxes,:]:\n",
    "                bbox = image[i[1]:i[3],i[0]:i[2]]\n",
    "                bbox_augmented = aug(image=bbox)\n",
    "                image[i[1]:i[3],i[0]:i[2]] = bbox_augmented\n",
    "        \n",
    "        p=np.random.random()\n",
    "        if p<=0.8:\n",
    "            square_size = np.random.randint(48)\n",
    "            aug = iaa.Cutout(nb_iterations=1, size=square_size/416, squared=True)\n",
    "            image, bbs = aug(image=image, bounding_boxes=bbs)\n",
    "            #bbs.remove_out_of_image().clip_out_of_image()\n",
    "            \n",
    "    elif policy==2:\n",
    "        p=np.random.random()\n",
    "        if p<=1:\n",
    "            aug = iaa.ShearY(shear=(int(-0.06*416), int(0.06*416)), order=1, cval=128)\n",
    "            image, bbs = aug(image=image, bounding_boxes=bbs)\n",
    "            #bbs.remove_out_of_image().clip_out_of_image()\n",
    "            \n",
    "        p=np.random.random()\n",
    "        if p<=0.6:\n",
    "            aug = iaa.TranslateY(px=(int(-0.18*416), int(0.18*416)),cval=128)\n",
    "            for i in bbs.to_xyxy_array(np.int32)[:num_real_boxes,:]:\n",
    "                bbox = image[i[1]:i[3],i[0]:i[2]]\n",
    "                bbox_augmented = aug(image=bbox)\n",
    "                image[i[1]:i[3],i[0]:i[2]] = bbox_augmented\n",
    "            \n",
    "    elif policy==3:\n",
    "        p=np.random.random()\n",
    "        if p<=0.6:    \n",
    "            aug = iaa.Rotate(rotate=(-30, 30), order=1, cval=128)\n",
    "            image, bbs = aug(image=image, bounding_boxes=bbs)\n",
    "            #bbs_aug.remove_out_of_image().clip_out_of_image()\n",
    "        \n",
    "        p=np.random.random()\n",
    "        if p<=1:\n",
    "            aug = iaa.MultiplySaturation((0.54, 1.54))\n",
    "            image, bbs = aug(image=image, bounding_boxes=bbs)\n",
    "            #bbs.remove_out_of_image().clip_out_of_image()\n",
    "            \n",
    "    bbs.remove_out_of_image()\n",
    "    \n",
    "    return image,np.clip(bbs.to_xyxy_array(np.float32),1,415),num_real_boxes\n",
    "    \n",
    "    \n",
    "def preprocessing(example_proto):\n",
    "    image_features = tf.io.parse_single_example(example_proto, image_feature_description)\n",
    "    image = tf.image.decode_jpeg(image_features['image_raw'],channels = 3)\n",
    "    image = tf.cast(tf.image.resize(image,size=(416,416)), tf.uint8)\n",
    "    bboxes =  tf.io.parse_tensor(image_features['bboxes'], out_type=tf.float32)\n",
    "    \n",
    "    num_real_boxes = image_features['num_real_boxes']\n",
    "    return image,bboxes,num_real_boxes\n",
    "\n",
    "def preprocessing_validation_set(example_proto):\n",
    "    image_features = tf.io.parse_single_example(example_proto, image_feature_description)\n",
    "    image = tf.image.decode_jpeg(image_features['image_raw'],channels = 3)\n",
    "    image = tf.cast(tf.image.resize(image,size=(416,416)), tf.uint8)\n",
    "    bboxes =  tf.io.parse_tensor(image_features['bboxes'], out_type=tf.float32)\n",
    "    bboxes = tf.clip_by_value(bboxes*416,1,415)\n",
    "    \n",
    "    num_real_boxes = image_features['num_real_boxes']\n",
    "    return image,bboxes,tf.cast(num_real_boxes,tf.int64)\n",
    "    \n",
    "@tf.function(input_signature=[tf.TensorSpec((416,416,3), tf.uint8),tf.TensorSpec((None,4), tf.float32),tf.TensorSpec((), tf.int64)]) \n",
    "def tf_numpy_albumentations_real(image,bboxes,num_real_boxes):\n",
    "    \n",
    "    boxes_shape = bboxes.shape\n",
    "    im_shape = image.shape\n",
    "\n",
    "    image,bboxes,num_real_boxes = tf.numpy_function(imgaug_data_augmentation,[image,bboxes,num_real_boxes],Tout =[tf.uint8,tf.float32,tf.int64])\n",
    " \n",
    "    image.set_shape(im_shape)\n",
    "    bboxes.set_shape(boxes_shape)\n",
    "    print(\"Imagen data type\",image.dtype)\n",
    "    print(\"Bboxes data type\",bboxes.dtype)\n",
    "    print(\"num_real_boxes\",num_real_boxes.dtype)\n",
    "\n",
    "    return image,bboxes,num_real_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imagen data type <dtype: 'uint8'>\n",
      "Bboxes data type <dtype: 'float32'>\n",
      "num_real_boxes <dtype: 'int64'>\n",
      "Tensor(\"ArgMax:0\", shape=(None,), dtype=int64)\n",
      "Tensor(\"strided_slice_2:0\", shape=(None,), dtype=int64)\n",
      "Tensor(\"GatherV2:0\", shape=(None,), dtype=int64)\n",
      "Tensor(\"strided_slice_3:0\", shape=(None,), dtype=int64)\n",
      "Tensor(\"GatherV2_1:0\", shape=(None,), dtype=int64)\n",
      "tipo de aoutput_positivon Tensor(\"add_18:0\", shape=(None, 1), dtype=int64)\n",
      "Tensor(\"add_18:0\", shape=(None, 1), dtype=int64)\n",
      "507\n",
      "tipo de aoutput_positivon Tensor(\"add_30:0\", shape=(None, 1), dtype=int64)\n",
      "Tensor(\"add_30:0\", shape=(None, 1), dtype=int64)\n",
      "2028\n",
      "Tensor(\"ArgMax:0\", dtype=int64)\n",
      "Tensor(\"strided_slice_2:0\", shape=(None,), dtype=int64)\n",
      "Tensor(\"GatherV2:0\", dtype=int64)\n",
      "Tensor(\"strided_slice_3:0\", shape=(None,), dtype=int64)\n",
      "Tensor(\"GatherV2_1:0\", dtype=int64)\n",
      "tipo de aoutput_positivon Tensor(\"add_18:0\", dtype=int64)\n",
      "Tensor(\"add_18:0\", dtype=int64)\n",
      "507\n",
      "tipo de aoutput_positivon Tensor(\"add_30:0\", dtype=int64)\n",
      "Tensor(\"add_30:0\", dtype=int64)\n",
      "2028\n"
     ]
    }
   ],
   "source": [
    "#USANDO TF.IMAGE MODULE\n",
    "#anchors =tf.constant(np.array([[0,0,0.015,0.037],[0,0,0.043,0.104],[0,0,0.11,0.278],[0,0,0.351,0.66]]),dtype=tf.float32)\n",
    "#anchors =tf.constant(np.array([[0,0,0.026,0.062],[0,0,0.067,0.183],[0,0,0.128,0.323],[0,0,0.343,0.650]]),dtype=tf.float32)\n",
    "anchors =tf.constant(np.array([[0,0,0.02078,0.049],[0,0,0.0426,0.128],[0,0,0.08523,0.19356],[0,0,0.1506,0.4163],[0,0,0.27835,0.58651],[0,0,0.5632,0.78614]]),dtype=tf.float32)\n",
    "\n",
    "\n",
    "\n",
    "os.chdir(root_path+\"/pedestrian_dataset_train_tfr\")\n",
    "filenames = os. listdir()\n",
    "raw_image_dataset = tf.data.TFRecordDataset(filenames)\n",
    "\n",
    "os.chdir(root_path+\"/pedestrian_dataset_val_tfr_fixed\")\n",
    "filenames = os. listdir()\n",
    "raw_image_dataset_val =tf.data.TFRecordDataset(filenames)\n",
    "\n",
    "os.chdir(root_path+\"/pedestrian_dataset_train_tfr\")\n",
    "#.shuffle(70000)\n",
    "\n",
    "#VAMOS A HACER UN ENTRENAMIENTO SIN DATA AUGMENTATION, ADEMAS DE USAR MSE Y EXP MSE Y REZAR PARA QUE MEJORE\n",
    "\n",
    "train_dataset = raw_image_dataset.map(preprocessing,num_parallel_calls=8)\n",
    "train_dataset = train_dataset.map(tf_numpy_albumentations_real,num_parallel_calls=8)\n",
    "train_dataset = train_dataset.map(lambda x,y,z:build_targets(x,y,z,anchors),num_parallel_calls=8)\n",
    "train_dataset = train_dataset.batch(16)\n",
    "\n",
    "val_dataset = raw_image_dataset_val.map(preprocessing_validation_set,num_parallel_calls=8)\n",
    "val_dataset = val_dataset.map(lambda x,y,z:build_targets(x,y,z,anchors),num_parallel_calls=8)\n",
    "val_dataset = val_dataset.batch(16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef loss_bce_objectness(y_true,y_pred):\\n    #bce = BinaryCrossentropy(reduction=tf.keras.losses.Reduction.NONE)\\n    #tw_true,th_true,obj_mask = tf.split(y_true, [1,1,1], axis=-1)\\n    #tw_pred,th_pred = tf.split(y_pred, [1,1], axis=-1)   \\n    \\n    loss_obj =tf.reduce_mean(tf.reduce_sum( y_true*tf.math.log(y_pred+1e-7)[:,tf.newaxis],axis=1))\\n    \\n    return loss_obj\\n\\ndef loss_bce_no_objectness(y_true,y_pred):\\n    #bce = BinaryCrossentropy(reduction=tf.keras.losses.Reduction.NONE)\\n    loss_noobj =tf.reduce_mean(tf.reduce_sum((1-y_true)*tf.math.log(1-y_pred+1e-7)[:,tf.newaxis],axis=1))\\n    \\n    return loss_noobj\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.losses import Loss,BinaryCrossentropy,MeanSquaredError,MeanSquaredLogarithmicError\n",
    "'''\n",
    "def loss_xy(y_true,y_pred):\n",
    "    \n",
    "    mse = MeanSquaredError(reduction=tf.keras.losses.Reduction.NONE)\n",
    "    \n",
    "    tx_true,ty_true,obj_mask = tf.split(y_true, [1,1,1], axis=-1)\n",
    "    tx_pred,ty_pred = tf.split(y_pred, [1,1], axis=-1)\n",
    "    \n",
    "    loss_x = tf.reduce_mean(tf.reduce_sum(obj_mask*(mse(tx_true,tx_pred)[:,:,tf.newaxis]),axis=1))\n",
    "    loss_y = tf.reduce_mean(tf.reduce_sum(obj_mask*(mse(ty_true,ty_pred)[:,:,tf.newaxis]),axis=1))\n",
    "\n",
    "    return loss_x+loss_y\n",
    "\n",
    "def loss_wh(y_true,y_pred):\n",
    "    \n",
    "    mse = MeanSquaredError(reduction=tf.keras.losses.Reduction.NONE)\n",
    "    \n",
    "    tw_true,th_true,obj_mask = tf.split(y_true, [1,1,1], axis=-1)\n",
    "    tw_pred,th_pred = tf.split(y_pred, [1,1], axis=-1)\n",
    "    \n",
    "    loss_w = tf.reduce_mean(tf.reduce_sum(obj_mask*(mse(tw_true,tw_pred)[:,:,tf.newaxis]),axis=1))\n",
    "    loss_h = tf.reduce_mean(tf.reduce_sum(obj_mask*(mse(th_true,th_pred)[:,:,tf.newaxis]),axis=1))\n",
    "\n",
    "\n",
    "    return loss_w+loss_h\n",
    "\n",
    "\n",
    "def loss_xy(y_true,y_pred):\n",
    "    \n",
    "    msle = MeanSquaredLogarithmicError(reduction=tf.keras.losses.Reduction.NONE)\n",
    "\n",
    "    tx_true,ty_true,obj_mask = tf.split(y_true, [1,1,1], axis=-1)\n",
    "    tx_pred,ty_pred = tf.split(y_pred, [1,1], axis=-1)\n",
    "    \n",
    "    #print(tf.math.square(tx_true-tx_pred))  \n",
    "    \n",
    "    loss_x = tf.reduce_mean(tf.reduce_sum(obj_mask*(msle(tx_true,tx_pred)[:,:,tf.newaxis]),axis=1))\n",
    "    loss_y = tf.reduce_mean(tf.reduce_sum(obj_mask*(msle(ty_true,ty_pred)[:,:,tf.newaxis]),axis=1))\n",
    "\n",
    "    return loss_x+loss_y\n",
    "\n",
    "def loss_wh(y_true,y_pred):\n",
    "\n",
    "    msle = MeanSquaredLogarithmicError(reduction=tf.keras.losses.Reduction.NONE)\n",
    "    \n",
    "    tw_true,th_true,obj_mask = tf.split(y_true, [1,1,1], axis=-1)\n",
    "    tw_pred,th_pred = tf.split(y_pred, [1,1], axis=-1)\n",
    "    \n",
    "    #print(tf.math.square(tw_true-tw_pred))\n",
    "\n",
    "    \n",
    "    loss_w = tf.reduce_mean(tf.reduce_sum(obj_mask*(msle(tw_true,tw_pred)[:,:,tf.newaxis]),axis=1))\n",
    "    loss_h = tf.reduce_mean(tf.reduce_sum(obj_mask*(msle(th_true,th_pred)[:,:,tf.newaxis]),axis=1))\n",
    "\n",
    "\n",
    "    return loss_w+loss_h\n",
    "    \n",
    "def loss_xy(y_true,y_pred):\n",
    "    \n",
    "    msle = MeanSquaredLogarithmicError(reduction=tf.keras.losses.Reduction.NONE)\n",
    "\n",
    "    tx_true,ty_true,obj_mask = tf.split(y_true, [1,1,1], axis=-1)\n",
    "    tx_pred,ty_pred = tf.split(y_pred, [1,1], axis=-1)\n",
    "    \n",
    "    #print(tf.math.square(tx_true-tx_pred))  \n",
    "    \n",
    "    loss_x = tf.reduce_mean(tf.reduce_sum(obj_mask*(msle(tx_true,tx_pred)[:,:,tf.newaxis]),axis=1))\n",
    "    loss_y = tf.reduce_mean(tf.reduce_sum(obj_mask*(msle(ty_true,ty_pred)[:,:,tf.newaxis]),axis=1))\n",
    "\n",
    "    return loss_x+loss_y\n",
    "    \n",
    "def loss_wh(y_true,y_pred):\n",
    "\n",
    "    mse = MeanSquaredError(reduction=tf.keras.losses.Reduction.NONE)\n",
    "    \n",
    "    tw_true,th_true,obj_mask = tf.split(y_true, [1,1,1], axis=-1)\n",
    "    tw_pred,th_pred = tf.split(y_pred, [1,1], axis=-1)\n",
    "    \n",
    "    #print(tf.math.square(tw_true-tw_pred))\n",
    "\n",
    "    \n",
    "    loss_w = tf.reduce_mean(tf.reduce_sum(obj_mask*(mse(tf.math.exp(tw_true),tf.math.exp(tw_pred))[:,:,tf.newaxis]),axis=1))\n",
    "    loss_h = tf.reduce_mean(tf.reduce_sum(obj_mask*(mse(tf.math.exp(th_true),tf.math.exp(th_pred))[:,:,tf.newaxis]),axis=1))\n",
    "\n",
    "\n",
    "    return loss_w+loss_h\n",
    "    \n",
    "'''\n",
    "\n",
    "\n",
    "def loss_xy(y_true,y_pred):\n",
    "    \n",
    "    mse = MeanSquaredError(reduction=tf.keras.losses.Reduction.NONE)\n",
    "    \n",
    "    tx_true,ty_true,obj_mask = tf.split(y_true, [1,1,1], axis=-1)\n",
    "    tx_pred,ty_pred = tf.split(y_pred, [1,1], axis=-1)\n",
    "    \n",
    "    loss_x = tf.reduce_mean(tf.reduce_sum(obj_mask*(mse(tx_true,tx_pred)[:,:,tf.newaxis]),axis=1))\n",
    "    loss_y = tf.reduce_mean(tf.reduce_sum(obj_mask*(mse(ty_true,ty_pred)[:,:,tf.newaxis]),axis=1))\n",
    "\n",
    "    return loss_x+loss_y\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def loss_wh(y_true,y_pred):\n",
    "    \n",
    "    mse = MeanSquaredError(reduction=tf.keras.losses.Reduction.NONE)\n",
    "    \n",
    "    tw_true,th_true,obj_mask = tf.split(y_true, [1,1,1], axis=-1)\n",
    "    tw_pred,th_pred = tf.split(y_pred, [1,1], axis=-1)\n",
    "    \n",
    "    loss_w = tf.reduce_mean(tf.reduce_sum(obj_mask*(mse(tw_true,tw_pred)[:,:,tf.newaxis]),axis=1))\n",
    "    loss_h = tf.reduce_mean(tf.reduce_sum(obj_mask*(mse(th_true,th_pred)[:,:,tf.newaxis]),axis=1))\n",
    "\n",
    "\n",
    "    return loss_w+loss_h\n",
    "\n",
    "def loss_objectness(y_true,y_pred):\n",
    "    bce = BinaryCrossentropy(reduction=tf.keras.losses.Reduction.NONE)\n",
    "    \n",
    "    #tw_true,th_true,obj_mask = tf.split(y_true, [1,1,1], axis=-1)\n",
    "    #tw_pred,th_pred = tf.split(y_pred, [1,1], axis=-1)   \n",
    "    \n",
    "    loss_obj =tf.reduce_mean(tf.reduce_sum( y_true*bce(y_true,y_pred)[:,tf.newaxis],axis=1))\n",
    "    \n",
    "    return loss_obj\n",
    "\n",
    "def loss_no_objectness(y_true,y_pred):\n",
    "    bce = BinaryCrossentropy(reduction=tf.keras.losses.Reduction.NONE)\n",
    "    \n",
    "    \n",
    "    loss_noobj =tf.reduce_mean(tf.reduce_sum((1-y_true)*bce(y_true,y_pred)[:,tf.newaxis],axis=1))\n",
    "    \n",
    "    return loss_noobj\n",
    "'''\n",
    "def loss_bce_objectness(y_true,y_pred):\n",
    "    #bce = BinaryCrossentropy(reduction=tf.keras.losses.Reduction.NONE)\n",
    "    #tw_true,th_true,obj_mask = tf.split(y_true, [1,1,1], axis=-1)\n",
    "    #tw_pred,th_pred = tf.split(y_pred, [1,1], axis=-1)   \n",
    "    \n",
    "    loss_obj =tf.reduce_mean(tf.reduce_sum( y_true*tf.math.log(y_pred+1e-7)[:,tf.newaxis],axis=1))\n",
    "    \n",
    "    return loss_obj\n",
    "\n",
    "def loss_bce_no_objectness(y_true,y_pred):\n",
    "    #bce = BinaryCrossentropy(reduction=tf.keras.losses.Reduction.NONE)\n",
    "    loss_noobj =tf.reduce_mean(tf.reduce_sum((1-y_true)*tf.math.log(1-y_pred+1e-7)[:,tf.newaxis],axis=1))\n",
    "    \n",
    "    return loss_noobj\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the functional model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f1630d67a58>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#anchors =[[10/416,14/416],[23/416,27/416],[37/416,58/416],[81/416,82/416],[135/416,169/416],[344/416,319/416]]\n",
    "anchors =[[0.02078,0.049],[0.0426,0.128],[0.08523,0.19356],[0.1506,0.4163],[0.27835,0.58651],[0.5632,0.78614]]\n",
    "\n",
    "model = TinyYOLOv3_functional(anchor_boxes = anchors,num_classes=0,training=True)\n",
    "current_directory= '/tf/home/sergio/Tesis/TinyYOLOv3-Pedestrian-Detection/Deployment'\n",
    "model.load_weights(current_directory+'/weights_functional_one_class/original_model_one_class')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with Model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_1 True\n",
      "conv2d True\n",
      "batch_normalization True\n",
      "leaky_re_lu True\n",
      "max_pooling2d True\n",
      "conv2d_1 True\n",
      "batch_normalization_1 True\n",
      "leaky_re_lu_1 True\n",
      "max_pooling2d_1 True\n",
      "conv2d_2 True\n",
      "batch_normalization_2 True\n",
      "leaky_re_lu_2 True\n",
      "max_pooling2d_2 True\n",
      "conv2d_3 True\n",
      "batch_normalization_3 True\n",
      "leaky_re_lu_3 True\n",
      "max_pooling2d_3 True\n",
      "conv2d_4 True\n",
      "batch_normalization_4 True\n",
      "leaky_re_lu_4 True\n",
      "max_pooling2d_4 True\n",
      "conv2d_5 True\n",
      "batch_normalization_5 True\n",
      "leaky_re_lu_5 True\n",
      "zero_padding2d True\n",
      "max_pooling2d_5 True\n",
      "conv2d_6 True\n",
      "batch_normalization_6 True\n",
      "leaky_re_lu_6 True\n",
      "conv2d_7 True\n",
      "batch_normalization_7 True\n",
      "leaky_re_lu_7 True\n",
      "conv2d_10 True\n",
      "batch_normalization_9 True\n",
      "leaky_re_lu_9 True\n",
      "tf_op_layer_ResizeBilinear True\n",
      "Concatenate True\n",
      "conv2d_8 True\n",
      "conv2d_11 True\n",
      "batch_normalization_8 True\n",
      "batch_normalization_10 True\n",
      "leaky_re_lu_8 True\n",
      "leaky_re_lu_10 True\n",
      "conv2d_9 True\n",
      "conv2d_12 True\n",
      "tf_op_layer_Shape True\n",
      "tf_op_layer_Shape_1 True\n",
      "tf_op_layer_Shape_2 True\n",
      "tf_op_layer_Shape_3 True\n",
      "tf_op_layer_strided_slice True\n",
      "tf_op_layer_strided_slice_1 True\n",
      "tf_op_layer_strided_slice_2 True\n",
      "tf_op_layer_strided_slice_3 True\n",
      "tf_op_layer_Reshape/shape True\n",
      "tf_op_layer_Reshape_3/shape True\n",
      "tf_op_layer_Reshape True\n",
      "tf_op_layer_Reshape_3 True\n",
      "tf_op_layer_split True\n",
      "tf_op_layer_Reshape_2/shape True\n",
      "tf_op_layer_split_1 True\n",
      "tf_op_layer_Reshape_5/shape True\n",
      "tf_op_layer_Reshape_2 True\n",
      "tf_op_layer_Reshape_5 True\n",
      "tf_op_layer_Sigmoid True\n",
      "tf_op_layer_Reshape_1/shape True\n",
      "tf_op_layer_Sigmoid_1 True\n",
      "tf_op_layer_Reshape_4/shape True\n",
      "Concatenate_boxes True\n",
      "tf_op_layer_Reshape_1 True\n",
      "tf_op_layer_Reshape_4 True\n",
      "tf_op_layer_split_2 True\n",
      "Concatenate_obj True\n"
     ]
    }
   ],
   "source": [
    "for l in model.layers:\n",
    "    print(l.name, l.trainable)#,l.weights[0].shape)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eager execution training (For debuging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "debugging = False\n",
    "\n",
    "if debugging:\n",
    "    model = TinyYOLOv3(1,anchor_boxes=anchors,train=True)\n",
    "    model.build(batch_input_shape=(None,416,416,3))\n",
    "    model.summary()\n",
    "    model.load_weights_darknet(\"/home/sergio/TinyYOLOv3-Pedestrian-Detection/yolov3-tiny.weights\");\n",
    "    optimizer = tf.keras.optimizers.Adam()\n",
    "    #model_loss = YOLOLoss()\n",
    "\n",
    "    for epochs in range(1,2,1):\n",
    "        for (images,y_true) in train_dataset:\n",
    "            with tf.GradientTape() as tape:\n",
    "                outputs = model(images)\n",
    "                print(\"Tamaño de la etiqueta\",y_true[0].shape)\n",
    "                print(\"Salida\",outputs[0].shape)\n",
    "                loss_x_y = loss_xy(y_true[0],outputs[0])\n",
    "                loss_w_h = loss_wh(y_true[1],outputs[1])\n",
    "                loss_obj = loss_objectness(y_true[2],outputs[2])\n",
    "                loss_noobj = loss_no_objectness(y_true[3],outputs[3])\n",
    "\n",
    "                total_loss =loss_x_y+loss_w_h+loss_obj+loss_noobj\n",
    "            #print(epochs,(total_loss.numpy()))\n",
    "            grads = tape.gradient(total_loss, model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "                #logging.info(\"{}_train_{}, {}, {}\".format(\n",
    "                #    epoch, batch, total_loss.numpy()))\n",
    "                \n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(0)\n",
    "\n",
    "from tensorflow.keras.metrics import TrueNegatives,TruePositives,FalseNegatives,FalsePositives,Precision,Recall\n",
    "\n",
    "opt = tf.keras.optimizers.RMSprop(learning_rate=1e-4)\n",
    "\n",
    "losses = {\"tf_op_layer_split_2\": loss_xy,\n",
    "          \"tf_op_layer_split_2_1\": loss_wh,\n",
    "          \"Concatenate_obj\":loss_objectness,\n",
    "          \"Concatenate_obj_1\":loss_no_objectness  \n",
    "}\n",
    "\n",
    "metrics = {\"Concatenate_obj\":[Precision(0.5),Recall(0.5),TruePositives(0.5),FalsePositives(0.5)]}\n",
    "model.compile(optimizer=opt, loss=losses,metrics=metrics,loss_weights=[2,2,1,1])\n",
    "os.chdir(root_path+ \"/pedestrian_dataset_train_tfr\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch 16, con pesos 1,1,1,1 with lr=0.0001 y usando mse para el (x,y) y usando mse para (w,h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "4607/4607 [==============================] - 507s 110ms/step - loss: 49.3641 - tf_op_layer_split_2_loss: 6.8285 - tf_op_layer_split_2_1_loss: 2.5779 - Concatenate_obj_loss: 0.0670 - Concatenate_obj_1_loss: 30.4845 - Concatenate_obj_precision: 0.0440 - Concatenate_obj_recall: 0.0504 - Concatenate_obj_true_positives: 13621.0000 - Concatenate_obj_false_positives: 295982.0000 - val_loss: 29.0434 - val_tf_op_layer_split_2_loss: 5.5292 - val_tf_op_layer_split_2_1_loss: 0.9659 - val_Concatenate_obj_loss: 0.0511 - val_Concatenate_obj_1_loss: 16.0022 - val_Concatenate_obj_precision: 0.7485 - val_Concatenate_obj_recall: 0.0539 - val_Concatenate_obj_true_positives: 637.0000 - val_Concatenate_obj_false_positives: 214.0000\n",
      "Epoch 2/10\n",
      "4607/4607 [==============================] - 509s 110ms/step - loss: 28.4992 - tf_op_layer_split_2_loss: 5.4355 - tf_op_layer_split_2_1_loss: 1.9554 - Concatenate_obj_loss: 0.0403 - Concatenate_obj_1_loss: 13.6773 - Concatenate_obj_precision: 0.6403 - Concatenate_obj_recall: 0.0872 - Concatenate_obj_true_positives: 23555.0000 - Concatenate_obj_false_positives: 13231.0000 - val_loss: 27.4160 - val_tf_op_layer_split_2_loss: 5.1144 - val_tf_op_layer_split_2_1_loss: 0.7861 - val_Concatenate_obj_loss: 0.0499 - val_Concatenate_obj_1_loss: 15.5651 - val_Concatenate_obj_precision: 0.7956 - val_Concatenate_obj_recall: 0.0520 - val_Concatenate_obj_true_positives: 615.0000 - val_Concatenate_obj_false_positives: 158.0000\n",
      "Epoch 3/10\n",
      "4607/4607 [==============================] - 523s 113ms/step - loss: 27.4035 - tf_op_layer_split_2_loss: 5.2083 - tf_op_layer_split_2_1_loss: 1.8247 - Concatenate_obj_loss: 0.0392 - Concatenate_obj_1_loss: 13.2984 - Concatenate_obj_precision: 0.6387 - Concatenate_obj_recall: 0.1033 - Concatenate_obj_true_positives: 27899.0000 - Concatenate_obj_false_positives: 15784.0000 - val_loss: 26.3145 - val_tf_op_layer_split_2_loss: 4.9124 - val_tf_op_layer_split_2_1_loss: 0.7246 - val_Concatenate_obj_loss: 0.0483 - val_Concatenate_obj_1_loss: 14.9923 - val_Concatenate_obj_precision: 0.7676 - val_Concatenate_obj_recall: 0.0718 - val_Concatenate_obj_true_positives: 849.0000 - val_Concatenate_obj_false_positives: 257.0000\n",
      "Epoch 4/10\n",
      "4607/4607 [==============================] - 527s 114ms/step - loss: 26.7424 - tf_op_layer_split_2_loss: 5.0700 - tf_op_layer_split_2_1_loss: 1.7774 - Concatenate_obj_loss: 0.0384 - Concatenate_obj_1_loss: 13.0092 - Concatenate_obj_precision: 0.6398 - Concatenate_obj_recall: 0.1154 - Concatenate_obj_true_positives: 31181.0000 - Concatenate_obj_false_positives: 17555.0000 - val_loss: 25.2121 - val_tf_op_layer_split_2_loss: 4.6333 - val_tf_op_layer_split_2_1_loss: 0.6909 - val_Concatenate_obj_loss: 0.0466 - val_Concatenate_obj_1_loss: 14.5171 - val_Concatenate_obj_precision: 0.7980 - val_Concatenate_obj_recall: 0.0815 - val_Concatenate_obj_true_positives: 964.0000 - val_Concatenate_obj_false_positives: 244.0000\n",
      "Epoch 5/10\n",
      "4607/4607 [==============================] - 519s 113ms/step - loss: 26.0091 - tf_op_layer_split_2_loss: 4.9651 - tf_op_layer_split_2_1_loss: 1.5969 - Concatenate_obj_loss: 0.0381 - Concatenate_obj_1_loss: 12.8470 - Concatenate_obj_precision: 0.6426 - Concatenate_obj_recall: 0.1233 - Concatenate_obj_true_positives: 33314.0000 - Concatenate_obj_false_positives: 18532.0000 - val_loss: 24.8594 - val_tf_op_layer_split_2_loss: 4.4637 - val_tf_op_layer_split_2_1_loss: 0.6549 - val_Concatenate_obj_loss: 0.0471 - val_Concatenate_obj_1_loss: 14.5752 - val_Concatenate_obj_precision: 0.8082 - val_Concatenate_obj_recall: 0.0770 - val_Concatenate_obj_true_positives: 910.0000 - val_Concatenate_obj_false_positives: 216.0000\n",
      "Epoch 6/10\n",
      "4607/4607 [==============================] - 515s 112ms/step - loss: 25.6573 - tf_op_layer_split_2_loss: 4.8214 - tf_op_layer_split_2_1_loss: 1.6536 - Concatenate_obj_loss: 0.0376 - Concatenate_obj_1_loss: 12.6696 - Concatenate_obj_precision: 0.6446 - Concatenate_obj_recall: 0.1322 - Concatenate_obj_true_positives: 35721.0000 - Concatenate_obj_false_positives: 19695.0000 - val_loss: 24.2769 - val_tf_op_layer_split_2_loss: 4.2728 - val_tf_op_layer_split_2_1_loss: 0.6276 - val_Concatenate_obj_loss: 0.0470 - val_Concatenate_obj_1_loss: 14.4290 - val_Concatenate_obj_precision: 0.8042 - val_Concatenate_obj_recall: 0.0865 - val_Concatenate_obj_true_positives: 1023.0000 - val_Concatenate_obj_false_positives: 249.0000\n",
      "Epoch 7/10\n",
      "4607/4607 [==============================] - 513s 111ms/step - loss: 25.0283 - tf_op_layer_split_2_loss: 4.6800 - tf_op_layer_split_2_1_loss: 1.5557 - Concatenate_obj_loss: 0.0372 - Concatenate_obj_1_loss: 12.5198 - Concatenate_obj_precision: 0.6474 - Concatenate_obj_recall: 0.1392 - Concatenate_obj_true_positives: 37618.0000 - Concatenate_obj_false_positives: 20489.0000 - val_loss: 23.7832 - val_tf_op_layer_split_2_loss: 4.2143 - val_tf_op_layer_split_2_1_loss: 0.6209 - val_Concatenate_obj_loss: 0.0456 - val_Concatenate_obj_1_loss: 14.0672 - val_Concatenate_obj_precision: 0.7839 - val_Concatenate_obj_recall: 0.1006 - val_Concatenate_obj_true_positives: 1190.0000 - val_Concatenate_obj_false_positives: 328.0000\n",
      "Epoch 8/10\n",
      "4607/4607 [==============================] - 516s 112ms/step - loss: 24.6939 - tf_op_layer_split_2_loss: 4.6211 - tf_op_layer_split_2_1_loss: 1.5128 - Concatenate_obj_loss: 0.0368 - Concatenate_obj_1_loss: 12.3893 - Concatenate_obj_precision: 0.6531 - Concatenate_obj_recall: 0.1467 - Concatenate_obj_true_positives: 39631.0000 - Concatenate_obj_false_positives: 21047.0000 - val_loss: 23.5462 - val_tf_op_layer_split_2_loss: 4.0728 - val_tf_op_layer_split_2_1_loss: 0.6314 - val_Concatenate_obj_loss: 0.0457 - val_Concatenate_obj_1_loss: 14.0922 - val_Concatenate_obj_precision: 0.7661 - val_Concatenate_obj_recall: 0.1006 - val_Concatenate_obj_true_positives: 1189.0000 - val_Concatenate_obj_false_positives: 363.0000\n",
      "Epoch 9/10\n",
      "4607/4607 [==============================] - 514s 112ms/step - loss: 24.4195 - tf_op_layer_split_2_loss: 4.4969 - tf_op_layer_split_2_1_loss: 1.5502 - Concatenate_obj_loss: 0.0365 - Concatenate_obj_1_loss: 12.2888 - Concatenate_obj_precision: 0.6520 - Concatenate_obj_recall: 0.1506 - Concatenate_obj_true_positives: 40693.0000 - Concatenate_obj_false_positives: 21716.0000 - val_loss: 22.8580 - val_tf_op_layer_split_2_loss: 3.9232 - val_tf_op_layer_split_2_1_loss: 0.5934 - val_Concatenate_obj_loss: 0.0447 - val_Concatenate_obj_1_loss: 13.7802 - val_Concatenate_obj_precision: 0.7825 - val_Concatenate_obj_recall: 0.1116 - val_Concatenate_obj_true_positives: 1320.0000 - val_Concatenate_obj_false_positives: 367.0000\n",
      "Epoch 10/10\n",
      "4607/4607 [==============================] - 523s 114ms/step - loss: 24.1083 - tf_op_layer_split_2_loss: 4.4192 - tf_op_layer_split_2_1_loss: 1.5305 - Concatenate_obj_loss: 0.0363 - Concatenate_obj_1_loss: 12.1728 - Concatenate_obj_precision: 0.6543 - Concatenate_obj_recall: 0.1575 - Concatenate_obj_true_positives: 42554.0000 - Concatenate_obj_false_positives: 22480.0000 - val_loss: 22.6929 - val_tf_op_layer_split_2_loss: 3.7689 - val_tf_op_layer_split_2_1_loss: 0.6768 - val_Concatenate_obj_loss: 0.0448 - val_Concatenate_obj_1_loss: 13.7567 - val_Concatenate_obj_precision: 0.7812 - val_Concatenate_obj_recall: 0.1102 - val_Concatenate_obj_true_positives: 1303.0000 - val_Concatenate_obj_false_positives: 365.0000\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_dataset, epochs=10,validation_data=val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_directory= '/tf/home/sergio/Tesis/TinyYOLOv3-Pedestrian-Detection/Deployment'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(current_directory+'/weights_fine_tuning_functional/rmsprop_2211_10_epochs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CONTINUACIÓN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7fad942bce10>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anchors =[[0.02078,0.049],[0.0426,0.128],[0.08523,0.19356],[0.1506,0.4163],[0.27835,0.58651],[0.5632,0.78614]]\n",
    "model = TinyYOLOv3_functional(anchor_boxes = anchors,num_classes=0,training=True)\n",
    "#print(model.load_weights_darknet(\"/home/sergio/TinyYOLOv3-Pedestrian-Detection/yolov3-tiny.weights\"))\n",
    "current_directory= '/tf/home/sergio/Tesis/TinyYOLOv3-Pedestrian-Detection/Deployment'\n",
    "model.load_weights(current_directory+'/weights_fine_tuning_functional/rmsprop_2211_10_epochs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_1 True\n",
      "conv2d True\n",
      "batch_normalization True\n",
      "leaky_re_lu True\n",
      "max_pooling2d True\n",
      "conv2d_1 True\n",
      "batch_normalization_1 True\n",
      "leaky_re_lu_1 True\n",
      "max_pooling2d_1 True\n",
      "conv2d_2 True\n",
      "batch_normalization_2 True\n",
      "leaky_re_lu_2 True\n",
      "max_pooling2d_2 True\n",
      "conv2d_3 True\n",
      "batch_normalization_3 True\n",
      "leaky_re_lu_3 True\n",
      "max_pooling2d_3 True\n",
      "conv2d_4 True\n",
      "batch_normalization_4 True\n",
      "leaky_re_lu_4 True\n",
      "max_pooling2d_4 True\n",
      "conv2d_5 True\n",
      "batch_normalization_5 True\n",
      "leaky_re_lu_5 True\n",
      "zero_padding2d True\n",
      "max_pooling2d_5 True\n",
      "conv2d_6 True\n",
      "batch_normalization_6 True\n",
      "leaky_re_lu_6 True\n",
      "conv2d_7 True\n",
      "batch_normalization_7 True\n",
      "leaky_re_lu_7 True\n",
      "conv2d_10 True\n",
      "batch_normalization_9 True\n",
      "leaky_re_lu_9 True\n",
      "tf_op_layer_ResizeBilinear True\n",
      "tf_op_layer_Concatenate True\n",
      "conv2d_8 True\n",
      "conv2d_11 True\n",
      "batch_normalization_8 True\n",
      "batch_normalization_10 True\n",
      "leaky_re_lu_8 True\n",
      "leaky_re_lu_10 True\n",
      "conv2d_9 True\n",
      "conv2d_12 True\n",
      "tf_op_layer_Shape True\n",
      "tf_op_layer_Shape_1 True\n",
      "tf_op_layer_Shape_2 True\n",
      "tf_op_layer_Shape_3 True\n",
      "tf_op_layer_strided_slice True\n",
      "tf_op_layer_strided_slice_1 True\n",
      "tf_op_layer_strided_slice_2 True\n",
      "tf_op_layer_strided_slice_3 True\n",
      "tf_op_layer_Reshape/shape True\n",
      "tf_op_layer_Reshape_3/shape True\n",
      "tf_op_layer_Reshape True\n",
      "tf_op_layer_Reshape_3 True\n",
      "tf_op_layer_split True\n",
      "tf_op_layer_Reshape_2/shape True\n",
      "tf_op_layer_split_1 True\n",
      "tf_op_layer_Reshape_5/shape True\n",
      "tf_op_layer_Reshape_2 True\n",
      "tf_op_layer_Reshape_5 True\n",
      "tf_op_layer_Sigmoid True\n",
      "tf_op_layer_Reshape_1/shape True\n",
      "tf_op_layer_Sigmoid_1 True\n",
      "tf_op_layer_Reshape_4/shape True\n",
      "Concateasdasdanate_boxes True\n",
      "tf_op_layer_Reshape_1 True\n",
      "tf_op_layer_Reshape_4 True\n",
      "tf_op_layer_split_2 True\n",
      "Concatenate_obj True\n"
     ]
    }
   ],
   "source": [
    "for l in model.layers:\n",
    "    print(l.name, l.trainable)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(0)\n",
    "\n",
    "from tensorflow.keras.metrics import TrueNegatives,TruePositives,FalseNegatives,FalsePositives,Precision,Recall\n",
    "\n",
    "opt = tf.keras.optimizers.RMSprop(learning_rate=1e-4)\n",
    "\n",
    "losses = {\"tf_op_layer_split_2\": loss_xy,\n",
    "          \"tf_op_layer_split_2_1\": loss_wh,\n",
    "          \"Concatenate_obj\":loss_objectness,\n",
    "          \"Concatenate_obj_1\":loss_no_objectness  \n",
    "}\n",
    "\n",
    "metrics = {\"Concatenate_obj\":[Precision(0.5),Recall(0.5),TruePositives(0.5),FalsePositives(0.5)]}\n",
    "model.compile(optimizer=opt, loss=losses,metrics=metrics,loss_weights=[2,2,1,1])\n",
    "os.chdir(root_path+ \"/pedestrian_dataset_train_tfr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "4607/4607 [==============================] - 518s 112ms/step - loss: 23.7484 - tf_op_layer_split_2_loss: 4.3206 - tf_op_layer_split_2_1_loss: 1.4996 - Concatenate_obj_loss: 0.0361 - Concatenate_obj_1_loss: 12.0720 - Concatenate_obj_precision: 0.6575 - Concatenate_obj_recall: 0.1623 - Concatenate_obj_true_positives: 43884.0000 - Concatenate_obj_false_positives: 22858.0000 - val_loss: 22.1258 - val_tf_op_layer_split_2_loss: 3.6276 - val_tf_op_layer_split_2_1_loss: 0.6833 - val_Concatenate_obj_loss: 0.0439 - val_Concatenate_obj_1_loss: 13.4602 - val_Concatenate_obj_precision: 0.7943 - val_Concatenate_obj_recall: 0.1228 - val_Concatenate_obj_true_positives: 1452.0000 - val_Concatenate_obj_false_positives: 376.0000\n",
      "Epoch 2/10\n",
      "4607/4607 [==============================] - 516s 112ms/step - loss: 23.3009 - tf_op_layer_split_2_loss: 4.2470 - tf_op_layer_split_2_1_loss: 1.4026 - Concatenate_obj_loss: 0.0357 - Concatenate_obj_1_loss: 11.9659 - Concatenate_obj_precision: 0.6588 - Concatenate_obj_recall: 0.1676 - Concatenate_obj_true_positives: 45293.0000 - Concatenate_obj_false_positives: 23459.0000 - val_loss: 21.7000 - val_tf_op_layer_split_2_loss: 3.5515 - val_tf_op_layer_split_2_1_loss: 0.6389 - val_Concatenate_obj_loss: 0.0433 - val_Concatenate_obj_1_loss: 13.2760 - val_Concatenate_obj_precision: 0.7916 - val_Concatenate_obj_recall: 0.1272 - val_Concatenate_obj_true_positives: 1504.0000 - val_Concatenate_obj_false_positives: 396.0000\n",
      "Epoch 3/10\n",
      "4607/4607 [==============================] - 516s 112ms/step - loss: 23.0950 - tf_op_layer_split_2_loss: 4.1826 - tf_op_layer_split_2_1_loss: 1.4065 - Concatenate_obj_loss: 0.0356 - Concatenate_obj_1_loss: 11.8811 - Concatenate_obj_precision: 0.6616 - Concatenate_obj_recall: 0.1730 - Concatenate_obj_true_positives: 46770.0000 - Concatenate_obj_false_positives: 23919.0000 - val_loss: 20.9633 - val_tf_op_layer_split_2_loss: 3.4217 - val_tf_op_layer_split_2_1_loss: 0.5663 - val_Concatenate_obj_loss: 0.0422 - val_Concatenate_obj_1_loss: 12.9451 - val_Concatenate_obj_precision: 0.7831 - val_Concatenate_obj_recall: 0.1420 - val_Concatenate_obj_true_positives: 1679.0000 - val_Concatenate_obj_false_positives: 465.0000\n",
      "Epoch 4/10\n",
      "4607/4607 [==============================] - 514s 112ms/step - loss: 22.8704 - tf_op_layer_split_2_loss: 4.1394 - tf_op_layer_split_2_1_loss: 1.3948 - Concatenate_obj_loss: 0.0352 - Concatenate_obj_1_loss: 11.7668 - Concatenate_obj_precision: 0.6647 - Concatenate_obj_recall: 0.1781 - Concatenate_obj_true_positives: 48122.0000 - Concatenate_obj_false_positives: 24279.0000 - val_loss: 20.8654 - val_tf_op_layer_split_2_loss: 3.3180 - val_tf_op_layer_split_2_1_loss: 0.5576 - val_Concatenate_obj_loss: 0.0428 - val_Concatenate_obj_1_loss: 13.0715 - val_Concatenate_obj_precision: 0.8021 - val_Concatenate_obj_recall: 0.1306 - val_Concatenate_obj_true_positives: 1544.0000 - val_Concatenate_obj_false_positives: 381.0000\n",
      "Epoch 5/10\n",
      "4607/4607 [==============================] - 508s 110ms/step - loss: 22.7447 - tf_op_layer_split_2_loss: 4.0419 - tf_op_layer_split_2_1_loss: 1.4642 - Concatenate_obj_loss: 0.0351 - Concatenate_obj_1_loss: 11.6974 - Concatenate_obj_precision: 0.6669 - Concatenate_obj_recall: 0.1828 - Concatenate_obj_true_positives: 49399.0000 - Concatenate_obj_false_positives: 24676.0000 - val_loss: 20.7826 - val_tf_op_layer_split_2_loss: 3.2192 - val_tf_op_layer_split_2_1_loss: 0.5626 - val_Concatenate_obj_loss: 0.0434 - val_Concatenate_obj_1_loss: 13.1756 - val_Concatenate_obj_precision: 0.7955 - val_Concatenate_obj_recall: 0.1418 - val_Concatenate_obj_true_positives: 1677.0000 - val_Concatenate_obj_false_positives: 431.0000\n",
      "Epoch 6/10\n",
      "4607/4607 [==============================] - 503s 109ms/step - loss: 22.3071 - tf_op_layer_split_2_loss: 3.9797 - tf_op_layer_split_2_1_loss: 1.3541 - Concatenate_obj_loss: 0.0348 - Concatenate_obj_1_loss: 11.6048 - Concatenate_obj_precision: 0.6717 - Concatenate_obj_recall: 0.1878 - Concatenate_obj_true_positives: 50749.0000 - Concatenate_obj_false_positives: 24807.0000 - val_loss: 20.0683 - val_tf_op_layer_split_2_loss: 3.1002 - val_tf_op_layer_split_2_1_loss: 0.5176 - val_Concatenate_obj_loss: 0.0419 - val_Concatenate_obj_1_loss: 12.7906 - val_Concatenate_obj_precision: 0.8348 - val_Concatenate_obj_recall: 0.1389 - val_Concatenate_obj_true_positives: 1642.0000 - val_Concatenate_obj_false_positives: 325.0000\n",
      "Epoch 7/10\n",
      "4607/4607 [==============================] - 499s 108ms/step - loss: 22.0886 - tf_op_layer_split_2_loss: 3.9122 - tf_op_layer_split_2_1_loss: 1.3670 - Concatenate_obj_loss: 0.0346 - Concatenate_obj_1_loss: 11.4955 - Concatenate_obj_precision: 0.6754 - Concatenate_obj_recall: 0.1945 - Concatenate_obj_true_positives: 52552.0000 - Concatenate_obj_false_positives: 25251.0000 - val_loss: 20.2434 - val_tf_op_layer_split_2_loss: 3.0561 - val_tf_op_layer_split_2_1_loss: 0.5410 - val_Concatenate_obj_loss: 0.0430 - val_Concatenate_obj_1_loss: 13.0062 - val_Concatenate_obj_precision: 0.8233 - val_Concatenate_obj_recall: 0.1375 - val_Concatenate_obj_true_positives: 1626.0000 - val_Concatenate_obj_false_positives: 349.0000\n",
      "Epoch 8/10\n",
      "4607/4607 [==============================] - 481s 104ms/step - loss: 21.8246 - tf_op_layer_split_2_loss: 3.8708 - tf_op_layer_split_2_1_loss: 1.3175 - Concatenate_obj_loss: 0.0343 - Concatenate_obj_1_loss: 11.4136 - Concatenate_obj_precision: 0.6751 - Concatenate_obj_recall: 0.1990 - Concatenate_obj_true_positives: 53773.0000 - Concatenate_obj_false_positives: 25884.0000 - val_loss: 19.4114 - val_tf_op_layer_split_2_loss: 2.9958 - val_tf_op_layer_split_2_1_loss: 0.5763 - val_Concatenate_obj_loss: 0.0401 - val_Concatenate_obj_1_loss: 12.2271 - val_Concatenate_obj_precision: 0.8253 - val_Concatenate_obj_recall: 0.1450 - val_Concatenate_obj_true_positives: 1715.0000 - val_Concatenate_obj_false_positives: 363.0000\n",
      "Epoch 9/10\n",
      "4607/4607 [==============================] - 503s 109ms/step - loss: 21.6159 - tf_op_layer_split_2_loss: 3.8195 - tf_op_layer_split_2_1_loss: 1.3000 - Concatenate_obj_loss: 0.0342 - Concatenate_obj_1_loss: 11.3426 - Concatenate_obj_precision: 0.6785 - Concatenate_obj_recall: 0.2033 - Concatenate_obj_true_positives: 54950.0000 - Concatenate_obj_false_positives: 26041.0000 - val_loss: 19.1098 - val_tf_op_layer_split_2_loss: 2.8188 - val_tf_op_layer_split_2_1_loss: 0.6050 - val_Concatenate_obj_loss: 0.0402 - val_Concatenate_obj_1_loss: 12.2219 - val_Concatenate_obj_precision: 0.7938 - val_Concatenate_obj_recall: 0.1729 - val_Concatenate_obj_true_positives: 2044.0000 - val_Concatenate_obj_false_positives: 531.0000\n",
      "Epoch 10/10\n",
      "4607/4607 [==============================] - 506s 110ms/step - loss: 21.3939 - tf_op_layer_split_2_loss: 3.7397 - tf_op_layer_split_2_1_loss: 1.3253 - Concatenate_obj_loss: 0.0339 - Concatenate_obj_1_loss: 11.2301 - Concatenate_obj_precision: 0.6793 - Concatenate_obj_recall: 0.2103 - Concatenate_obj_true_positives: 56834.0000 - Concatenate_obj_false_positives: 26831.0000 - val_loss: 19.0091 - val_tf_op_layer_split_2_loss: 2.7816 - val_tf_op_layer_split_2_1_loss: 0.5106 - val_Concatenate_obj_loss: 0.0412 - val_Concatenate_obj_1_loss: 12.3836 - val_Concatenate_obj_precision: 0.8038 - val_Concatenate_obj_recall: 0.1698 - val_Concatenate_obj_true_positives: 2008.0000 - val_Concatenate_obj_false_positives: 490.0000\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_dataset, epochs=10,validation_data=val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_directory= '/tf/home/sergio/Tesis/TinyYOLOv3-Pedestrian-Detection/Deployment'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(current_directory+'/weights_fine_tuning_functional/rmsprop_2211_10_20_epochs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## eNTRENAMIENTO 10-20 EPOCAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7fc11b051ac8>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anchors =[[0.02078,0.049],[0.0426,0.128],[0.08523,0.19356],[0.1506,0.4163],[0.27835,0.58651],[0.5632,0.78614]]\n",
    "model = TinyYOLOv3_functional(anchor_boxes = anchors,num_classes=0,training=True)\n",
    "#print(model.load_weights_darknet(\"/home/sergio/TinyYOLOv3-Pedestrian-Detection/yolov3-tiny.weights\"))\n",
    "current_directory= '/tf/home/sergio/Tesis/TinyYOLOv3-Pedestrian-Detection/Deployment'\n",
    "model.load_weights(current_directory+'/weights_fine_tuning_functional/rmsprop_2211_10_20_epochs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(0)\n",
    "\n",
    "from tensorflow.keras.metrics import TrueNegatives,TruePositives,FalseNegatives,FalsePositives,Precision,Recall\n",
    "\n",
    "opt = tf.keras.optimizers.RMSprop(learning_rate=1e-4)\n",
    "\n",
    "losses = {\"tf_op_layer_split_2\": loss_xy,\n",
    "          \"tf_op_layer_split_2_1\": loss_wh,\n",
    "          \"Concatenate_obj\":loss_objectness,\n",
    "          \"Concatenate_obj_1\":loss_no_objectness  \n",
    "}\n",
    "\n",
    "metrics = {\"Concatenate_obj\":[Precision(0.5),Recall(0.5),TruePositives(0.5),FalsePositives(0.5)]}\n",
    "model.compile(optimizer=opt, loss=losses,metrics=metrics,loss_weights=[2,2,1,1])\n",
    "os.chdir(root_path+ \"/pedestrian_dataset_train_tfr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#val_loss: 15.5969 - val_output_1_loss: 0.8771 - val_output_2_loss: 0.2821 - val_output_3_loss: 0.0319 - val_output_4_loss: 9.7367"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "4607/4607 [==============================] - 522s 113ms/step - loss: 21.2655 - tf_op_layer_split_2_loss: 3.7062 - tf_op_layer_split_2_1_loss: 1.3169 - Concatenate_obj_loss: 0.0338 - Concatenate_obj_1_loss: 11.1854 - Concatenate_obj_precision: 0.6812 - Concatenate_obj_recall: 0.2136 - Concatenate_obj_true_positives: 57716.0000 - Concatenate_obj_false_positives: 27012.0000 - val_loss: 18.7153 - val_tf_op_layer_split_2_loss: 2.6984 - val_tf_op_layer_split_2_1_loss: 0.5771 - val_Concatenate_obj_loss: 0.0400 - val_Concatenate_obj_1_loss: 12.1243 - val_Concatenate_obj_precision: 0.8269 - val_Concatenate_obj_recall: 0.1669 - val_Concatenate_obj_true_positives: 1973.0000 - val_Concatenate_obj_false_positives: 413.0000loss: 11.2657 - Concatenate_obj_p\n",
      "Epoch 2/10\n",
      "4607/4607 [==============================] - 511s 111ms/step - loss: 21.1879 - tf_op_layer_split_2_loss: 3.6652 - tf_op_layer_split_2_1_loss: 1.3647 - Concatenate_obj_loss: 0.0336 - Concatenate_obj_1_loss: 11.0945 - Concatenate_obj_precision: 0.6861 - Concatenate_obj_recall: 0.2202 - Concatenate_obj_true_positives: 59490.0000 - Concatenate_obj_false_positives: 27223.0000 - val_loss: 18.5235 - val_tf_op_layer_split_2_loss: 2.6935 - val_tf_op_layer_split_2_1_loss: 0.5027 - val_Concatenate_obj_loss: 0.0402 - val_Concatenate_obj_1_loss: 12.0910 - val_Concatenate_obj_precision: 0.8321 - val_Concatenate_obj_recall: 0.1664 - val_Concatenate_obj_true_positives: 1968.0000 - val_Concatenate_obj_false_positives: 397.0000\n",
      "Epoch 3/10\n",
      "4607/4607 [==============================] - 518s 113ms/step - loss: 21.1114 - tf_op_layer_split_2_loss: 3.6564 - tf_op_layer_split_2_1_loss: 1.3658 - Concatenate_obj_loss: 0.0334 - Concatenate_obj_1_loss: 11.0334 - Concatenate_obj_precision: 0.6892 - Concatenate_obj_recall: 0.2246 - Concatenate_obj_true_positives: 60675.0000 - Concatenate_obj_false_positives: 27360.0000 - val_loss: 17.8823 - val_tf_op_layer_split_2_loss: 2.5356 - val_tf_op_layer_split_2_1_loss: 0.5285 - val_Concatenate_obj_loss: 0.0388 - val_Concatenate_obj_1_loss: 11.7154 - val_Concatenate_obj_precision: 0.8156 - val_Concatenate_obj_recall: 0.1871 - val_Concatenate_obj_true_positives: 2212.0000 - val_Concatenate_obj_false_positives: 500.0000\n",
      "Epoch 4/10\n",
      "4607/4607 [==============================] - 512s 111ms/step - loss: 20.7865 - tf_op_layer_split_2_loss: 3.5984 - tf_op_layer_split_2_1_loss: 1.2944 - Concatenate_obj_loss: 0.0333 - Concatenate_obj_1_loss: 10.9677 - Concatenate_obj_precision: 0.6910 - Concatenate_obj_recall: 0.2280 - Concatenate_obj_true_positives: 61602.0000 - Concatenate_obj_false_positives: 27546.0000 - val_loss: 17.8254 - val_tf_op_layer_split_2_loss: 2.4676 - val_tf_op_layer_split_2_1_loss: 0.5227 - val_Concatenate_obj_loss: 0.0393 - val_Concatenate_obj_1_loss: 11.8055 - val_Concatenate_obj_precision: 0.8293 - val_Concatenate_obj_recall: 0.1951 - val_Concatenate_obj_true_positives: 2307.0000 - val_Concatenate_obj_false_positives: 475.0000\n",
      "Epoch 5/10\n",
      "4607/4607 [==============================] - 515s 112ms/step - loss: 20.6516 - tf_op_layer_split_2_loss: 3.5520 - tf_op_layer_split_2_1_loss: 1.3347 - Concatenate_obj_loss: 0.0329 - Concatenate_obj_1_loss: 10.8455 - Concatenate_obj_precision: 0.6958 - Concatenate_obj_recall: 0.2367 - Concatenate_obj_true_positives: 63980.0000 - Concatenate_obj_false_positives: 27966.0000 - val_loss: 17.4820 - val_tf_op_layer_split_2_loss: 2.4277 - val_tf_op_layer_split_2_1_loss: 0.4710 - val_Concatenate_obj_loss: 0.0388 - val_Concatenate_obj_1_loss: 11.6458 - val_Concatenate_obj_precision: 0.8452 - val_Concatenate_obj_recall: 0.1967 - val_Concatenate_obj_true_positives: 2326.0000 - val_Concatenate_obj_false_positives: 426.0000\n",
      "Epoch 6/10\n",
      "4607/4607 [==============================] - 509s 111ms/step - loss: 20.5120 - tf_op_layer_split_2_loss: 3.5169 - tf_op_layer_split_2_1_loss: 1.3111 - Concatenate_obj_loss: 0.0329 - Concatenate_obj_1_loss: 10.8231 - Concatenate_obj_precision: 0.6971 - Concatenate_obj_recall: 0.2397 - Concatenate_obj_true_positives: 64795.0000 - Concatenate_obj_false_positives: 28161.0000 - val_loss: 17.4947 - val_tf_op_layer_split_2_loss: 2.3906 - val_tf_op_layer_split_2_1_loss: 0.4797 - val_Concatenate_obj_loss: 0.0392 - val_Concatenate_obj_1_loss: 11.7147 - val_Concatenate_obj_precision: 0.8279 - val_Concatenate_obj_recall: 0.2054 - val_Concatenate_obj_true_positives: 2429.0000 - val_Concatenate_obj_false_positives: 505.0000\n",
      "Epoch 7/10\n",
      "4607/4607 [==============================] - 514s 112ms/step - loss: 20.3352 - tf_op_layer_split_2_loss: 3.4743 - tf_op_layer_split_2_1_loss: 1.3171 - Concatenate_obj_loss: 0.0327 - Concatenate_obj_1_loss: 10.7197 - Concatenate_obj_precision: 0.7019 - Concatenate_obj_recall: 0.2459 - Concatenate_obj_true_positives: 66466.0000 - Concatenate_obj_false_positives: 28234.0000 - val_loss: 17.3104 - val_tf_op_layer_split_2_loss: 2.3245 - val_tf_op_layer_split_2_1_loss: 0.4749 - val_Concatenate_obj_loss: 0.0394 - val_Concatenate_obj_1_loss: 11.6723 - val_Concatenate_obj_precision: 0.8499 - val_Concatenate_obj_recall: 0.2031 - val_Concatenate_obj_true_positives: 2401.0000 - val_Concatenate_obj_false_positives: 424.0000\n",
      "Epoch 8/10\n",
      "4607/4607 [==============================] - 518s 112ms/step - loss: 20.1517 - tf_op_layer_split_2_loss: 3.4438 - tf_op_layer_split_2_1_loss: 1.3011 - Concatenate_obj_loss: 0.0324 - Concatenate_obj_1_loss: 10.6295 - Concatenate_obj_precision: 0.7056 - Concatenate_obj_recall: 0.2535 - Concatenate_obj_true_positives: 68513.0000 - Concatenate_obj_false_positives: 28584.0000 - val_loss: 16.8619 - val_tf_op_layer_split_2_loss: 2.2624 - val_tf_op_layer_split_2_1_loss: 0.4662 - val_Concatenate_obj_loss: 0.0383 - val_Concatenate_obj_1_loss: 11.3664 - val_Concatenate_obj_precision: 0.8591 - val_Concatenate_obj_recall: 0.2094 - val_Concatenate_obj_true_positives: 2476.0000 - val_Concatenate_obj_false_positives: 406.0000\n",
      "Epoch 9/10\n",
      "4607/4607 [==============================] - 517s 112ms/step - loss: 20.0916 - tf_op_layer_split_2_loss: 3.4122 - tf_op_layer_split_2_1_loss: 1.3217 - Concatenate_obj_loss: 0.0324 - Concatenate_obj_1_loss: 10.5914 - Concatenate_obj_precision: 0.7060 - Concatenate_obj_recall: 0.2557 - Concatenate_obj_true_positives: 69111.0000 - Concatenate_obj_false_positives: 28775.0000 - val_loss: 16.6400 - val_tf_op_layer_split_2_loss: 2.2746 - val_tf_op_layer_split_2_1_loss: 0.5391 - val_Concatenate_obj_loss: 0.0367 - val_Concatenate_obj_1_loss: 10.9758 - val_Concatenate_obj_precision: 0.8602 - val_Concatenate_obj_recall: 0.2175 - val_Concatenate_obj_true_positives: 2572.0000 - val_Concatenate_obj_false_positives: 418.0000\n",
      "Epoch 10/10\n",
      "4607/4607 [==============================] - 518s 112ms/step - loss: 19.8807 - tf_op_layer_split_2_loss: 3.4118 - tf_op_layer_split_2_1_loss: 1.2626 - Concatenate_obj_loss: 0.0321 - Concatenate_obj_1_loss: 10.4998 - Concatenate_obj_precision: 0.7073 - Concatenate_obj_recall: 0.2607 - Concatenate_obj_true_positives: 70446.0000 - Concatenate_obj_false_positives: 29148.0000 - val_loss: 16.2284 - val_tf_op_layer_split_2_loss: 2.2035 - val_tf_op_layer_split_2_1_loss: 0.5042 - val_Concatenate_obj_loss: 0.0361 - val_Concatenate_obj_1_loss: 10.7769 - val_Concatenate_obj_precision: 0.8675 - val_Concatenate_obj_recall: 0.2226 - val_Concatenate_obj_true_positives: 2632.0000 - val_Concatenate_obj_false_positives: 402.0000\n"
     ]
    }
   ],
   "source": [
    "os.chdir(root_path+ \"/pedestrian_dataset_train_tfr\")\n",
    "history = model.fit(train_dataset, epochs=10,validation_data=val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_directory= '/tf/home/sergio/Tesis/TinyYOLOv3-Pedestrian-Detection/Deployment'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(current_directory+'/weights_fine_tuning_functional/rmsprop_2211_20_30_epochs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "4607/4607 [==============================] - 512s 111ms/step - loss: 19.5762 - tf_op_layer_split_2_loss: 3.3312 - tf_op_layer_split_2_1_loss: 1.2314 - Concatenate_obj_loss: 0.0319 - Concatenate_obj_1_loss: 10.4192 - Concatenate_obj_precision: 0.7109 - Concatenate_obj_recall: 0.2672 - Concatenate_obj_true_positives: 72219.0000 - Concatenate_obj_false_positives: 29363.0000 - val_loss: 15.9157 - val_tf_op_layer_split_2_loss: 2.1207 - val_tf_op_layer_split_2_1_loss: 0.4635 - val_Concatenate_obj_loss: 0.0361 - val_Concatenate_obj_1_loss: 10.7111 - val_Concatenate_obj_precision: 0.8586 - val_Concatenate_obj_recall: 0.2275 - val_Concatenate_obj_true_positives: 2690.0000 - val_Concatenate_obj_false_positives: 443.0000\n",
      "Epoch 2/10\n",
      "4607/4607 [==============================] - 513s 111ms/step - loss: 19.5244 - tf_op_layer_split_2_loss: 3.3485 - tf_op_layer_split_2_1_loss: 1.2254 - Concatenate_obj_loss: 0.0317 - Concatenate_obj_1_loss: 10.3449 - Concatenate_obj_precision: 0.7148 - Concatenate_obj_recall: 0.2724 - Concatenate_obj_true_positives: 73627.0000 - Concatenate_obj_false_positives: 29378.0000 - val_loss: 15.6779 - val_tf_op_layer_split_2_loss: 2.0808 - val_tf_op_layer_split_2_1_loss: 0.4807 - val_Concatenate_obj_loss: 0.0357 - val_Concatenate_obj_1_loss: 10.5192 - val_Concatenate_obj_precision: 0.8715 - val_Concatenate_obj_recall: 0.2341 - val_Concatenate_obj_true_positives: 2768.0000 - val_Concatenate_obj_false_positives: 408.0000\n",
      "Epoch 3/10\n",
      "4607/4607 [==============================] - 514s 111ms/step - loss: 19.5949 - tf_op_layer_split_2_loss: 3.2943 - tf_op_layer_split_2_1_loss: 1.3413 - Concatenate_obj_loss: 0.0316 - Concatenate_obj_1_loss: 10.2920 - Concatenate_obj_precision: 0.7178 - Concatenate_obj_recall: 0.2768 - Concatenate_obj_true_positives: 74796.0000 - Concatenate_obj_false_positives: 29409.0000 - val_loss: 15.8358 - val_tf_op_layer_split_2_loss: 2.1203 - val_tf_op_layer_split_2_1_loss: 0.4361 - val_Concatenate_obj_loss: 0.0361 - val_Concatenate_obj_1_loss: 10.6868 - val_Concatenate_obj_precision: 0.8594 - val_Concatenate_obj_recall: 0.2332 - val_Concatenate_obj_true_positives: 2757.0000 - val_Concatenate_obj_false_positives: 451.0000\n",
      "Epoch 4/10\n",
      "4607/4607 [==============================] - 510s 111ms/step - loss: 19.4148 - tf_op_layer_split_2_loss: 3.2976 - tf_op_layer_split_2_1_loss: 1.2786 - Concatenate_obj_loss: 0.0315 - Concatenate_obj_1_loss: 10.2308 - Concatenate_obj_precision: 0.7214 - Concatenate_obj_recall: 0.2829 - Concatenate_obj_true_positives: 76447.0000 - Concatenate_obj_false_positives: 29528.0000 - val_loss: 15.5978 - val_tf_op_layer_split_2_loss: 2.0271 - val_tf_op_layer_split_2_1_loss: 0.4428 - val_Concatenate_obj_loss: 0.0360 - val_Concatenate_obj_1_loss: 10.6220 - val_Concatenate_obj_precision: 0.8689 - val_Concatenate_obj_recall: 0.2443 - val_Concatenate_obj_true_positives: 2889.0000 - val_Concatenate_obj_false_positives: 436.0000\n",
      "Epoch 5/10\n",
      "4607/4607 [==============================] - 510s 111ms/step - loss: 19.2716 - tf_op_layer_split_2_loss: 3.2678 - tf_op_layer_split_2_1_loss: 1.2621 - Concatenate_obj_loss: 0.0313 - Concatenate_obj_1_loss: 10.1805 - Concatenate_obj_precision: 0.7244 - Concatenate_obj_recall: 0.2858 - Concatenate_obj_true_positives: 77229.0000 - Concatenate_obj_false_positives: 29380.0000 - val_loss: 14.9364 - val_tf_op_layer_split_2_loss: 1.9343 - val_tf_op_layer_split_2_1_loss: 0.4284 - val_Concatenate_obj_loss: 0.0345 - val_Concatenate_obj_1_loss: 10.1765 - val_Concatenate_obj_precision: 0.8871 - val_Concatenate_obj_recall: 0.2420 - val_Concatenate_obj_true_positives: 2861.0000 - val_Concatenate_obj_false_positives: 364.0000\n",
      "Epoch 6/10\n",
      "4607/4607 [==============================] - 511s 111ms/step - loss: 19.1774 - tf_op_layer_split_2_loss: 3.2294 - tf_op_layer_split_2_1_loss: 1.2974 - Concatenate_obj_loss: 0.0311 - Concatenate_obj_1_loss: 10.0928 - Concatenate_obj_precision: 0.7256 - Concatenate_obj_recall: 0.2922 - Concatenate_obj_true_positives: 78967.0000 - Concatenate_obj_false_positives: 29861.0000 - val_loss: 15.6459 - val_tf_op_layer_split_2_loss: 2.0015 - val_tf_op_layer_split_2_1_loss: 0.4473 - val_Concatenate_obj_loss: 0.0366 - val_Concatenate_obj_1_loss: 10.7115 - val_Concatenate_obj_precision: 0.8820 - val_Concatenate_obj_recall: 0.2434 - val_Concatenate_obj_true_positives: 2878.0000 - val_Concatenate_obj_false_positives: 385.0000\n",
      "Epoch 7/10\n",
      "4607/4607 [==============================] - 508s 110ms/step - loss: 19.0516 - tf_op_layer_split_2_loss: 3.1978 - tf_op_layer_split_2_1_loss: 1.3077 - Concatenate_obj_loss: 0.0309 - Concatenate_obj_1_loss: 10.0097 - Concatenate_obj_precision: 0.7282 - Concatenate_obj_recall: 0.2988 - Concatenate_obj_true_positives: 80736.0000 - Concatenate_obj_false_positives: 30129.0000 - val_loss: 14.9543 - val_tf_op_layer_split_2_loss: 1.8996 - val_tf_op_layer_split_2_1_loss: 0.4437 - val_Concatenate_obj_loss: 0.0350 - val_Concatenate_obj_1_loss: 10.2327 - val_Concatenate_obj_precision: 0.8943 - val_Concatenate_obj_recall: 0.2526 - val_Concatenate_obj_true_positives: 2987.0000 - val_Concatenate_obj_false_positives: 353.0000\n",
      "Epoch 8/10\n",
      "4607/4607 [==============================] - 515s 112ms/step - loss: 18.9033 - tf_op_layer_split_2_loss: 3.2167 - tf_op_layer_split_2_1_loss: 1.2309 - Concatenate_obj_loss: 0.0308 - Concatenate_obj_1_loss: 9.9773 - Concatenate_obj_precision: 0.7305 - Concatenate_obj_recall: 0.3022 - Concatenate_obj_true_positives: 81636.0000 - Concatenate_obj_false_positives: 30123.0000 - val_loss: 14.8624 - val_tf_op_layer_split_2_loss: 1.9597 - val_tf_op_layer_split_2_1_loss: 0.4504 - val_Concatenate_obj_loss: 0.0341 - val_Concatenate_obj_1_loss: 10.0082 - val_Concatenate_obj_precision: 0.8814 - val_Concatenate_obj_recall: 0.2690 - val_Concatenate_obj_true_positives: 3181.0000 - val_Concatenate_obj_false_positives: 428.0000\n",
      "Epoch 9/10\n",
      "4607/4607 [==============================] - 503s 109ms/step - loss: 18.8404 - tf_op_layer_split_2_loss: 3.1876 - tf_op_layer_split_2_1_loss: 1.2453 - Concatenate_obj_loss: 0.0308 - Concatenate_obj_1_loss: 9.9440 - Concatenate_obj_precision: 0.7336 - Concatenate_obj_recall: 0.3056 - Concatenate_obj_true_positives: 82597.0000 - Concatenate_obj_false_positives: 29990.0000 - val_loss: 14.1435 - val_tf_op_layer_split_2_loss: 1.8213 - val_tf_op_layer_split_2_1_loss: 0.4455 - val_Concatenate_obj_loss: 0.0327 - val_Concatenate_obj_1_loss: 9.5771 - val_Concatenate_obj_precision: 0.8841 - val_Concatenate_obj_recall: 0.2814 - val_Concatenate_obj_true_positives: 3327.0000 - val_Concatenate_obj_false_positives: 436.0000\n",
      "Epoch 10/10\n",
      "4607/4607 [==============================] - 512s 111ms/step - loss: 18.5526 - tf_op_layer_split_2_loss: 3.1290 - tf_op_layer_split_2_1_loss: 1.2119 - Concatenate_obj_loss: 0.0304 - Concatenate_obj_1_loss: 9.8404 - Concatenate_obj_precision: 0.7359 - Concatenate_obj_recall: 0.3110 - Concatenate_obj_true_positives: 84022.0000 - Concatenate_obj_false_positives: 30150.0000 - val_loss: 14.8092 - val_tf_op_layer_split_2_loss: 1.8954 - val_tf_op_layer_split_2_1_loss: 0.4328 - val_Concatenate_obj_loss: 0.0348 - val_Concatenate_obj_1_loss: 10.1180 - val_Concatenate_obj_precision: 0.8904 - val_Concatenate_obj_recall: 0.2708 - val_Concatenate_obj_true_positives: 3202.0000 - val_Concatenate_obj_false_positives: 394.0000\n"
     ]
    }
   ],
   "source": [
    "os.chdir(root_path+ \"/pedestrian_dataset_train_tfr\")\n",
    "history = model.fit(train_dataset, epochs=10,validation_data=val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_directory= '/tf/home/sergio/Tesis/TinyYOLOv3-Pedestrian-Detection/Deployment'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(current_directory+'/weights_fine_tuning_functional/rmsprop_2211_30_40_epochs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "4607/4607 [==============================] - 520s 113ms/step - loss: 18.7244 - tf_op_layer_split_2_loss: 3.1369 - tf_op_layer_split_2_1_loss: 1.3118 - Concatenate_obj_loss: 0.0303 - Concatenate_obj_1_loss: 9.7968 - Concatenate_obj_precision: 0.7369 - Concatenate_obj_recall: 0.3157 - Concatenate_obj_true_positives: 85306.0000 - Concatenate_obj_false_positives: 30462.0000 - val_loss: 14.3368 - val_tf_op_layer_split_2_loss: 1.8055 - val_tf_op_layer_split_2_1_loss: 0.4417 - val_Concatenate_obj_loss: 0.0338 - val_Concatenate_obj_1_loss: 9.8087 - val_Concatenate_obj_precision: 0.9055 - val_Concatenate_obj_recall: 0.2797 - val_Concatenate_obj_true_positives: 3307.0000 - val_Concatenate_obj_false_positives: 345.0000\n",
      "Epoch 2/10\n",
      "4607/4607 [==============================] - 520s 113ms/step - loss: 18.5417 - tf_op_layer_split_2_loss: 3.1512 - tf_op_layer_split_2_1_loss: 1.2337 - Concatenate_obj_loss: 0.0302 - Concatenate_obj_1_loss: 9.7417 - Concatenate_obj_precision: 0.7401 - Concatenate_obj_recall: 0.3204 - Concatenate_obj_true_positives: 86581.0000 - Concatenate_obj_false_positives: 30410.0000 - val_loss: 14.1193 - val_tf_op_layer_split_2_loss: 1.7557 - val_tf_op_layer_split_2_1_loss: 0.4520 - val_Concatenate_obj_loss: 0.0334 - val_Concatenate_obj_1_loss: 9.6705 - val_Concatenate_obj_precision: 0.8869 - val_Concatenate_obj_recall: 0.2979 - val_Concatenate_obj_true_positives: 3522.0000 - val_Concatenate_obj_false_positives: 449.0000\n",
      "Epoch 3/10\n",
      "4607/4607 [==============================] - 509s 111ms/step - loss: 18.6013 - tf_op_layer_split_2_loss: 3.1389 - tf_op_layer_split_2_1_loss: 1.2857 - Concatenate_obj_loss: 0.0301 - Concatenate_obj_1_loss: 9.7219 - Concatenate_obj_precision: 0.7411 - Concatenate_obj_recall: 0.3222 - Concatenate_obj_true_positives: 87083.0000 - Concatenate_obj_false_positives: 30418.0000 - val_loss: 14.3450 - val_tf_op_layer_split_2_loss: 1.7888 - val_tf_op_layer_split_2_1_loss: 0.4260 - val_Concatenate_obj_loss: 0.0343 - val_Concatenate_obj_1_loss: 9.8811 - val_Concatenate_obj_precision: 0.8967 - val_Concatenate_obj_recall: 0.2936 - val_Concatenate_obj_true_positives: 3471.0000 - val_Concatenate_obj_false_positives: 400.0000\n",
      "Epoch 4/10\n",
      "4607/4607 [==============================] - 508s 110ms/step - loss: 18.3918 - tf_op_layer_split_2_loss: 3.1238 - tf_op_layer_split_2_1_loss: 1.2255 - Concatenate_obj_loss: 0.0300 - Concatenate_obj_1_loss: 9.6632 - Concatenate_obj_precision: 0.7430 - Concatenate_obj_recall: 0.3261 - Concatenate_obj_true_positives: 88125.0000 - Concatenate_obj_false_positives: 30487.0000 - val_loss: 13.7844 - val_tf_op_layer_split_2_loss: 1.7079 - val_tf_op_layer_split_2_1_loss: 0.4466 - val_Concatenate_obj_loss: 0.0325 - val_Concatenate_obj_1_loss: 9.4427 - val_Concatenate_obj_precision: 0.8961 - val_Concatenate_obj_recall: 0.3018 - val_Concatenate_obj_true_positives: 3569.0000 - val_Concatenate_obj_false_positives: 414.0000\n",
      "Epoch 5/10\n",
      "4607/4607 [==============================] - 514s 112ms/step - loss: 18.1017 - tf_op_layer_split_2_loss: 3.0683 - tf_op_layer_split_2_1_loss: 1.1864 - Concatenate_obj_loss: 0.0297 - Concatenate_obj_1_loss: 9.5627 - Concatenate_obj_precision: 0.7463 - Concatenate_obj_recall: 0.3331 - Concatenate_obj_true_positives: 90044.0000 - Concatenate_obj_false_positives: 30605.0000 - val_loss: 13.8148 - val_tf_op_layer_split_2_loss: 1.7609 - val_tf_op_layer_split_2_1_loss: 0.4381 - val_Concatenate_obj_loss: 0.0323 - val_Concatenate_obj_1_loss: 9.3846 - val_Concatenate_obj_precision: 0.9116 - val_Concatenate_obj_recall: 0.2982 - val_Concatenate_obj_true_positives: 3526.0000 - val_Concatenate_obj_false_positives: 342.0000\n",
      "Epoch 6/10\n",
      "4607/4607 [==============================] - 502s 109ms/step - loss: 18.2385 - tf_op_layer_split_2_loss: 3.0572 - tf_op_layer_split_2_1_loss: 1.2936 - Concatenate_obj_loss: 0.0296 - Concatenate_obj_1_loss: 9.5074 - Concatenate_obj_precision: 0.7486 - Concatenate_obj_recall: 0.3374 - Concatenate_obj_true_positives: 91189.0000 - Concatenate_obj_false_positives: 30616.0000 - val_loss: 13.5701 - val_tf_op_layer_split_2_loss: 1.6945 - val_tf_op_layer_split_2_1_loss: 0.4304 - val_Concatenate_obj_loss: 0.0321 - val_Concatenate_obj_1_loss: 9.2883 - val_Concatenate_obj_precision: 0.9158 - val_Concatenate_obj_recall: 0.3025 - val_Concatenate_obj_true_positives: 3577.0000 - val_Concatenate_obj_false_positives: 329.0000\n",
      "Epoch 7/10\n",
      "4607/4607 [==============================] - 509s 111ms/step - loss: 18.0766 - tf_op_layer_split_2_loss: 3.0465 - tf_op_layer_split_2_1_loss: 1.2422 - Concatenate_obj_loss: 0.0294 - Concatenate_obj_1_loss: 9.4699 - Concatenate_obj_precision: 0.7510 - Concatenate_obj_recall: 0.3411 - Concatenate_obj_true_positives: 92149.0000 - Concatenate_obj_false_positives: 30560.0000 - val_loss: 13.6319 - val_tf_op_layer_split_2_loss: 1.6512 - val_tf_op_layer_split_2_1_loss: 0.4221 - val_Concatenate_obj_loss: 0.0329 - val_Concatenate_obj_1_loss: 9.4524 - val_Concatenate_obj_precision: 0.9061 - val_Concatenate_obj_recall: 0.3052 - val_Concatenate_obj_true_positives: 3609.0000 - val_Concatenate_obj_false_positives: 374.0000\n",
      "Epoch 8/10\n",
      "4607/4607 [==============================] - 510s 111ms/step - loss: 17.9775 - tf_op_layer_split_2_loss: 3.0415 - tf_op_layer_split_2_1_loss: 1.2103 - Concatenate_obj_loss: 0.0294 - Concatenate_obj_1_loss: 9.4445 - Concatenate_obj_precision: 0.7527 - Concatenate_obj_recall: 0.3438 - Concatenate_obj_true_positives: 92893.0000 - Concatenate_obj_false_positives: 30522.0000 - val_loss: 13.3136 - val_tf_op_layer_split_2_loss: 1.6296 - val_tf_op_layer_split_2_1_loss: 0.4489 - val_Concatenate_obj_loss: 0.0316 - val_Concatenate_obj_1_loss: 9.1250 - val_Concatenate_obj_precision: 0.9205 - val_Concatenate_obj_recall: 0.3123 - val_Concatenate_obj_true_positives: 3693.0000 - val_Concatenate_obj_false_positives: 319.0000\n",
      "Epoch 9/10\n",
      "4607/4607 [==============================] - 511s 111ms/step - loss: 17.9152 - tf_op_layer_split_2_loss: 3.0114 - tf_op_layer_split_2_1_loss: 1.2272 - Concatenate_obj_loss: 0.0293 - Concatenate_obj_1_loss: 9.4086 - Concatenate_obj_precision: 0.7526 - Concatenate_obj_recall: 0.3454 - Concatenate_obj_true_positives: 93364.0000 - Concatenate_obj_false_positives: 30692.0000 - val_loss: 13.3443 - val_tf_op_layer_split_2_loss: 1.7348 - val_tf_op_layer_split_2_1_loss: 0.4069 - val_Concatenate_obj_loss: 0.0313 - val_Concatenate_obj_1_loss: 9.0297 - val_Concatenate_obj_precision: 0.9147 - val_Concatenate_obj_recall: 0.3166 - val_Concatenate_obj_true_positives: 3744.0000 - val_Concatenate_obj_false_positives: 349.0000\n",
      "Epoch 10/10\n",
      "4607/4607 [==============================] - 513s 111ms/step - loss: 17.9041 - tf_op_layer_split_2_loss: 3.0033 - tf_op_layer_split_2_1_loss: 1.2558 - Concatenate_obj_loss: 0.0292 - Concatenate_obj_1_loss: 9.3567 - Concatenate_obj_precision: 0.7563 - Concatenate_obj_recall: 0.3506 - Concatenate_obj_true_positives: 94770.0000 - Concatenate_obj_false_positives: 30537.0000 - val_loss: 13.1615 - val_tf_op_layer_split_2_loss: 1.6780 - val_tf_op_layer_split_2_1_loss: 0.4734 - val_Concatenate_obj_loss: 0.0304 - val_Concatenate_obj_1_loss: 8.8284 - val_Concatenate_obj_precision: 0.9066 - val_Concatenate_obj_recall: 0.3216 - val_Concatenate_obj_true_positives: 3803.0000 - val_Concatenate_obj_false_positives: 392.0000\n"
     ]
    }
   ],
   "source": [
    "os.chdir(root_path+ \"/pedestrian_dataset_train_tfr\")\n",
    "history = model.fit(train_dataset, epochs=10,validation_data=val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_directory= '/tf/home/sergio/Tesis/TinyYOLOv3-Pedestrian-Detection/Deployment'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(current_directory+'/weights_fine_tuning_functional/rmsprop_2211_40_50_epochs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "4607/4607 [==============================] - 516s 112ms/step - loss: 17.8119 - tf_op_layer_split_2_loss: 2.9748 - tf_op_layer_split_2_1_loss: 1.2627 - Concatenate_obj_loss: 0.0290 - Concatenate_obj_1_loss: 9.3080 - Concatenate_obj_precision: 0.7572 - Concatenate_obj_recall: 0.3534 - Concatenate_obj_true_positives: 95517.0000 - Concatenate_obj_false_positives: 30635.0000 - val_loss: 13.9047 - val_tf_op_layer_split_2_loss: 1.6999 - val_tf_op_layer_split_2_1_loss: 0.4492 - val_Concatenate_obj_loss: 0.0333 - val_Concatenate_obj_1_loss: 9.5732 - val_Concatenate_obj_precision: 0.9293 - val_Concatenate_obj_recall: 0.2978 - val_Concatenate_obj_true_positives: 3521.0000 - val_Concatenate_obj_false_positives: 268.0000\n",
      "Epoch 2/10\n",
      "4607/4607 [==============================] - 509s 110ms/step - loss: 17.7091 - tf_op_layer_split_2_loss: 2.9850 - tf_op_layer_split_2_1_loss: 1.2111 - Concatenate_obj_loss: 0.0290 - Concatenate_obj_1_loss: 9.2879 - Concatenate_obj_precision: 0.7572 - Concatenate_obj_recall: 0.3553 - Concatenate_obj_true_positives: 96032.0000 - Concatenate_obj_false_positives: 30788.0000 - val_loss: 13.1511 - val_tf_op_layer_split_2_loss: 1.5561 - val_tf_op_layer_split_2_1_loss: 0.4194 - val_Concatenate_obj_loss: 0.0322 - val_Concatenate_obj_1_loss: 9.1681 - val_Concatenate_obj_precision: 0.9172 - val_Concatenate_obj_recall: 0.3204 - val_Concatenate_obj_true_positives: 3789.0000 - val_Concatenate_obj_false_positives: 342.0000\n",
      "Epoch 3/10\n",
      "4607/4607 [==============================] - 508s 110ms/step - loss: 17.5977 - tf_op_layer_split_2_loss: 2.9839 - tf_op_layer_split_2_1_loss: 1.1938 - Concatenate_obj_loss: 0.0287 - Concatenate_obj_1_loss: 9.2135 - Concatenate_obj_precision: 0.7607 - Concatenate_obj_recall: 0.3606 - Concatenate_obj_true_positives: 97440.0000 - Concatenate_obj_false_positives: 30658.0000 - val_loss: 12.8373 - val_tf_op_layer_split_2_loss: 1.6334 - val_tf_op_layer_split_2_1_loss: 0.4276 - val_Concatenate_obj_loss: 0.0305 - val_Concatenate_obj_1_loss: 8.6849 - val_Concatenate_obj_precision: 0.9188 - val_Concatenate_obj_recall: 0.3319 - val_Concatenate_obj_true_positives: 3924.0000 - val_Concatenate_obj_false_positives: 347.0000\n",
      "Epoch 4/10\n",
      "4607/4607 [==============================] - 517s 112ms/step - loss: 17.4046 - tf_op_layer_split_2_loss: 2.9273 - tf_op_layer_split_2_1_loss: 1.1843 - Concatenate_obj_loss: 0.0286 - Concatenate_obj_1_loss: 9.1526 - Concatenate_obj_precision: 0.7616 - Concatenate_obj_recall: 0.3643 - Concatenate_obj_true_positives: 98470.0000 - Concatenate_obj_false_positives: 30818.0000 - val_loss: 12.8690 - val_tf_op_layer_split_2_loss: 1.5993 - val_tf_op_layer_split_2_1_loss: 0.4236 - val_Concatenate_obj_loss: 0.0309 - val_Concatenate_obj_1_loss: 8.7922 - val_Concatenate_obj_precision: 0.9184 - val_Concatenate_obj_recall: 0.3408 - val_Concatenate_obj_true_positives: 4030.0000 - val_Concatenate_obj_false_positives: 358.0000\n",
      "Epoch 5/10\n",
      "4607/4607 [==============================] - 505s 110ms/step - loss: 17.4932 - tf_op_layer_split_2_loss: 2.9566 - tf_op_layer_split_2_1_loss: 1.2022 - Concatenate_obj_loss: 0.0286 - Concatenate_obj_1_loss: 9.1469 - Concatenate_obj_precision: 0.7662 - Concatenate_obj_recall: 0.3679 - Concatenate_obj_true_positives: 99428.0000 - Concatenate_obj_false_positives: 30333.0000 - val_loss: 12.6286 - val_tf_op_layer_split_2_loss: 1.5847 - val_tf_op_layer_split_2_1_loss: 0.4103 - val_Concatenate_obj_loss: 0.0301 - val_Concatenate_obj_1_loss: 8.6086 - val_Concatenate_obj_precision: 0.9135 - val_Concatenate_obj_recall: 0.3457 - val_Concatenate_obj_true_positives: 4088.0000 - val_Concatenate_obj_false_positives: 387.0000\n",
      "Epoch 6/10\n",
      "4607/4607 [==============================] - 511s 111ms/step - loss: 17.5411 - tf_op_layer_split_2_loss: 2.9467 - tf_op_layer_split_2_1_loss: 1.2545 - Concatenate_obj_loss: 0.0285 - Concatenate_obj_1_loss: 9.1102 - Concatenate_obj_precision: 0.7645 - Concatenate_obj_recall: 0.3698 - Concatenate_obj_true_positives: 99955.0000 - Concatenate_obj_false_positives: 30788.0000 - val_loss: 12.5950 - val_tf_op_layer_split_2_loss: 1.5400 - val_tf_op_layer_split_2_1_loss: 0.4410 - val_Concatenate_obj_loss: 0.0302 - val_Concatenate_obj_1_loss: 8.6030 - val_Concatenate_obj_precision: 0.9112 - val_Concatenate_obj_recall: 0.3531 - val_Concatenate_obj_true_positives: 4175.0000 - val_Concatenate_obj_false_positives: 407.0000\n",
      "Epoch 7/10\n",
      "4607/4607 [==============================] - 514s 112ms/step - loss: 17.2634 - tf_op_layer_split_2_loss: 2.9038 - tf_op_layer_split_2_1_loss: 1.2083 - Concatenate_obj_loss: 0.0282 - Concatenate_obj_1_loss: 9.0112 - Concatenate_obj_precision: 0.7675 - Concatenate_obj_recall: 0.3772 - Concatenate_obj_true_positives: 101927.0000 - Concatenate_obj_false_positives: 30877.0000 - val_loss: 12.3926 - val_tf_op_layer_split_2_loss: 1.5137 - val_tf_op_layer_split_2_1_loss: 0.3984 - val_Concatenate_obj_loss: 0.0298 - val_Concatenate_obj_1_loss: 8.5387 - val_Concatenate_obj_precision: 0.9268 - val_Concatenate_obj_recall: 0.3403 - val_Concatenate_obj_true_positives: 4024.0000 - val_Concatenate_obj_false_positives: 318.0000\n",
      "Epoch 8/10\n",
      "4607/4607 [==============================] - 514s 111ms/step - loss: 17.2377 - tf_op_layer_split_2_loss: 2.9295 - tf_op_layer_split_2_1_loss: 1.1674 - Concatenate_obj_loss: 0.0282 - Concatenate_obj_1_loss: 9.0158 - Concatenate_obj_precision: 0.7684 - Concatenate_obj_recall: 0.3776 - Concatenate_obj_true_positives: 102063.0000 - Concatenate_obj_false_positives: 30766.0000 - val_loss: 12.3117 - val_tf_op_layer_split_2_loss: 1.5127 - val_tf_op_layer_split_2_1_loss: 0.4310 - val_Concatenate_obj_loss: 0.0294 - val_Concatenate_obj_1_loss: 8.3949 - val_Concatenate_obj_precision: 0.9317 - val_Concatenate_obj_recall: 0.3544 - val_Concatenate_obj_true_positives: 4191.0000 - val_Concatenate_obj_false_positives: 307.0000\n",
      "Epoch 9/10\n",
      "4607/4607 [==============================] - 516s 112ms/step - loss: 17.1855 - tf_op_layer_split_2_loss: 2.8988 - tf_op_layer_split_2_1_loss: 1.1866 - Concatenate_obj_loss: 0.0281 - Concatenate_obj_1_loss: 8.9866 - Concatenate_obj_precision: 0.7691 - Concatenate_obj_recall: 0.3790 - Concatenate_obj_true_positives: 102400.0000 - Concatenate_obj_false_positives: 30747.0000 - val_loss: 12.4477 - val_tf_op_layer_split_2_loss: 1.5407 - val_tf_op_layer_split_2_1_loss: 0.4306 - val_Concatenate_obj_loss: 0.0302 - val_Concatenate_obj_1_loss: 8.4749 - val_Concatenate_obj_precision: 0.9257 - val_Concatenate_obj_recall: 0.3612 - val_Concatenate_obj_true_positives: 4271.0000 - val_Concatenate_obj_false_positives: 343.0000\n",
      "Epoch 10/10\n",
      "4607/4607 [==============================] - 511s 111ms/step - loss: 17.1242 - tf_op_layer_split_2_loss: 2.9228 - tf_op_layer_split_2_1_loss: 1.1545 - Concatenate_obj_loss: 0.0279 - Concatenate_obj_1_loss: 8.9417 - Concatenate_obj_precision: 0.7696 - Concatenate_obj_recall: 0.3831 - Concatenate_obj_true_positives: 103538.0000 - Concatenate_obj_false_positives: 31004.0000 - val_loss: 12.2869 - val_tf_op_layer_split_2_loss: 1.4995 - val_tf_op_layer_split_2_1_loss: 0.4207 - val_Concatenate_obj_loss: 0.0297 - val_Concatenate_obj_1_loss: 8.4170 - val_Concatenate_obj_precision: 0.9249 - val_Concatenate_obj_recall: 0.3697 - val_Concatenate_obj_true_positives: 4371.0000 - val_Concatenate_obj_false_positives: 355.0000\n"
     ]
    }
   ],
   "source": [
    "os.chdir(root_path+ \"/pedestrian_dataset_train_tfr\")\n",
    "history = model.fit(train_dataset, epochs=10,validation_data=val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_directory= '/tf/home/sergio/Tesis/TinyYOLOv3-Pedestrian-Detection/Deployment'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(current_directory+'/weights_fine_tuning_functional/rmsprop_2211_50_60_epochs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "4607/4607 [==============================] - 511s 111ms/step - loss: 16.9920 - tf_op_layer_split_2_loss: 2.8917 - tf_op_layer_split_2_1_loss: 1.1488 - Concatenate_obj_loss: 0.0278 - Concatenate_obj_1_loss: 8.8830 - Concatenate_obj_precision: 0.7712 - Concatenate_obj_recall: 0.3875 - Concatenate_obj_true_positives: 104725.0000 - Concatenate_obj_false_positives: 31065.0000 - val_loss: 12.0608 - val_tf_op_layer_split_2_loss: 1.4654 - val_tf_op_layer_split_2_1_loss: 0.4332 - val_Concatenate_obj_loss: 0.0290 - val_Concatenate_obj_1_loss: 8.2344 - val_Concatenate_obj_precision: 0.9229 - val_Concatenate_obj_recall: 0.3704 - val_Concatenate_obj_true_positives: 4380.0000 - val_Concatenate_obj_false_positives: 366.0000\n",
      "Epoch 2/10\n",
      "4607/4607 [==============================] - 513s 111ms/step - loss: 16.9798 - tf_op_layer_split_2_loss: 2.8872 - tf_op_layer_split_2_1_loss: 1.1344 - Concatenate_obj_loss: 0.0279 - Concatenate_obj_1_loss: 8.9087 - Concatenate_obj_precision: 0.7706 - Concatenate_obj_recall: 0.3852 - Concatenate_obj_true_positives: 104102.0000 - Concatenate_obj_false_positives: 30995.0000 - val_loss: 12.5612 - val_tf_op_layer_split_2_loss: 1.4753 - val_tf_op_layer_split_2_1_loss: 0.4102 - val_Concatenate_obj_loss: 0.0309 - val_Concatenate_obj_1_loss: 8.7594 - val_Concatenate_obj_precision: 0.9405 - val_Concatenate_obj_recall: 0.3330 - val_Concatenate_obj_true_positives: 3937.0000 - val_Concatenate_obj_false_positives: 249.0000\n",
      "Epoch 3/10\n",
      "4607/4607 [==============================] - 512s 111ms/step - loss: 16.9635 - tf_op_layer_split_2_loss: 2.8895 - tf_op_layer_split_2_1_loss: 1.1633 - Concatenate_obj_loss: 0.0277 - Concatenate_obj_1_loss: 8.8303 - Concatenate_obj_precision: 0.7711 - Concatenate_obj_recall: 0.3896 - Concatenate_obj_true_positives: 105316.0000 - Concatenate_obj_false_positives: 31259.0000 - val_loss: 11.9831 - val_tf_op_layer_split_2_loss: 1.4767 - val_tf_op_layer_split_2_1_loss: 0.3847 - val_Concatenate_obj_loss: 0.0289 - val_Concatenate_obj_1_loss: 8.2314 - val_Concatenate_obj_precision: 0.9233 - val_Concatenate_obj_recall: 0.3665 - val_Concatenate_obj_true_positives: 4333.0000 - val_Concatenate_obj_false_positives: 360.0000\n",
      "Epoch 4/10\n",
      "4607/4607 [==============================] - 510s 111ms/step - loss: 16.9725 - tf_op_layer_split_2_loss: 2.8680 - tf_op_layer_split_2_1_loss: 1.1963 - Concatenate_obj_loss: 0.0276 - Concatenate_obj_1_loss: 8.8163 - Concatenate_obj_precision: 0.7763 - Concatenate_obj_recall: 0.3934 - Concatenate_obj_true_positives: 106336.0000 - Concatenate_obj_false_positives: 30645.0000 - val_loss: 12.4044 - val_tf_op_layer_split_2_loss: 1.4685 - val_tf_op_layer_split_2_1_loss: 0.4203 - val_Concatenate_obj_loss: 0.0304 - val_Concatenate_obj_1_loss: 8.5965 - val_Concatenate_obj_precision: 0.9262 - val_Concatenate_obj_recall: 0.3566 - val_Concatenate_obj_true_positives: 4217.0000 - val_Concatenate_obj_false_positives: 336.0000\n",
      "Epoch 5/10\n",
      "4607/4607 [==============================] - 515s 112ms/step - loss: 16.9761 - tf_op_layer_split_2_loss: 2.8847 - tf_op_layer_split_2_1_loss: 1.1995 - Concatenate_obj_loss: 0.0276 - Concatenate_obj_1_loss: 8.7802 - Concatenate_obj_precision: 0.7762 - Concatenate_obj_recall: 0.3957 - Concatenate_obj_true_positives: 106928.0000 - Concatenate_obj_false_positives: 30826.0000 - val_loss: 11.5541 - val_tf_op_layer_split_2_loss: 1.4468 - val_tf_op_layer_split_2_1_loss: 0.3967 - val_Concatenate_obj_loss: 0.0279 - val_Concatenate_obj_1_loss: 7.8392 - val_Concatenate_obj_precision: 0.9225 - val_Concatenate_obj_recall: 0.3807 - val_Concatenate_obj_true_positives: 4501.0000 - val_Concatenate_obj_false_positives: 378.0000\n",
      "Epoch 6/10\n",
      "4607/4607 [==============================] - 511s 111ms/step - loss: 16.9337 - tf_op_layer_split_2_loss: 2.8544 - tf_op_layer_split_2_1_loss: 1.2196 - Concatenate_obj_loss: 0.0274 - Concatenate_obj_1_loss: 8.7583 - Concatenate_obj_precision: 0.7760 - Concatenate_obj_recall: 0.3974 - Concatenate_obj_true_positives: 107399.0000 - Concatenate_obj_false_positives: 31008.0000 - val_loss: 11.5245 - val_tf_op_layer_split_2_loss: 1.4185 - val_tf_op_layer_split_2_1_loss: 0.4070 - val_Concatenate_obj_loss: 0.0276 - val_Concatenate_obj_1_loss: 7.8458 - val_Concatenate_obj_precision: 0.9195 - val_Concatenate_obj_recall: 0.3872 - val_Concatenate_obj_true_positives: 4578.0000 - val_Concatenate_obj_false_positives: 401.0000\n",
      "Epoch 7/10\n",
      "4607/4607 [==============================] - 511s 111ms/step - loss: 16.7993 - tf_op_layer_split_2_loss: 2.8444 - tf_op_layer_split_2_1_loss: 1.1716 - Concatenate_obj_loss: 0.0274 - Concatenate_obj_1_loss: 8.7398 - Concatenate_obj_precision: 0.7774 - Concatenate_obj_recall: 0.4000 - Concatenate_obj_true_positives: 108065.0000 - Concatenate_obj_false_positives: 30946.0000 - val_loss: 11.8074 - val_tf_op_layer_split_2_loss: 1.4323 - val_tf_op_layer_split_2_1_loss: 0.4223 - val_Concatenate_obj_loss: 0.0287 - val_Concatenate_obj_1_loss: 8.0695 - val_Concatenate_obj_precision: 0.9347 - val_Concatenate_obj_recall: 0.3716 - val_Concatenate_obj_true_positives: 4394.0000 - val_Concatenate_obj_false_positives: 307.0000\n",
      "Epoch 8/10\n",
      "4607/4607 [==============================] - 511s 111ms/step - loss: 16.7934 - tf_op_layer_split_2_loss: 2.8472 - tf_op_layer_split_2_1_loss: 1.1748 - Concatenate_obj_loss: 0.0275 - Concatenate_obj_1_loss: 8.7219 - Concatenate_obj_precision: 0.7776 - Concatenate_obj_recall: 0.4000 - Concatenate_obj_true_positives: 108091.0000 - Concatenate_obj_false_positives: 30920.0000 - val_loss: 11.6136 - val_tf_op_layer_split_2_loss: 1.4019 - val_tf_op_layer_split_2_1_loss: 0.3917 - val_Concatenate_obj_loss: 0.0286 - val_Concatenate_obj_1_loss: 7.9978 - val_Concatenate_obj_precision: 0.9346 - val_Concatenate_obj_recall: 0.3745 - val_Concatenate_obj_true_positives: 4428.0000 - val_Concatenate_obj_false_positives: 310.0000\n",
      "Epoch 9/10\n",
      "4607/4607 [==============================] - 513s 111ms/step - loss: 16.8161 - tf_op_layer_split_2_loss: 2.8551 - tf_op_layer_split_2_1_loss: 1.2082 - Concatenate_obj_loss: 0.0271 - Concatenate_obj_1_loss: 8.6622 - Concatenate_obj_precision: 0.7790 - Concatenate_obj_recall: 0.4036 - Concatenate_obj_true_positives: 109068.0000 - Concatenate_obj_false_positives: 30939.0000 - val_loss: 11.6251 - val_tf_op_layer_split_2_loss: 1.4059 - val_tf_op_layer_split_2_1_loss: 0.3791 - val_Concatenate_obj_loss: 0.0286 - val_Concatenate_obj_1_loss: 8.0264 - val_Concatenate_obj_precision: 0.9403 - val_Concatenate_obj_recall: 0.3810 - val_Concatenate_obj_true_positives: 4505.0000 - val_Concatenate_obj_false_positives: 286.0000\n",
      "Epoch 10/10\n",
      "4607/4607 [==============================] - 515s 112ms/step - loss: 16.8316 - tf_op_layer_split_2_loss: 2.8797 - tf_op_layer_split_2_1_loss: 1.1963 - Concatenate_obj_loss: 0.0272 - Concatenate_obj_1_loss: 8.6525 - Concatenate_obj_precision: 0.7786 - Concatenate_obj_recall: 0.4054 - Concatenate_obj_true_positives: 109558.0000 - Concatenate_obj_false_positives: 31148.0000 - val_loss: 10.9479 - val_tf_op_layer_split_2_loss: 1.3866 - val_tf_op_layer_split_2_1_loss: 0.3992 - val_Concatenate_obj_loss: 0.0259 - val_Concatenate_obj_1_loss: 7.3503 - val_Concatenate_obj_precision: 0.9376 - val_Concatenate_obj_recall: 0.4003 - val_Concatenate_obj_true_positives: 4733.0000 - val_Concatenate_obj_false_positives: 315.0000\n"
     ]
    }
   ],
   "source": [
    "os.chdir(root_path+ \"/pedestrian_dataset_train_tfr\")\n",
    "history = model.fit(train_dataset, epochs=10,validation_data=val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_directory= '/tf/home/sergio/Tesis/TinyYOLOv3-Pedestrian-Detection/Deployment'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(current_directory+'/weights_fine_tuning_functional/rmsprop_2211_60_70_epochs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CONTNUACION DESDE EPOCA 80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modo entrenamiento\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f3562eb9e80>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anchors =tf.constant(np.array([[0,0,0.02078,0.049],[0,0,0.0426,0.128],[0,0,0.08523,0.19356],[0,0,0.1506,0.4163],[0,0,0.27835,0.58651],[0,0,0.5632,0.78614]]),dtype=tf.float32)\n",
    "model = TinyYOLOv3(1,anchor_boxes=anchors,train=True,mode = \"finetuning\")\n",
    "model.build(batch_input_shape=(None,416,416,3))\n",
    "#print(model.load_weights_darknet(\"/home/sergio/TinyYOLOv3-Pedestrian-Detection/yolov3-tiny.weights\"))\n",
    "model.load_weights(root_path+'/last_weights/pesos_finetuning_1121_70_80_epoch_nadam_0dot00001_mse_mse_3anchors_con_data_aug')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BasicBlock1 True\n",
      "BasicBlock2 True\n",
      "BasicBlock3 True\n",
      "BasicBlock4 True\n",
      "BasicBlock5 True\n",
      "BasicBlock6 True\n",
      "BasicBlock7 True\n",
      "BasicBlock8 True\n",
      "BasicBlock9 True\n",
      "FinalBlock1 True\n",
      "BasicBlock11 True\n",
      "BasicBlock12 True\n",
      "FinalBlock2 True\n",
      "Concatenate True\n",
      "Upsampling True\n",
      "Prediction1 True\n",
      "Prediction2 True\n",
      "Concatenate_BBOX True\n"
     ]
    }
   ],
   "source": [
    "for l in model.layers:\n",
    "    print(l.name, l.trainable)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(0)\n",
    "\n",
    "from tensorflow.keras.metrics import TrueNegatives,TruePositives,FalseNegatives,FalsePositives,Precision,Recall\n",
    "\n",
    "opt = tf.keras.optimizers.Nadam(learning_rate=3e-4)\n",
    "\n",
    "losses = {\"output_1\": loss_xy,\n",
    "          \"output_2\": loss_wh,\n",
    "          \"output_3\":loss_objectness,\n",
    "          \"output_4\":loss_no_objectness  \n",
    "}\n",
    "\n",
    "metrics = {\"output_3\":[Precision(0.5),Recall(0.5),TruePositives(0.5),FalsePositives(0.5)]}\n",
    "model.compile(optimizer=opt, loss=losses,metrics=metrics,loss_weights=[1,1,2,1])\n",
    "os.chdir(root_path+ \"/pedestrian_dataset_train_tfr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#val_loss: 15.5969 - val_output_1_loss: 0.8771 - val_output_2_loss: 0.2821 - val_output_3_loss: 0.0319 - val_output_4_loss: 9.7367"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "Modo entrenamiento\n",
      "Modo entrenamiento\n",
      "   4607/Unknown - 556s 121ms/step - loss: 10.1637 - output_1_loss: 2.7599 - output_2_loss: 1.0363 - output_3_loss: 0.0196 - output_4_loss: 6.3282 - output_3_precision: 0.8569 - output_3_recall: 0.5845 - output_3_true_positives: 157983.0000 - output_3_false_positives: 26391.0000Modo entrenamiento\n",
      "4607/4607 [==============================] - 564s 122ms/step - loss: 10.1637 - output_1_loss: 2.7599 - output_2_loss: 1.0363 - output_3_loss: 0.0196 - output_4_loss: 6.3282 - output_3_precision: 0.8569 - output_3_recall: 0.5845 - output_3_true_positives: 157983.0000 - output_3_false_positives: 26391.0000 - val_loss: 6.3997 - val_output_1_loss: 1.4494 - val_output_2_loss: 0.3623 - val_output_3_loss: 0.0164 - val_output_4_loss: 4.5553 - val_output_3_precision: 0.9776 - val_output_3_recall: 0.5793 - val_output_3_true_positives: 6850.0000 - val_output_3_false_positives: 157.0000\n",
      "Epoch 2/10\n",
      "4607/4607 [==============================] - 552s 120ms/step - loss: 10.0397 - output_1_loss: 2.7140 - output_2_loss: 1.0330 - output_3_loss: 0.0193 - output_4_loss: 6.2541 - output_3_precision: 0.8577 - output_3_recall: 0.5910 - output_3_true_positives: 159715.0000 - output_3_false_positives: 26500.0000 - val_loss: 6.1277 - val_output_1_loss: 1.4369 - val_output_2_loss: 0.3668 - val_output_3_loss: 0.0154 - val_output_4_loss: 4.2931 - val_output_3_precision: 0.9802 - val_output_3_recall: 0.5934 - val_output_3_true_positives: 7016.0000 - val_output_3_false_positives: 142.0000\n",
      "Epoch 3/10\n",
      "4607/4607 [==============================] - 568s 123ms/step - loss: 9.9833 - output_1_loss: 2.7280 - output_2_loss: 0.9940 - output_3_loss: 0.0192 - output_4_loss: 6.2229 - output_3_precision: 0.8596 - output_3_recall: 0.5937 - output_3_true_positives: 160421.0000 - output_3_false_positives: 26196.0000 - val_loss: 6.2877 - val_output_1_loss: 1.4226 - val_output_2_loss: 0.3710 - val_output_3_loss: 0.0159 - val_output_4_loss: 4.4622 - val_output_3_precision: 0.9792 - val_output_3_recall: 0.5803 - val_output_3_true_positives: 6861.0000 - val_output_3_false_positives: 146.0000\n",
      "Epoch 4/10\n",
      "4607/4607 [==============================] - 571s 124ms/step - loss: 10.0977 - output_1_loss: 2.7615 - output_2_loss: 1.0151 - output_3_loss: 0.0195 - output_4_loss: 6.2822 - output_3_precision: 0.8576 - output_3_recall: 0.5884 - output_3_true_positives: 158998.0000 - output_3_false_positives: 26409.0000 - val_loss: 6.5181 - val_output_1_loss: 1.4831 - val_output_2_loss: 0.4061 - val_output_3_loss: 0.0164 - val_output_4_loss: 4.5960 - val_output_3_precision: 0.9756 - val_output_3_recall: 0.5817 - val_output_3_true_positives: 6878.0000 - val_output_3_false_positives: 172.0000\n",
      "Epoch 5/10\n",
      "4607/4607 [==============================] - 552s 120ms/step - loss: 10.0510 - output_1_loss: 2.7549 - output_2_loss: 0.9957 - output_3_loss: 0.0194 - output_4_loss: 6.2615 - output_3_precision: 0.8577 - output_3_recall: 0.5892 - output_3_true_positives: 159266.0000 - output_3_false_positives: 26431.0000 - val_loss: 6.0569 - val_output_1_loss: 1.4290 - val_output_2_loss: 0.3695 - val_output_3_loss: 0.0150 - val_output_4_loss: 4.2284 - val_output_3_precision: 0.9774 - val_output_3_recall: 0.5991 - val_output_3_true_positives: 7084.0000 - val_output_3_false_positives: 164.0000\n",
      "Epoch 6/10\n",
      "4607/4607 [==============================] - 555s 120ms/step - loss: 9.9768 - output_1_loss: 2.7240 - output_2_loss: 1.0132 - output_3_loss: 0.0192 - output_4_loss: 6.2011 - output_3_precision: 0.8609 - output_3_recall: 0.5946 - output_3_true_positives: 160682.0000 - output_3_false_positives: 25954.0000 - val_loss: 6.0562 - val_output_1_loss: 1.4268 - val_output_2_loss: 0.3585 - val_output_3_loss: 0.0151 - val_output_4_loss: 4.2406 - val_output_3_precision: 0.9819 - val_output_3_recall: 0.5951 - val_output_3_true_positives: 7037.0000 - val_output_3_false_positives: 130.0000\n",
      "Epoch 7/10\n",
      "4607/4607 [==============================] - 575s 125ms/step - loss: 9.9317 - output_1_loss: 2.6925 - output_2_loss: 1.0173 - output_3_loss: 0.0192 - output_4_loss: 6.1836 - output_3_precision: 0.8607 - output_3_recall: 0.5952 - output_3_true_positives: 160809.0000 - output_3_false_positives: 26031.0000 - val_loss: 6.1225 - val_output_1_loss: 1.4244 - val_output_2_loss: 0.3606 - val_output_3_loss: 0.0155 - val_output_4_loss: 4.3065 - val_output_3_precision: 0.9784 - val_output_3_recall: 0.5924 - val_output_3_true_positives: 7005.0000 - val_output_3_false_positives: 155.0000\n",
      "Epoch 8/10\n",
      "4607/4607 [==============================] - 557s 121ms/step - loss: 9.9605 - output_1_loss: 2.7127 - output_2_loss: 1.0245 - output_3_loss: 0.0192 - output_4_loss: 6.1848 - output_3_precision: 0.8610 - output_3_recall: 0.5957 - output_3_true_positives: 161002.0000 - output_3_false_positives: 25995.0000 - val_loss: 6.1525 - val_output_1_loss: 1.4159 - val_output_2_loss: 0.3804 - val_output_3_loss: 0.0156 - val_output_4_loss: 4.3250 - val_output_3_precision: 0.9780 - val_output_3_recall: 0.5964 - val_output_3_true_positives: 7052.0000 - val_output_3_false_positives: 159.0000\n",
      "Epoch 9/10\n",
      "4607/4607 [==============================] - 559s 121ms/step - loss: 9.9791 - output_1_loss: 2.7460 - output_2_loss: 1.0077 - output_3_loss: 0.0193 - output_4_loss: 6.1869 - output_3_precision: 0.8612 - output_3_recall: 0.5956 - output_3_true_positives: 160946.0000 - output_3_false_positives: 25948.0000 - val_loss: 5.9797 - val_output_1_loss: 1.3943 - val_output_2_loss: 0.3843 - val_output_3_loss: 0.0149 - val_output_4_loss: 4.1713 - val_output_3_precision: 0.9809 - val_output_3_recall: 0.5990 - val_output_3_true_positives: 7083.0000 - val_output_3_false_positives: 138.0000\n",
      "Epoch 10/10\n",
      "4607/4607 [==============================] - 550s 119ms/step - loss: 9.8308 - output_1_loss: 2.6907 - output_2_loss: 0.9980 - output_3_loss: 0.0188 - output_4_loss: 6.1044 - output_3_precision: 0.8632 - output_3_recall: 0.6028 - output_3_true_positives: 162915.0000 - output_3_false_positives: 25826.0000 - val_loss: 5.7591 - val_output_1_loss: 1.3914 - val_output_2_loss: 0.3680 - val_output_3_loss: 0.0144 - val_output_4_loss: 3.9710 - val_output_3_precision: 0.9814 - val_output_3_recall: 0.6235 - val_output_3_true_positives: 7372.0000 - val_output_3_false_positives: 140.0000\n"
     ]
    }
   ],
   "source": [
    "os.chdir(root_path+ \"/pedestrian_dataset_train_tfr\")\n",
    "history = model.fit(train_dataset, epochs=10,validation_data=val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(root_path+\"/json_final_experiment\")\n",
    "import json\n",
    "#json.dumps(str(a))\n",
    "with open('history_finetuning_1121_80_90_epoch_nadam_0dot00001_mse_mse_3anchors_con_data_aug.json', 'w') as fp:\n",
    "    json.dump(str(history.history), fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(root_path+'/last_weights/pesos_finetuning_1121_80_90_epoch_nadam_0dot00001_mse_mse_3anchors_con_data_aug')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CONTINUACION EPOCAS DE LA 90 A LA 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modo entrenamiento\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7fc08bedb4e0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anchors =tf.constant(np.array([[0,0,0.02078,0.049],[0,0,0.0426,0.128],[0,0,0.08523,0.19356],[0,0,0.1506,0.4163],[0,0,0.27835,0.58651],[0,0,0.5632,0.78614]]),dtype=tf.float32)\n",
    "model = TinyYOLOv3(1,anchor_boxes=anchors,train=True,mode = \"finetuning\")\n",
    "model.build(batch_input_shape=(None,416,416,3))\n",
    "#print(model.load_weights_darknet(\"/home/sergio/TinyYOLOv3-Pedestrian-Detection/yolov3-tiny.weights\"))\n",
    "model.load_weights(root_path+'/last_weights/pesos_finetuning_1121_80_90_epoch_nadam_0dot00001_mse_mse_3anchors_con_data_aug')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(0)\n",
    "\n",
    "from tensorflow.keras.metrics import TrueNegatives,TruePositives,FalseNegatives,FalsePositives,Precision,Recall\n",
    "\n",
    "opt = tf.keras.optimizers.Nadam(learning_rate=3e-4)\n",
    "\n",
    "losses = {\"output_1\": loss_xy,\n",
    "          \"output_2\": loss_wh,\n",
    "          \"output_3\":loss_objectness,\n",
    "          \"output_4\":loss_no_objectness  \n",
    "}\n",
    "\n",
    "metrics = {\"output_3\":[Precision(0.5),Recall(0.5),TruePositives(0.5),FalsePositives(0.5)]}\n",
    "model.compile(optimizer=opt, loss=losses,metrics=metrics,loss_weights=[1,1,2,1])\n",
    "os.chdir(root_path+ \"/pedestrian_dataset_train_tfr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "Modo entrenamiento\n",
      "Modo entrenamiento\n",
      "   4607/Unknown - 579s 126ms/step - loss: 9.8747 - output_1_loss: 2.6954 - output_2_loss: 1.0173 - output_3_loss: 0.0189 - output_4_loss: 6.1242 - output_3_precision: 0.8630 - output_3_recall: 0.6006 - output_3_true_positives: 162368.0000 - output_3_false_positives: 25780.0000- 546s 126ms/step - loss: 9.8828 - output_1_loss: 2.7033 - output_2_loss: 1.01 - 566s 125ms/step - loss: 9.9384 - output_1_loss: 2.7149 - output_2_loss: 1.0277 - output_3_loss: 0.0191 - output_4_loss: 6.1577 - output_3_precision: 0.8631 - outputModo entrenamiento\n",
      "4607/4607 [==============================] - 586s 127ms/step - loss: 9.8747 - output_1_loss: 2.6954 - output_2_loss: 1.0173 - output_3_loss: 0.0189 - output_4_loss: 6.1242 - output_3_precision: 0.8630 - output_3_recall: 0.6006 - output_3_true_positives: 162368.0000 - output_3_false_positives: 25780.0000 - val_loss: 5.8917 - val_output_1_loss: 1.4194 - val_output_2_loss: 0.3791 - val_output_3_loss: 0.0144 - val_output_4_loss: 4.0643 - val_output_3_precision: 0.9791 - val_output_3_recall: 0.6129 - val_output_3_true_positives: 7247.0000 - val_output_3_false_positives: 155.0000\n",
      "Epoch 2/10\n",
      "4607/4607 [==============================] - 566s 123ms/step - loss: 9.8829 - output_1_loss: 2.7021 - output_2_loss: 1.0160 - output_3_loss: 0.0190 - output_4_loss: 6.1268 - output_3_precision: 0.8644 - output_3_recall: 0.5991 - output_3_true_positives: 161886.0000 - output_3_false_positives: 25400.0000 - val_loss: 6.0954 - val_output_1_loss: 1.3866 - val_output_2_loss: 0.3637 - val_output_3_loss: 0.0156 - val_output_4_loss: 4.3139 - val_output_3_precision: 0.9839 - val_output_3_recall: 0.5934 - val_output_3_true_positives: 7016.0000 - val_output_3_false_positives: 115.0000\n",
      "Epoch 3/10\n",
      "4607/4607 [==============================] - 565s 123ms/step - loss: 9.8138 - output_1_loss: 2.6894 - output_2_loss: 0.9649 - output_3_loss: 0.0190 - output_4_loss: 6.1215 - output_3_precision: 0.8637 - output_3_recall: 0.6002 - output_3_true_positives: 162177.0000 - output_3_false_positives: 25584.0000 - val_loss: 5.5700 - val_output_1_loss: 1.3538 - val_output_2_loss: 0.3663 - val_output_3_loss: 0.0136 - val_output_4_loss: 3.8227 - val_output_3_precision: 0.9793 - val_output_3_recall: 0.6428 - val_output_3_true_positives: 7601.0000 - val_output_3_false_positives: 161.0000\n",
      "Epoch 4/10\n",
      "4607/4607 [==============================] - 569s 124ms/step - loss: 9.8280 - output_1_loss: 2.6811 - output_2_loss: 1.0109 - output_3_loss: 0.0189 - output_4_loss: 6.0981 - output_3_precision: 0.8654 - output_3_recall: 0.6034 - output_3_true_positives: 163066.0000 - output_3_false_positives: 25357.0000 - val_loss: 5.8943 - val_output_1_loss: 1.3806 - val_output_2_loss: 0.3676 - val_output_3_loss: 0.0145 - val_output_4_loss: 4.1171 - val_output_3_precision: 0.9785 - val_output_3_recall: 0.6048 - val_output_3_true_positives: 7151.0000 - val_output_3_false_positives: 157.0000\n",
      "Epoch 5/10\n",
      "4607/4607 [==============================] - 568s 123ms/step - loss: 9.7522 - output_1_loss: 2.6577 - output_2_loss: 1.0072 - output_3_loss: 0.0187 - output_4_loss: 6.0498 - output_3_precision: 0.8670 - output_3_recall: 0.6065 - output_3_true_positives: 163858.0000 - output_3_false_positives: 25142.0000 - val_loss: 5.7603 - val_output_1_loss: 1.3680 - val_output_2_loss: 0.3573 - val_output_3_loss: 0.0143 - val_output_4_loss: 4.0063 - val_output_3_precision: 0.9832 - val_output_3_recall: 0.6184 - val_output_3_true_positives: 7312.0000 - val_output_3_false_positives: 125.0000\n",
      "Epoch 6/10\n",
      "4607/4607 [==============================] - 571s 124ms/step - loss: 9.7983 - output_1_loss: 2.6853 - output_2_loss: 1.0158 - output_3_loss: 0.0188 - output_4_loss: 6.0597 - output_3_precision: 0.8672 - output_3_recall: 0.6062 - output_3_true_positives: 163794.0000 - output_3_false_positives: 25087.0000 - val_loss: 5.7389 - val_output_1_loss: 1.3756 - val_output_2_loss: 0.3659 - val_output_3_loss: 0.0141 - val_output_4_loss: 3.9691 - val_output_3_precision: 0.9823 - val_output_3_recall: 0.6234 - val_output_3_true_positives: 7371.0000 - val_output_3_false_positives: 133.0000\n",
      "Epoch 7/10\n",
      "4607/4607 [==============================] - 571s 124ms/step - loss: 9.7918 - output_1_loss: 2.6471 - output_2_loss: 1.0622 - output_3_loss: 0.0188 - output_4_loss: 6.0450 - output_3_precision: 0.8670 - output_3_recall: 0.6072 - output_3_true_positives: 164063.0000 - output_3_false_positives: 25172.0000 - val_loss: 5.8696 - val_output_1_loss: 1.3482 - val_output_2_loss: 0.3838 - val_output_3_loss: 0.0147 - val_output_4_loss: 4.1082 - val_output_3_precision: 0.9840 - val_output_3_recall: 0.6078 - val_output_3_true_positives: 7187.0000 - val_output_3_false_positives: 117.0000\n",
      "Epoch 8/10\n",
      "4607/4607 [==============================] - 564s 122ms/step - loss: 9.6080 - output_1_loss: 2.6239 - output_2_loss: 0.9678 - output_3_loss: 0.0184 - output_4_loss: 5.9794 - output_3_precision: 0.8697 - output_3_recall: 0.6132 - output_3_true_positives: 165722.0000 - output_3_false_positives: 24820.0000 - val_loss: 5.8741 - val_output_1_loss: 1.3600 - val_output_2_loss: 0.3629 - val_output_3_loss: 0.0147 - val_output_4_loss: 4.1217 - val_output_3_precision: 0.9849 - val_output_3_recall: 0.6117 - val_output_3_true_positives: 7233.0000 - val_output_3_false_positives: 111.0000\n",
      "Epoch 9/10\n",
      "4607/4607 [==============================] - 566s 123ms/step - loss: 9.7446 - output_1_loss: 2.6578 - output_2_loss: 1.0244 - output_3_loss: 0.0186 - output_4_loss: 6.0251 - output_3_precision: 0.8681 - output_3_recall: 0.6089 - output_3_true_positives: 164511.0000 - output_3_false_positives: 25000.0000 - val_loss: 5.5011 - val_output_1_loss: 1.2898 - val_output_2_loss: 0.3490 - val_output_3_loss: 0.0137 - val_output_4_loss: 3.8350 - val_output_3_precision: 0.9874 - val_output_3_recall: 0.6252 - val_output_3_true_positives: 7392.0000 - val_output_3_false_positives: 94.0000\n",
      "Epoch 10/10\n",
      "4607/4607 [==============================] - 563s 122ms/step - loss: 9.6277 - output_1_loss: 2.6187 - output_2_loss: 0.9900 - output_3_loss: 0.0185 - output_4_loss: 5.9820 - output_3_precision: 0.8689 - output_3_recall: 0.6108 - output_3_true_positives: 165057.0000 - output_3_false_positives: 24901.0000 - val_loss: 5.5137 - val_output_1_loss: 1.3150 - val_output_2_loss: 0.3670 - val_output_3_loss: 0.0135 - val_output_4_loss: 3.8048 - val_output_3_precision: 0.9836 - val_output_3_recall: 0.6357 - val_output_3_true_positives: 7517.0000 - val_output_3_false_positives: 125.0000\n"
     ]
    }
   ],
   "source": [
    "os.chdir(root_path+ \"/pedestrian_dataset_train_tfr\")\n",
    "history = model.fit(train_dataset, epochs=10,validation_data=val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(root_path+\"/json_final_experiment\")\n",
    "import json\n",
    "#json.dumps(str(a))\n",
    "with open('history_finetuning_1121_90_100_epoch_nadam_0dot00001_mse_mse_3anchors_con_data_aug.json', 'w') as fp:\n",
    "    json.dump(str(history.history), fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(root_path+'/last_weights/pesos_finetuning_1121_90_100_epoch_nadam_0dot00001_mse_mse_3anchors_con_data_aug')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
